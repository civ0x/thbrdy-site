AIRE (AI Research and Education) 2022 OP1
AIRE’s mission for Machine Learning (ML) is to make the impossible possible (Lablets), to make the possible easy (Open Source and Services), and to make the easy accessible (Education). As such, Lablets advances the state of the art, investing in long-term strategic research, thereby extending the scope of what is possible. They provide new capabilities for existing services (Lookout for Metrics, Rekognition Video, Panorama, Lookout for Vision), they contribute to planned ones (Guru Code Analysis, Gecko, Evidently, Forecast, M*, Bifrost), and they break new ground in the areas of Graph Neural Networks (GNN), Causality, Explainability and Fairness, Efficient Algorithms and Statistical Estimation. AIRE lowers the barrier to use ML through AutoGluon, in scalable GNNs - through DGL and Neptune ML, in access to Deep Learning (DL) accelerators (Inferentia, Trainium) - through Meta/TVM, and in training large scale language models - through M5. Lastly, AIRE engages in training Amazonians through the Machine Learning University (MLU) and engineers and scientists at large through the Dive into Deep Learning (D2L) project.
Our research tackles hard challenges that AWS needs to solve on behalf of our customers. Some of these challenges correspond to core technologies which are not economically feasible for any single service team, but come with great synergies such that they have a significant positive impact over all of Amazon. This includes large scale language models, knowledge representation and extraction (knowledge graphs), multi-modal and relational data (text, images, graphs, social networks, time series). In most cases, such data does not occur in isolation. Even though it is common to focus on a single modality (e.g. text), it is only through comprehensive modeling of all modalities that the full benefits of data can be unlocked. Likewise, for ML to become widespread, it requires highly robust Automatic ML algorithms. Combining automation with multifaceted data advances the state of the art. Many services need help with training and inference at scale, on the edge, and on AWS’ own DL accelerators. We support them through DL compilers, code optimization, framework integration, and models for computer vision and NLP that can be used across the full range of platforms. We dedicate significant time to forward-looking research that focuses on solving some of the hardest science problems that may not have an immediate product impact, such as causality and natural language generation (from structured data). The Lablets in Tübingen, Shanghai, and Palo Alto have attracted 18 Scholars and Amazon Visiting Academics (AVA) that are helping AWS AI and beyond to excel scientifically in areas ranging from Fairness to NLP, Reinforcement Learning, vision, graphs, and statistics. Over the past year, we have started to develop research directly with a new service in mind by working backwards from customers (Particular, M*, Stagehand, DeepEarth, Bifrost). Lastly, the D2L project makes ML knowledge widely accessible in the form of a book, a notebook, a website, and a GitHub repository. Turning inwards, MLU teaches ML to Amazonians. In 2021, we will equip 50% of all engineers with operational knowledge for building ML models.
Highlights: We have seen success in graphs (Neptune ML, 27 PoCs in progress; 20+ PoCs of DGL, some of them are whale customers such as Uber, ByteDance; won top ranks in public leaderboard and text-graph competitions), Causality (six feature contributions to services), D2L (YoY 150% user growth in EN, adoption by 175+ universities), MLU (YoY 642% training growth), AutoGluon (41 internal adoptions after released brazil package two months ago, won four competitions as the 1st place, two as the 2nd place), DL compilation (40% end-to-end training performance improvement compared to the state-of-the-art solution on PyTorch) and scalable modeling (M5). In aggregate, our open source teams added 817,257 lines of code into seven packages. Beyond that, we are delighted to contribute to raising AWS standing in ML with 40 publications in top tier conferences in 2020 and 29 publications YTD. Scientists in AIRE were presented with three research awards (e.g. ICLR, PAKDD), acted as Program and Conference Chairs (e.g. MLSys), and we contributed to Amazon Science’s most downloaded paper in 2020. We won seven internal and external ML competitions.
Lowlights: There are also areas where we need to improve. In 2020 we decided to switch all toolkits wholesale from MXNet to PyTorch. While this has worked well for D2L and AutoGluon (DGL supported multiple frameworks from the beginning), the same cannot be said for GluonCV and GluonNLP. Even though both libraries gained significant PyTorch functionality (e.g. GluonCV acquired advanced video processing abilities relative to the Detectron, the GluonNLP BERT implementations tend to be faster than their HuggingFace counterparts), this was insufficient to attract a meaningful user base from the incumbents. Internal adoption was far more positive, e.g. in Panorama or the SageMaker model zoo. The key learning we take from this is that it is very difficult to glean mindshare from a dominant incumbent, something D2L, DGL and AutoGluon had to contend with to a lesser degree. Consequently, we will reduce our investment into standalone CV and NLP toolkits and focus on strengthening AutoGluon and DGL with CV and NLP capabilities. To illustrate how hard it is to displace incumbents, consider DGL which is more scalable and has seen strong adoption in industry (14 out of 22 major customer production projects, as per NVIDIA, picked DGL over all other alternatives). Nonetheless, PyG remains the most popular tool for academic experiments. Lastly, transfer of ideas and innovation from the Lablets into services has been slower than expected. We have taken corrective action by adding TPM and PMT team members and by updating the Lablets strategy. Overall, we learned that we need to invest meaningfully in team members whose task it is to ensure that we meet our customers where they are, rather than just expecting customers to discover what we build.
Before we go into detail, let us give a brief overview over the priority projects and big bets we have for 2022. We will double down on AutoGluon. It will acquire the ability to perform multimodal modeling for vision, text, tabular data, time series and graphs jointly. It will benefit from our expertise with large language models (M5 and GluonNLP) and of the new M* [I++:13] multimodal, multitask, multi-objective language modeling service. Moreover, we will leverage our results with Deep Learning on Graphs (DGL) and NLP (M*/M5) to build Bifrost [I++:13], an enterprise knowledge graph construction feature for either Kendra, HealthLake or SageMaker DataWrangler (conversations with the teams are ongoing). In addition, we will double down on education with D2L and MLU to make training content available externally. In most cases AIRE works with partners to go to market. For Trainium support on EC2, we joined teams with Annapurna Labs to deliver Meta for user-friendly and performant DL framework support. We are aligning with the IoT team to build Particular, a root cause analysis tool for industrial applications. Lastly, we will work with AWS sustainability team, and [Axis] team to deliver DeepEarth [I++:4], a toolkit for geospatial, climate, and remote sensing modeling on AWS.
Lablets [F:33, I:15]
Since Q1 2020, our three lablets published 46 papers in top-tier venues, creating significant visibility for Amazon in the scientific community. Lablet research was also key to several major launches: in addition to three DGL releases YTD, the Graph Research group helped launch Neptune ML which uses GNNs for easy, fast, and accurate predictions with graph data. DGL has emerged to be the GNN backbone of many influential research projects, such as Graph4NLP on DGL (for NLP tasks), DeepChem on DGL-LifeSci (the most popular DL package for chemistry & material science research), and KG tasks of OGB-LSC on DGL-KE (Open Graph Benchmark Large Scale Challenge). Shanghai and Palo Alto lablets engaged with 20+ customers, including RED (book) Inc ($10k MRR on SageMaker), Ant Financial, ByteDance, and ADP who are using DGL for GNN model training. The Tübingen lablet provided root-cause analysis features for Lookout for Metrics and automated root-cause analysis in Amazons Supply Chain Optimization Team, reducing the time required for a root-cause analysis from days to minutes.
Our work with the academic community leaned on Amazon Academic Hiring and Hubs University (Hubs) programs. Currently, Lablets hosts 18 academics who partner with teams within AI/ML to solve complex scientific problems. Scholars become ineligible for the Amazon Research Awards funding as soon as they join Amazon and have trouble securing additional funding sources due to their affiliation with AWS. We aim to support these academics with PhD student fellowships. Hubs draws funding from multiple teams across Amazon to pull a consolidated budget for academic investments. We wrapped negotiations with the Max Planck Institute, driven by  Lablets, into the Hubs framework to increase its impact and attracted investment from Softlines and Physical Stores Tech. Agreement negotiations are expected to close in early Q3.
Lablet Solutions Lab (LSL) Lablets scientists focus on long-term research projects that require years of exploration before they can be applied to solve customer problems. Technology transitions, on the other hand, are our vehicles of creating short to mid-term customer impact, based on scientific breakthroughs created by Lablets. Moreover, they create a customer focused pull that helps guide Lablets research. Achieving market fit as a research and education organization without its own product teams is difficult. For this reason, in 2020, we hired PMs across AIRE to help articulate the products and customer benefits enabled by Lablets research in cooperation with service teams. To further strengthen the transition mechanism over the next two years, we will equip our three lablets with LSL teams [F:2, I:6]. We will staff each of these teams with 2-3 AS/SDE and a PMT, and will ramp to full capacity by 2023, resulting in an incremental ask of six for 2022. Each LSL team is close to its lablet’s research activities and deeply understands its potential. Its mission is to work closely with customer-facing organizations like SA, MLSL, PMs from other organizations, and customer teams to propose and prototype solutions to customer problems, leveraging Lablets research. To deliver on its mission, the LSL team acts like a specialized ML Solutions Lab (MLSL) to develop small PoCs with representative customers, mainly with internal customers. In addition, the LSL team is responsible for producing PRFAQs with the goal of starting technology deployment projects for the most promising solutions. For example, the LSL team working on Causality-based ML will engage WBR finance & EC2 and, if successful, deliver PoCs and subsequent PRFAQs (See Appendix 3 for Operational Framework details).
Graph and Knowledge Graph Research [F:11, I:6]
The team, which includes scientists in the Shanghai and Palo Alto Lablets, is advancing the science in systems and models for DL on graphs and is pioneering how structured information is extracted and used from text documents. The team’s work consists of three thrusts: (i) Deep Graph Library (DGL)-OSS—a non-direct revenue generating open-source product, serving and advanced by the increasing momentum of the research community and industrial adoption; (ii) GNN Research—developing system-level and algorithmic optimizations for efficient, massive-scale GNN training and inference and novel GNN models for solving emerging real-world problems; and (iii) Text-to-Graph and Reasoning (TGER)—developing technologies to extract information-carrying structural data from unstructured text, and to perform reasoning on the resulting graph.
DGL-OSS continues to establish itself as one of the leading GNN frameworks with a broad user-based, industry adoption, and community support. DGL’s YoY stats are: 7.4k github stars, 8x Google citations, +24% external contributors, +71% and +175% doc & forum views, 15 AWS/Amazon team adoptions (against 4 in the previous year), PoCs/adoptions in two dozen external customers, including key customers such as Uber, ByteDance, MeiTuan and RED, Nike. We have struck key partnerships with Nvidia and Intel, collaborating on Nvidia cuGraph and Intel One API with engineering contributions. DGL’s Slack user group grew from 300 to over 800 actives in less than three months. DGL and PyG are the two leading GNN frameworks with distant followers. Among industrial users, DGL is the preferred framework (e.g., among Nvidia’s enterprise GNN customers, 60% chose DGL versus PyG’s 15%), whereas among researchers, the preferred framework is PyG. The reason that DGL has not succeeded in closing the gap with PyG among researchers is that we focused on industrial users whose needs are different to that of researchers. We have adjusted our development and feature roadmap to meet the needs for both personas. Our GNN research has resulted in 10 papers spanning new models, algorithms, and applications. Seven of our models achieved rank #1 on the Open Graph Benchmark (OGB) leaderboard. We are collaborating with the Rekognition team to co-develop Hi-Lander, a weakly-supervised learner to be incorporated in Face V2. Our work on TGER established bidirectional mappings between text and graphs with a total of six publications while winning the #1 rank in multiple tracks of the WebNLG 2020 competition and led to collaboration with the Lex team on dialogue modeling with graph.
With flat HC our plans are: DGL-OSS: (i) command line tools for training & applying GNNs on various tasks; pre-trained models, data loaders from industrial data pipelines, and support for spatial-temporal and dynamic graphs [F:2], (ii) scale system capacity to industrial-scale graph data, GPU-only based distributed training, and high performance inference infrastructure [F:2], (iii) tutorials, outreach, and support [F:1]. GNN Research: (i) develop novel GNN models that accommodate heterogeneous node/edge features with rich semantics, and temporally-varying graphs, which are common ingredients of real-world graphs (e.g., social networks, relational databases, etc.) that current GNN architectures are ill-equipped to handle [F:2] and (ii) develop system-level and algorithmic optimizations for efficient, massive-scale GNN training and inference [F:1]. TGER: (i) develop knowledge graph extraction techniques under low-resource constraints, at fine-granularity over multi-documents, and structural bidirectional mapping between text and image data [F:2] and (ii) develop approaches for multi-turn question understanding and reasoning [F:1].
We ask for an incremental headcount of [I:5], which we will use to move the work associated with DGL-OSS outside the Lablets. DGL-OSS has become a framework on which many Amazon/AWS services and products depend. A large portion of the planned DGL-OSS activities are geared towards meeting their requirements. As such, the core of the DGL-OSS effort is no longer a forward-looking research project and therefore, it does not align with the Lablets’ mission (see Operational Framework in Appendix 3). We plan to use the freed Lablets headcount and an additional [I:1] to expand our activities as follows: GNN Research: (i) efficient graph sampling algorithms and systems optimizations to support the training of billion-size graphs with complex node-level models, including new compiler techniques [2]; (ii) new architectures to address spatial-temporal graphs, tabular data, and robustness against noise/graph uncertainty [2]. TGER: (i) conversational response generation [1] and (ii) novel embedding techniques to efficiently serve downstream queries [1].
Reinforcement Learning [F:3]
The Reinforcement Learning (RL) research team works on advancing the state of the art in robust and flexible RL methods. While many popular RL algorithms can work if the hyperparameters are tuned just right, they are often brittle whenever they are applied to new problems. This makes them difficult to deploy in practice. The team designs RL algorithms that are sample efficient, readily transferable to new tasks, robust and reliable with regard to parameters, simple to implement and deploy, relevant to Amazon. Besides publications in top-tier conferences (NeurIPS, UAI, ICLR) the team works with the Annapurna SRD (Scalable Reliable Datagram) team on optimizing the communication protocol in EC2. In the past year we worked on a high fidelity simulator (this turned out to be the main obstacle). Our RL algorithms by far surpass the predefined heuristics. Unfortunately, we discovered that the objectives were not effectively defined by the customers of the algorithm (this was subsequently mitigated, still with good results). This underscores that RL so far is not an easily applicable technique in practice and that domain knowledge of the actual problem to be addressed is paramount for successful deployment. In 2022 we will continue working on SRD [F:1] towards a launch at re:Invent’22. Moreover, we will focus on offline and hybrid off-policy RL [F:2] - while underserved by the research community, this matches a common situation in Amazon and with AWS’ customers where plenty of historical behavioral data is available (e.g. SCOT, retail). Few of them are willing to disband an existing deployment in favor of the potential gains of RL. Offline RL addresses this.
Causality Research [F:11, I:3]
Our team’s mission is to leverage causal principles to learn, explain, and exploit the complex cause-effect relationships underlying real-world data. This is reflected in our three key endeavors which benefit from the cross-pollination of interdisciplinary causality research: representation learning, fairness and explainability, and causal inference. Since last OP1, we hired senior scientists and scholars (Alan Turing Institute, Max-Planck, ETH Zurich), each with individual scientific agendas, before defining a detailed research scope for the lablet. It took significant time to align the various research interests into a coherent agenda with AWS priorities. Today, our research roadmap reflects the collaborations across Lablets, as well as the needs of partner teams, such as Rekognition, MLSL, Clarify, Themis, or Elastic Search. We will adjust our hiring strategy to prioritize prior alignment.
Causal Representation Learning [F:5, I:3] This endeavor develops robust and transferable representation learning algorithms that embrace the reality that most data is not “independently-and-identically-distributed (IID)”: the intended use-case of a model rarely matches available training data perfectly. YTD, the team has published three papers in top venues (ICML). We achieved state-of-the-art performance on industrial anomaly detection (MVTec) tasks - an ongoing collaboration with the Lookout for Vision team. Our research roadmap revolves around three goals: (1) design a set of stress-tests and evaluation protocols for object classification and detection to identify how a model would perform facing distribution shifts on a given dataset. With [I:1], our aim is to provide actionable feedback on a variety of potential data changes before a model is deployed. We will also apply this to Custom Labels and MLSL. (2) Learn and exploit invariances extracted from additional non-IID data like videos and data with distribution shifts/drifts to improve reliability of representation learning. Our architectural innovations enforce natural invariances in the representation that are more meaningful than artificial data augmentations. (3) With [I:2], we formalize applications of causality in computer vision tasks, for which we generate the first real-world visual data with causal ground-truth and establish it as the first benchmark for causal representation learning. This will yield an MVP in 2022 for abstract visual reasoning and engage in community efforts. Overall for next year, we target the publication of 10 research papers and one data set, and we will build a toolkit and prepare a demo on model evaluation.
Fairness and Explainability [F:2, I:0] This endeavor develops and deploys causal modeling approaches to identify bias factors, moving strong conceptual insights like fairness-optimized classification and counterfactual explanations to business-applicable solutions. We published eight papers in 2020 and YTD in top venues (NeurIPS, CVPR, Computer Law & Security Review with BBC news coverage), and we pioneered a new fairness metric and supported its transition into a SageMaker Clarify feature. Going forward, one focus is the extension of our adaptive sampling approach for minmax fairness. We are currently engaging with SageMaker Clarify, with a view to offer it as a service to external customers. We are currently developing minmax-based generative methods for facial recognition systems and if successful, we will apply it to Rekognition and Faces. We work with the Themis team to develop and deploy causal modeling approaches to identify bias factor. We further incorporate Themis’ bias and fairness assessment requirements into the stress tests that we develop. Together with the Amazon Search Customer Experience team, we have started to look into issues of inclusive search and to work on bias mitigation strategies. We are engaging with Amazon Personalize to add Amazon style explanations “We are showing you this item because you previously bought X” by generating counterfactual explanations for turn-key recommender system. We further target the publication of at least four tier-1 research papers.
Causal Inference [F:4, I:0] In this endeavor we focus on fundamental research in the area of causal machine learning. Since last OP1, however, we have prioritized the collaborations with services: we launched a total of six features (see Core section) but we underdelivered on peer-reviewed publications with only five in 2020 and YTD (ICML, AISTATS, AAAI). The learnings from this miss have flown into our new Lablets operations framework and the dedicated mission of LSL. With these changes, we target six peer-reviewed publications in top-tier venues until end of 2022. There are three focus topics: First, causal discovery, i.e. discovering the structure of cause-effect relationships under usage of a priori knowledge about the problem structure like time-series data. Second, robustly quantifying root-causes for different types of observations like distribution shifts. Third, merging causal data sets to generate new causal insights, in collaboration with the Causal Representation Learning efforts on acquiring causal ground truth data. The biggest risk for fundamental research in causality is the lack of labeled data for benchmarking, which makes it hard to assess empirically whether the research has the expected impact. To mitigate this risk, we work closely with our partner teams to acquire ground-truth data.
Large Volume Internship (LVI) program, run by the Shanghai lablet, provides science and engineering velocity for AIRE and beyond, with high-quality, year-round available intern candidates in China. Interns in this program work on projects spanning over a wide range of application domains, and are supervised by local and global mentors, as well as some jointly-supervised projects with product groups. The program has onboarded 93 interns in 2020 and 2021 YTD, taking advantage of the availability of talented junior students. As part of LVI, the interns contributed numerous model implementations for various toolkits (e.g. 34 out of 45 of new DGL models in 2021 YTD), produced 1000+ pull requests, and co-authored 18 papers. For 2021, we will maintain the same level of investment, while improving on seniority (COVID-19 meant that many students did not go overseas) and strengthening collaboration ties with key faculty.
Open Source and Services [F: 38, I: 12, BTL: 4]
Causality-based Products and Services [F:0, AI Verticals F:2, External F:5: This section covers technology transitions from Causality Research. We help developers and businesses understand and explain the complex cause-effect relationships underlying their business problems and enable them to make better business predictions and decisions. Since last OP1, we have engaged with 15 internal and external customers, delivered seven PoCs, and engaged in technology transitions with one CDO and two AWS service teams which fund a total of seven HC. After the first production successes in 2021 (GA with Lookout for Metrics and tool launch with the Supply Chain Optimization Technology (SCOT) team), we will continue to deliver product features in our three fully externally funded technology transition projects: In GraphY [External F:1], the SCOT team funds one HC to automate root-cause quantification across the supply chain management system. Evidently [External F:4] is funding four HC to develop advanced A/B testing. Lookout for Metrics [AI Verticals F:2] is funding 2HC to develop root-cause quantification for time-series data. In addition to these technology transitions, we propose Particular (see separate PRFAQ), a cooperation with IoT to develop a new IoT SiteWise service to reduce quality-related cost in manufacturing. We are working with the IoT team to figure out the funding. With flat HC, we will confine our work on PoCs that further demonstrate the capability for both internal and external customers.
Neptune ML Team [F:4, I:0] The team is developing Neptune ML, a feature of Amazon Neptune that makes it easy to train and use graph-ML models and predictions using REST APIs and graph queries. Neptune ML was launched at re:Invent’20. Since its release, 27 customers had expressed interest and are pursuing PoCs, and Uber/Careem is launching a fraud & abuse application. Our ability to quickly iterate and add new features in Neptune ML has been blocked by Neptune’s own release cycles. We have re-engineered the interfaces to accelerate our new feature velocity. For next year, with flat headcount, we will continue expanding Neptune ML’s capabilities and support ML models over virtual views combining property graphs from Neptune with node/edge properties from RDBs; reduce real-time inference latency via model distillation and in situ inference; support large model training by adding distributed training; and expand and simplify custom model support. With incremental HC, we will build a suite of domain-specific GNN models to allow graph application developers to easily build accurate solutions for their domains [I:4 - included in Neptune’s OP1]. The target domains are fraud and abuse detection, entity disambiguation, health-care and life-science, and recommendations. These domains correspond to the four most-frequent use-cases of Neptune ML’s customers.
Multi-dimensional Systems and Technologies [F:8, I:4] The team is advancing science and developing the software infrastructure for training and using large deep learning models. It is responsible for (i) GluonNLP, an OSS toolkit for building, training, and using deep learning models for NLP tasks, and (ii) working with Amazon Search to develop the infrastructure and science for M5, which includes distributed training, multi-modal modeling, multi-task learning using graphs, and model vending. Due to the accelerated adoption of Huggingface as the standard PyTorch-based OSS NLP toolkit, we made the decision to reduce the 4 HC investment in GluonNLP to [F:1], allocate 2 HC to M5 that now has [F:7], and allocate the other 1 HC to AutoGluon. With flat head count, the efforts on GluonNLP will focus on KTLO by supporting existing MXNet users; whereas the efforts on M5 will focus on scaling distributed training to models with over 1T parameters, optimize training for Trainium, develop large-scale distributed training of the GNN-based multi-task approach, and extend model vending to multi-modal and multi-entity models. We will be doing a comprehensive review of all the GluonNLP dependencies, collecting customers’ requirements and organizing meetings to discuss plans and recommendations. In addition, we plan to undertake the following initiatives requiring incremental HC: M* for AWS AI/ML HLT: [I++:13] Develop a service for training and vending large M5-style pre-trained models for our HLT services. The vending includes task-specific fine-tuning, distillation, and optimized inference. We have already started working on a PoC to show the benefit of an M*-based model with Lex and Comprehend. CoDEL: Research and Toolkit for Code DEep Learning: [I:4] Develop the infrastructure and science to generate and use large pre-trained models for code modeling and code generation applications that include multiple modalities (natural language, code, code graphs), multiple pre-training objectives and tasks, and addresses privacy issues. CoDEL is being developed in with the CodeGuru and Vector teams and is used by these projects.
Meta [F:15; I:4]. Meta uses compiler techniques to accelerate the training of DL models in various frameworks. Meta targets Trainium, an ASIC built by Annapurna Labs (re:Invent 2021, GA planned in 3/2022), as well as Nvidia GPUs. Today, Meta supports PyTorch, because most of our MVP target customer workloads use it. Meta compiles popular CV and NLP models from TorchVision and HuggingFace model zoos, including ResNet, VGG, DenseNet, MobileNet, SqueezeNet, Mask R-CNN, BERT, GPT-2, and M5 1.5B. This covers 70% of the MVP workloads. On P3/P4, Meta delivers up to 40% speedup, compared to PyTorch baselines.
Today, DL frameworks are optimized by high-performance operator (kernel) vendor-libraries. The frameworks are not designed to combine graph-level optimizations with kernel-level optimizations. Customers can use the XLA TensorFlow compiler to process PyTorch workloads. Annapurna and Deep Engine Hopper are starting with XLA to build their respective training compilers for Trainium and GPUs, but plan to adopt Meta in 2022. Meta introduces a new mechanism (RAF) that extends the Apache TVM compiler from inference to training. XLA has a low-risk for GA, while RAF offers the higher-risk, better-performance path. The main risk is building the compiler infrastructure from scratch, including frontend, graph & tensor optimizations, and backend with framework integration. The advantage is that we can design our own IR to better fit dynamic DL models as well as custom cluster and system hardware architectures, offering opportunities for whole-system optimization. The Annapurna team is using RAF with their low-level compiler for distributed training, since it covers 10x more model types than XLA. Our work on operator fusion and kernel dispatching in RAF shows promising performance on GPUs. The Deep Engine team plans to use RAF to enhance their high-level Hopper compiler for SageMaker in 2022. We will double down on this to take better advantage of low-level computation and communication libraries from Annapurna, EFA, and NVIDIA for end-to-end optimization.
With flat HC, we will keep optimizing key customer workloads on Trainium and GPUs until 2022. We will cover all MVP/GA PyTorch workloads through Torchscript [F:4, including project leader]. We will use MLIR to unify the RAF path and XLA path to reduce the friction and improve user experience [F:1]. Optimizing the performance requires the largest chunk of the resources. 1) At the graph-level, we will optimize the end-to-end graph workloads (forward, backward, and weight updating) for Trainium [F:3]. Although some optimizations can be reused for GPUs, we need additional passes for GPU [F:1]. 2) At the tensor-level, Meta will rely on the low-level Trainium compiler (Penguin) for kernel optimization. We will work with Annapurna on Penguin for both bug fixes and performance optimization [F:2]. As most TVM operators are optimized only for inference, we need to re-tune or even re-write the schedules for training workloads and integrate with NVIDIA libraries such as CuDNN and Cutlass [F:2]. 3) integrate with communication libraries for distributed training on both Trainium and GPUs [F:2]. With an incremental HC of 4, we will extend Meta to support TensorFlow [I:2], federated learning [I:1], and distributed training of large-scale language models (e.g. M5/M*) on thousands of processors [I:1].
One of the biggest risks we see is our dependence on Penguin, which has been a blocker to the compilation flow on multiple occasions. Penguin performs sophisticated loop-level optimizations, which often causes bugs that are nontrivial to fix (the XLA path has a similar problem but triggers different bugs). We are mitigating the risk by helping the Penguin team with 2HC, but ultimately we rely on them. Optimizing large-scale distributed deep learning training workloads through a compiler approach is highly promising but science hard.
Autogluon (AG) [F:7, I:4, BTL:4] is an open-source AutoML toolkit. Given raw image, text, or tabular data, AG enables novice ML users to train/deploy state-of-the-art models in three lines of code. AG has been adopted by internal teams including Catalog, AWS Enterprise, and MLSL. It has been used by 41 projects in two months after we released the Brazil package. Externally, AG has 619k package downloads (+75% YoY) and integrated with RAPIDS by NVIDIA. In both internal and external ML competitions, AG won seven competitions with ranking in top-3 (4 as the champion). Ease of use is the main draw of AG, it allows to rapidly test ideas. However, AG onboarding is not always a smooth experience: customers require handling data with different characteristics, new unsupported tasks, and tight inference latency constraints. We will focus on functional improvements and deployability.
As the demand for multi-modal applications grows, we are expanding the spectrum of multi-modal features handled by AG from text to tabular data, images, graphs, and time series. Most of this functionality is driven by pre-trained encoders for image embeddings (e.g. ResNet), language (e.g. BERT) and graphs. We will add GluonCV and GluonNLP representations and release the models to SageMaker JumpStart [F:7, E:2]. Ease of deployment remains a challenge. To address this, we will add integrations with model servers. Moreover, we will add detection of covariate label and concept shift and drift, in some cases designing new statistical estimators to catch data issues. We will add an incremental training mode for continuous deployment, improved model distillation [I:2], and cloud.fit() training for hybrid cloud modeling locally and on SageMaker [BTL:2]; c) we will improve community support by creating more notebook examples, write a chapter for D2L and teach a class at Stanford [I:2];  d) we will work on easy task definition for customers, via many shot learning. This is the riskiest part of the plan, since there isn’t much research in the area between one-to-few shot learning (less than 10 observations) where heuristics abound and fine-tuning (1k and more observations). This will allow customers to define new estimation problems on their own, without the need for objective function engineering [BTL:2]; e) lastly, we will support the Amazon Classification and Policy Platform (CPP) team for matching tasks [E:3] and Central Econ for logistics applications [E:2]. We plan on funding the multi-modal aspects of this work by merging the corresponding parts of the GluonCV and GluonNLP teams into AutoGluon.
The biggest risk for AG is community uptake. Since it is used by very different customers (e.g. developers, scientists, users, internal and external teams), there is no single silver bullet. We will work on lowering the bar for adoption (Brazil, PIP, tutorials, videos, hackathons), ensure the project’s visibility (publications, lectures, benchmarks), and work with partners to address their requirements. A blocker for AWS AI adoption is the fact that AG’s design is very different from AutoPilot (the difference is what gives it accuracy and engineering velocity). We will need to work with the AutoPilot team to see how we can help them.
MXNet [F:2, I:0] In 2020 OP1, we paused investments in MXNet. The team focused on supporting existing users, releasing v2.0 with a reduced scope and project graduation from the Apache Incubator. With [F:2], we will continue to support MXNet users and maintain v2.0 to keep the lights on. We will also enable partners, including Nvidia and Intel, to continue contributing to MXNet.
GluonCV [F:2, I:0] GluonCV provides 1178 pre-trained models for six CV tasks and tracked 5.6M downloads in 2020 (+207% YoY). All these models can be deployed efficiently through TVM, a key feature for industrial users. There are 181 internal adoptions of GluonCV (56 new in 2020). However, GluonCV customers are unwilling to support HC, partially because open-source alternatives exist. We will reduce our efforts to expand to new general CV models. 1) We will continue maintaining GluonCV to support existing customers. [F:1] 2) There is a strong demand for a high-level easy-to-use interface which we will offer through AutoGluon. 3) The team collaborated with Rekognition and delivered a video package which adopted by teams including Rekognition, Prime Video and Data Center Automation. We will continue exploring this niche [F:1].
Education [F:8, I:6, BTL:1]
The AIRE Education team’s mission is to empower customers to identify ML problems and provide them with the skills to build and deploy models within academic, Amazonian, and AWS enterprise settings. We teach beginners and advanced practitioners to deploy ML to real world problems through two products: MLU and D2L. MLU produces educational courses for Amazonians and MLSL customers (Embark) who want to apply ML to business problems. D2L publishes an open source textbook that enables students, engineers, and scientists to understand and use DL via code, math, and discussions. In 2020, MLU built a 12-course, end-to-end ML curriculum balancing theory and application to expose practitioners to bootcamp-style accelerators, technique, and application content. In 2021, MLU is on track to teach ~35,000 Amazonians (approximately 7x YoY growth) and deliver training to ~200 AWS Embark customers. Our course catalog currently contains 14 3-6 day classes that cross-reference D2L content where appropriate and have earned average CSAT scores of 4.28 out of 5 YTD as of 6/2021 (as compared to our target of 4.2). In 2020, D2L attracted 790K online users (+47% YoY), 1.66M Web sessions (+51% YoY), 782K PDF downloads (+53% YoY), 389k code downloads (+52% YoY), and grew university adoptions from 70 to 175 (+150% YoY). In 2021, D2L has 34.3k GitHub stars, 330 GitHub contributors, and is on track to complete both English and Chinese versions for press publications, increase university adoptions from 175 to 250 (+43% YoY), and integrate D2L with SageMaker Seychelles.
Today, there are two primary pathways to learning ML: courses (university and Massive Online Open Courses (MOOC) style) and short format content (videos, blogs and tutorials). Many courses are comprehensive but fail to teach to solve real problems. Short, specific, topical content is quick and free but fragmented or lacking a systematic learning pathway. MLU and D2L fill these gaps for our respective customer segments. That said, customers care about ROI: time investment versus effectiveness such as solving an immediate problem or completing challenging projects that may lead to career growth. We see evidence of this tension in our MLU classrooms, where 40% of MLU registered students failed to complete a course after starting, citing “urgent business projects” (40%) and “personal scheduling conflicts (vacations)” (19%) as their top reasons for not completing. While complementary, MLU and D2L exist as monolithic customer offerings intended as a complete course or chapters of a textbook and delivered through separate mechanisms. Signals from our publicly available video content tell us that some customers are hungry for practical, bite-sized learning assets that simultaneously teach and provide an opportunity to apply. Shorter segments receive significantly more engagement. When MLU launched its YouTube channel in 2020 (105 videos, 1-35 minute segments), it earned 63.4K subscribers in nine months. Similarly, D2L’s newest MOOC (Apr 2021 — Sep 2021) attracted 46k followers with 730k views and 40k comments in the first two months. D2L also released 5 3min length videos that attracted 1.4m views. These videos have been surprisingly popular, but lack a customer interface or clear learning pathway that helps them intuit what to learn next.
In 2020, we added PyTorch (and some TensorFlow) to the MXNet implementations in D2L. This resulted in a +89% increase in web sessions, leading us to believe that a framework-neutral strategy will benefit more customers. We will complete TF and plan to add JAX coverage. Similarly, translating 1/3 of the updated English content to Chinese in early 2021 led to a +246% increase in web sessions. Translating D2L to other languages will attract more non-native English speakers. Besides China, 6% of the top-5 traffic comes from South Korea. A Korean and Turkish community translations are in the works. Comparing D2L to popular ML education websites, we found that the top queries are about traditional ML, currently not covered by D2L. Also, customers are hungry for advanced topics such as RL and GANs. We plan to extend D2L to cover both ends of the spectrum to broaden the audience.
In 2022, we will disambiguate MLU and D2L resourcing to reflect the customer segments we serve. D2L is requesting explicit funding to restore MLU’s 2021 resourcing to its total of 26. With the [I:4] freed from D2L, we will hire engineers to (1) build data collection capabilities in our learning portal, (2) build infrastructure to give automated feedback on in-class code submissions, (3) and create a single point of registration ingress for all MLU courses, challenges and tech talks. We will review MLU’s plan in a separate document which will articulate our strategy to reprioritize centralized flat HC to replace resourcing previously funded by other teams (AWS T&C Curriculum and Region Services), upgrade our Leaderboard tool and add an analytics function in the program. With HC [E: 3], we will (1) modularize course content and (2) [E:1] teach our new case study course designed for customers in decision maker roles who prefer to learn outside of a coding environment.
D2L will identify and bridge gaps in the customer journey from ML beginner to expert, create learning assets that can be used to address a wide variety of customer pain points and create a culture of content sharing to accelerate the creation of cutting-edge ML training for AWS customers of all kinds. With the [F:4] we will: (1) refactor D2L contents to allow customers to easily customize their learning plan, (2) refactor D2L’s code implementation to make it more reusable for customers as a light-weight high-level framework with high quality documentation, (3) will release 50 hours of video training to teach D2L. With the incremental HC, we propose to (1) [I:1] improve SageMaker Seychelles integration (at present this HC is only temporary but we expect additional work after GA at re:Invent 2021, hence asking for HC) (2) [I:1] work with external authors on D2L to deliver two additional chapters on advanced topics, (3) [BTL:1] add an introductory book to cover data science and ML.
Appendices
Appendix 1: Headcount Tables
Appendix 2: FAQs
Appendix 3: Lablets Operational Framework
Appendix 4: DGL-OSS Graduation
Appendix 5: Causal Representation Learning Scientific Agenda
Appendix 6: AutoGluon Roadmap
Appendix 7: Meta Roadmap
Appendix 8: Education - Microlearning
Appendix 9: AWS Announces [DeepEarth]
Appendix 10: AWS announces Amazon [IoT SiteWise Particular]
Appendix 11: AWS announces [Bifrost]
Appendix 12: AWS Announces [M*]
Appendix 13: Open Source Contributions
Appendix 1: Headcount Tables
Lablets HC (CC 7619)
| # | F/I | Project | Description | Total HC | Cumulative HC |
|---|---|---|---|---|---|
| 1 | Flat | Support Org | Management & Support Team | 6 | 6 |
| 2 | Flat | LSL | PMTs | 2 | 8 |
| 3 | Flat | DGL-OSS | Command line tools for training & applying GNNs on various tasks; pre-trained models, data loaders from industrial data pipelines, and support for spatial-temporal and dynamic graphs | 2 | 10 |
| 4 | Flat | DGL-OSS | Scale system capacity to industrial-scale graph data, GPU-only based distributed training, and high performance inference infrastructure | 2 | 12 |
| 5 | Flat | DGL-OSS | Tutorials, outreach, and support | 1 | 13 |
| 6 | Flat | GNN Research | Novel GNN models that accommodate heterogeneous node/edge features with rich semantics, and temporally-varying graphs, which are common ingredients of real-world graphs that current GNN architectures are ill-equipped to handle | 2 | 15 |
| 7 | Flat | GNN Research | Develop system-level and algorithmic optimizations for efficient, massive-scale GNN training and inference | 1 | 16 |
| 8 | Flat | TGER | Develop approaches for low-resource, fine-grained, multi-document structured information extraction of multi-modal data. | 2 | 18 |
| 9 | Flat | TGER | Develop approaches for multi-turn question understanding and reasoning | 1 | 19 |
| 10 | Flat | Representation Learning | Stress-tests and evaluation protocols for object classification and detection | 2 | 21 |
| 11 | Flat | Representation Learning | Learn and exploit invariances extracted from additional non-IID data and data with distribution shifts/drifts to improve reliability of representation learning | 3 | 24 |
| 12 | Flat | Fairness | Work on fairness and explainability to move from conceptual insights like minimax fairness and counterfactual explanations to business-applicable solutions. | 2 | 26 |
| 13 | Flat | Causal Inference | Fundamental research in the area of causal machine learning, Causality SW Library support. | 4 | 30 |
| 14 | Flat | Reinforcement Learning | SRD work on on optimizing the communication protocol in EC2 | 1 | 31 |
| 15 | Flat | Reinforcement Learning | Offline and hybrid off-policy RL | 2 | 33 |
| Lablets 2022 Flat HC | Lablets 2022 Flat HC | Lablets 2022 Flat HC | Lablets 2022 Flat HC | Lablets 2022 Flat HC | 33 |
| 16 | Inc | LSL | PMTs & SDEs to work with customer-facing teams from other organizations, prototype solutions to customer problems, leveraging lablet research. | 6 | 39 |
| 17 | Inc | GNN Research | New architectures to address spatial-temporal graphs, tabular data, and robustness against noise/graph uncertainty | 2 | 41 |
| 18 | Inc | GNN Research | Robust models against noises, spatial-temporal graph and tabular data | 2 | 43 |
| 19 | Inc | TGER | Conversational response generation | 1 | 44 |
| 20 | Inc | TGER | Novel embedding techniques to efficiently serve downstream queries | 1 | 45 |
| 21 | Inc | Representation Learning | Adopt and apply stress-tests and evaluation protocols beyond science activities to customer problems from Custom Labels and MLSL | 1 | 46 |
| 22 | Inc | Representation Learning | Formalize applications of causality in computer vision tasks, collect visual data-set with causal ground truth as a benchmark | 2 | 48 |
| Lablets 2022 Incremental HC | Lablets 2022 Incremental HC | Lablets 2022 Incremental HC | Lablets 2022 Incremental HC | Lablets 2022 Incremental HC | 15 |
| Lablets 2022 Total HC | Lablets 2022 Total HC | Lablets 2022 Total HC | Lablets 2022 Total HC | Lablets 2022 Total HC | 48 |

Core HC (CC 7952)
| # | F/I/BTL | Project | Description | Total HC | Cumulative HC |
|---|---|---|---|---|---|
| 1 | Flat | GuonNLP | Supporting existing MXNet users, model co-design and optimization for (pre-)training, distillation, and inference on AWS in-house ASIC hardware, and integration with huggingface for expanding backbone model offerings | 1 | 1 |
| 2 | Flat | M5 | Scaling distributed training to models with over 1T parameters, optimize training for Trainium, develop large-scale distributed training of the GNN-based multi-task approach, and extend model vending to multi-modal and multi-entity models | 7 | 8 |
| 3 | Flat | GluonCV | Maintaining GluonCV to support existing customers | 1 | 9 |
| 4 | Flat | GluonCV | Video package. | 1 | 10 |
| 5 | Flat | AutoGluon | Expanding the spectrum of multi-modal features automatically handled by AG from text to tabular data, images, graphs, and time series. Adding GluonCV and GluonNLP representations. | 7 | 17 |
| 6 | Flat | Meta | Manager/Project Lead | 1 | 18 |
| 7 | Flat | Meta | Optimizing key customer workloads on Trainium and GPUs, cover all MVP/GA PyTorch workloads through Torchscript | 3 | 21 |
| 8 | Flat | Meta | Use MLIR to unify the RAF path and XLA path to reduce the friction and improve user experience | 1 | 22 |
| 9 | Flat | Meta | End-to-end graph workloads (forward, backward, and weight updating) for Trainium | 3 | 25 |
| 10 | Flat | Meta | Additional optimization passes for GPU | 1 | 26 |
| 11 | Flat | Meta | Bug fixes and performance optimization | 2 | 28 |
| 12 | Flat | Meta | Re-write the schedules for training workloads and integrate with NVIDIA libraries such as CuDNN and Cutlass | 2 | 30 |
| 13 | Flat | Meta | Integrate with communication libraries for distributed training on both Trainium and GPUs | 2 | 32 |
| 14 | Flat | MXNet | Supporting MXNet users and maintaining v2.0. | 2 | 34 |
| 15 | Flat | Neptune ML | Reducing real-time inference latency, combining property graphs from Neptune with node/edge properties from RDBs, distributed training, custom model support | 4 | 38 |
| Core 2022 Flat HC | Core 2022 Flat HC | Core 2022 Flat HC | Core 2022 Flat HC | Core 2022 Flat HC | 38 |
| 16 | Inc | CoDEL | Develop the infrastructure and science approaches to efficiently generate and use large pre-trained models for code that include multiple modalities, multiple pre-training objectives and tasks. | 4 | 42 |
| 17 | Inc | Meta | Extend Meta support to TensorFlow | 2 | 44 |
| 18 | Inc | Meta | Federated learning | 1 | 45 |
| 19 | Inc | Meta | Distributed training of large-scale language models such as M5/M* on thousands of processors | 1 | 46 |
| 20 | Inc | AutoGluon | Add an incremental training mode for continuous deployment, improved model distillation | 2 | 48 |
| 21 | Inc | AutoGluon | Improve community support by creating more notebook examples, write a chapter for D2L and teach a class at Stanford | 2 | 50 |
| Core 2022 Incremental HC | Core 2022 Incremental HC | Core 2022 Incremental HC | Core 2022 Incremental HC | Core 2022 Incremental HC | 12 |
| Core 2022 Total HC | Core 2022 Total HC | Core 2022 Total HC | Core 2022 Total HC | Core 2022 Total HC | 50 |
| 22 | BTL | AutoGluon | Cloud.fit() training for hybrid cloud modeling locally and on SageMaker | 2 | 52 |
| 23 | BTL | AutoGluon | Easy task definition for customers, via many shot learning | 2 | 54 |

Education HC (CC 7647)
| # | F/I/BTL | Project | Description | Total HC | Cumulative HC |
|---|---|---|---|---|---|
| 1 | Flat | MLU | Product & Program Support | 5 | 5 |
| 2 | Flat | MLU | SDE resources for Leaderboard support and course infrastructure maintenance | 4 | 9 |
| 3 | Flat | MLU | Course content creation and science instruction. Refreshing, updating and re-recording content for all entry-level MLU courses as micro-learning assets. | 9 | 18 |
| 4 | Flat | MLU | Training specialists. Publishing content and customizable syllabi or templates to guide customers through targeted learning pathways. | 3 | 21 |
| 5 | Flat | MLU | Business analytics | 1 | 22 |
| 6 | Flat | D2L | Refactoring D2L’s code implementation to make it more reusable for customers as a light-weight high-level framework with high-quality documentation | 2 | 24 |
| 7 | Flat | D2L | Releasing 50 hours of video training to teach D2L and refactoring D2L contents to allow customers to easily customize their learning plan | 2 | 26 |
| Education 2022 Flat HC | Education 2022 Flat HC | Education 2022 Flat HC | Education 2022 Flat HC | Education 2022 Flat HC | 26 |
| 8 | Inc | MLU | SDEs to (1) build data collection capabilities in our learning portal, (2) build infrastructure to give automated feedback on in-class code submissions, (3) and create a single point of registration ingress for all MLU courses, challenges and tech talks. | 4 | 30 |
| 9 | Inc | D2L | SageMaker - Seychelles integration | 1 | 31 |
| 10 | Inc | D2L | Two additional D2L chapters on advanced topics | 1 | 32 |
| Education 2022 Incremental HC | Education 2022 Incremental HC | Education 2022 Incremental HC | Education 2022 Incremental HC | Education 2022 Incremental HC | 6 |
| Education 2022 Total HC | Education 2022 Total HC | Education 2022 Total HC | Education 2022 Total HC | Education 2022 Total HC | 32 |
| 11 | BTL | D2L | Introductory book to cover data science and traditional machine learning. | 1 | 33 |

Big Bets HC
| # | I++ | Project | Description | Total HC | Cumulative HC |
|---|---|---|---|---|---|
| 1 | I++ | M* | Manager | 1 | 1 |
| 2 | I++ | M* | Data scrapping, catalog management and pre-processing for M* model training. | 1 | 2 |
| 3 | I++ | M* | Creating and maintaining cluster infrastructure. | 1 | 3 |
| 4 | I++ | M* | Distributed training toolkit | 1 | 4 |
| 5 | I++ | M* | Model vending toolkit | 2 | 6 |
| 6 | I++ | M* | Pre-training objectives, multi-task learning, multi-modal learning, knowledge graphs, and GNNs - creating the base models for AI Language Services science team. | 4 | 10 |
| 7 | I++ | M* | Model-size reduction strategies. | 2 | 12 |
| 8 | I++ | M* | Performing evaluations on benchmark tasks | 1 | 13 |
| M* 2022 Total HC | M* 2022 Total HC | M* 2022 Total HC | M* 2022 Total HC | M* 2022 Total HC | 13 |
| 9 | I++ | Bifrost | Applied Scientists | 5 | 18 |
| 10 | I++ | Bifrost | SDEs | 6 | 24 |
| 11 | I++ | Bifrost | SDM | 1 | 25 |
| 12 | I++ | Bifrost | PM | 1 | 26 |
| Bifrost 2022 Total HC | Bifrost 2022 Total HC | Bifrost 2022 Total HC | Bifrost 2022 Total HC | Bifrost 2022 Total HC | 13 |
| 13 | I++ | DeepEarth | 2 Applied Scientists, SDE & PM | 4 | 30 |
| DeepEarth 2022 Total HC | DeepEarth 2022 Total HC | DeepEarth 2022 Total HC | DeepEarth 2022 Total HC | DeepEarth 2022 Total HC | 4 |
| Big Bets 2022 Total HC | Big Bets 2022 Total HC | Big Bets 2022 Total HC | Big Bets 2022 Total HC | Big Bets 2022 Total HC | 30 |

Externally-Funded HC
| # | EF/EI | Funding Team | Project | Description | Total HC | Cumulative HC |
|---|---|---|---|---|---|---|
| 1 | Ext Flat | MLSL | Shanghai MLSL | Leading external customer engagements in China | 3 | 3 |
| 2 | Ext Flat | AI Verticals | Causality-based Products & Services | Developing root-cause quantification for time-series data | 2 | 5 |
| 3 | Ext Flat | Evidently | Causality-based Products & Services | Developing advanced A/B testing technologies | 4 | 9 |
| 4 | Ext Flat | SCOT | Causality-based Products & Services | Automating root-cause quantification in across the supply chain management system. | 1 | 10 |
| 5 | Ext Flat | Search | M5 | Accelerating GNN training | 3 | 13 |
| 6 | Ext Flat | Search | M5 | Model vending | 3 | 16 |
| Externally Funded Flat 2022 HC | Externally Funded Flat 2022 HC | Externally Funded Flat 2022 HC | Externally Funded Flat 2022 HC | Externally Funded Flat 2022 HC | Externally Funded Flat 2022 HC | 16 |
| 7 | Ext Inc | SageMaker | AutoGluon | Release the models to SageMaker JumpStart | 2 | 18 |
| 8 | Ext Inc | CPP | AutoGluon | Supporting the Amazon Classification and Policy Platform (CPP) team for matching tasks | 3 | 21 |
| 9 | Ext Inc | Cetral Econ | AutoGluon | Supporting Central Economics team for logistics applications | 2 | 23 |
| 10 | Ext Inc | Neptune | NeptuneML | Building a suite of domain-specific GNN models to allow graph application developers to easily build accurate solutions for their domains | 4 | 27 |
| 11 | Ext Inc | Centrally funded | MLU | Modularizing course content | 3 | 30 |
| 12 | Ext Inc | Centrally funded | MLU | New case study course designed for customers in decision maker roles | 1 | 31 |
| Externally Requested 2022 Incremental HC | Externally Requested 2022 Incremental HC | Externally Requested 2022 Incremental HC | Externally Requested 2022 Incremental HC | Externally Requested 2022 Incremental HC | Externally Requested 2022 Incremental HC | 15 |
| Externally Funded 2022 Total HC | Externally Funded 2022 Total HC | Externally Funded 2022 Total HC | Externally Funded 2022 Total HC | Externally Funded 2022 Total HC | Externally Funded 2022 Total HC | 31 |

Scholar & Consultant HC
| # | Name | University | Research Area | VP Org |
|---|---|---|---|---|
| 1 | Pratik Chaudhari | UPenn | Reinforcement Learning | Alex Smola |
| 2 | Thomas Brox | U Freiburg | Computer Vision | Alex Smola |
| 3 | Ryan Tibshirani | CMU | Statistics & Machine Learning | Alex Smola |
| 4 | Anshumali Shrivastava | Rice | Efficient Algorithms | Alex Smola |
| 5 | Andrew Wilson | NYU | Bayesian Deep Learning & Gaussian Processes | Alex Smola |
| 6 | He He | NYU | NLP | Alex Smola |
| 7 | Bernt Schiele | MPI | Computer Vision | Alex Smola |
| 8 | Christos Faloutsos | CMU | Graph Mining, Anomaly Detection, Time Series | Alex Smola |
| 9 | Yu-Xiang Wang (pending June start) | UC Santa Barbara | Differential Privacy | Alex Smola |
| 10 | Xavier Bresson (pending June start) | U of Singapore | Graph Research | Alex Smola |
| 11 | Zachary Lipton (Consultant) | CMU | Dive into Deep Learning / Statistics | Alex Smola |
| 12 | Zengfeng Huang (Consultant) | Fudan University | Graph Research | Alex Smola |
| 13 | Xipeng Qiu (Consultant) | Fudan University | Graph Research | Alex Smola |
| 14 | Junchi Yan (Consultant) | SJTU | Graph Research | Alex Smola |
| 15 | Weinan Zhang (Consultant) | SJTU | Graph Research | Alex Smola |
| 16 | Matt Lease | UT Austin | Information Retrieval, Human Computation, NLP | Bratin Saha |
| 17 | Sanjiv Das | Santa Clara U | FinTech ML | Bratin Saha |
| 18 | Alexandra Chouldechova | CMU | Algorithmic Fairness | Michelle Lee |
| 19 | Aaron Roth | UPenn | Differential Privacy | Michelle Lee |
| 20 | Smaranda Muresan | Columbia | NLP | Stefano Soatto |
| 21 | Zhuowen Tu | UC San Diego | Computer Vision | Stefano Soatto |
| 22 | Kathleen McKeown | Columbia | NLP | Stefano Soatto |
| 23 | Paolo Favaro | U Bern | Computer Vision | Stefano Soatto |
| 24 | Charless Fowlkes | UC Irvine | Computer Vision | Stefano Soatto |
| 25 | Michael Kearns | UPenn | Fairness, Algorithmic Game Theory | Swami Sivasubramanian |
|  | + 5 to be hired by 2021 EOY |  |  |  |

Appendix 2: FAQs
FAQ 1. What are your disruptive ideas?
M* (pronounced “M-Star”) is an internal MLservice generating semantic representations for use across AWS HLT products. M* maintains a family of language models focusing on text and language-related tasks and addresses an unmet need for scalable production of sophisticated pre-trained models for AWS.
Particular makes it easy to quickly identify root causes for acute product-related issues in high-volume, highly automated manufacturing settings. It also allows to perform virtual trials that quantify the impact of manufacturing process parameters on product quality and yield. By employing causal MLtechniques, Particular goes beyond mere correlational analysis and provides actionable insights based on the cause-effect relationships underlying each specific manufacturing setting. Leveraging these insights, manufacturing companies can drive continuous process improvements to reduce quality-related cost.
Bifrost is a service that makes it easy for companies to build a knowledge graph from all of their data. It automates the discovery and semantic linking of all relevant information across data repositories, regardless of data type and storage location e.g. cloud or on-premise. Bifrost processes different types of data including databases, documents, and images, and establishes relationships between existing bits of information using schema matching, natural language processing, optical character recognition, and other MLtechniques. Using these new connections, Bifrost indexes the information across data into a fully-managed, searchable enterprise knowledge graph (EKG) in the cloud. Bifrost makes the information available through a centralized interface that enables enterprise customers, analysts and scientists to search, manipulate, and analyze the data.
Stagehand is a set-and-forget batch AI annotation service for media data stored in a data lake, or AWS S3. Stagehand takes care of instance management, off-the-shelf and custom model management, data labeling organization. Target customers are companies with large amount of multi-media (e.g. Disney, Netflix, Sony, Amazon Prime Video, Amazon Photos), including images, archival footage, that can benefit from an in-depth analysis, annotation, and summary. The value proposition of Stagehand is cost reduction to a fraction of the on-demand API-based services (up to 100x) and the convenience of not having to think about storage and compute separately.
DeepEarth is a set of open-source, automatic ML tools, model zoo (in collaboration with Axis), community managed services, and fully managed AWS service APIs to accelerate technology development for Earth imagery, modeling, and environmental sustainability. DeepEarth provides ground-breaking hybrid physics and deep learning models developed in partnership with NOAA and NCAR that provide 10x more accuracy and efficiency than traditional HPC-based (High performance computing) simulations for weather, ocean and land forecasting. Customers can utilize a broad and comprehensive set of public environmental data available on AWS Open Data ASDI Registry to build, train, and analyze complex models at scale. We are targeting forecasting and insurance companies, commercial weather companies, and scientists working in climate research.
FAQ 2. What were your top misses and learnings? What are you most disappointed about in your area (dirty laundry)?
Achieving market fit as a research and education organization with no own product teams is a challenging task. We regularly interact with product teams who rather solve research problems on their own, instead of taking dependencies on breakthroughs from us. We need to invest more into building trust with the product organizations and have started the LSL teams to transition our research into products more effectively.
We own several open source initiatives and are especially successful with AutoGluon, Deep Graph Library and D2L. While we got excited about growing adoption and positive feedback from the open source community and internal teams taking dependencies on us (GlupnCV, GluonNLP, MXNet in particular), there doesn’t seem to be a clear funding mechanism that would allow us to grow the project. As a result, we currently have applied scientists who maintain OSS projects, which leads to a reduced scientific impact of our org. To date, we decided to consolidate our existing activities, and double-down on multi-modal AutoML and ramp up our scientific impact again. However, without dedicated engineering resources to further and maintain the open source projects, the internal AWS/Amazon community, as well as our external community relying on our OSS projects will suffer.
FAQ 3. What “dogs not barking” worry you?
Multimodal data modeling: Real data is structured and contains many different modalities (e.g. a product listing has multiple images, reviews, reviewers, prices, ratings, product description, size, etc.) that are often interrelated. Custom modeling of these structures is difficult and error prone. It also requires highly trained engineers and scientists. We aim to automate this via AutoGluon as we believe that Amazon is in a good position to do this if we tackle it now (as opposed to when the academic hype has happened).
Large model training/serving/distillation/vending: Many customers want to use and benefit from large scale language models. However, most of them lack the technical expertise to correctly pre-train, fine-tune, and distill them. We proposed M* as an internal large-model vending toolkit and family of models. What we might be missing out on is creating an external  service. For example, customers can bring their own data for pre-training or use/combine with public-data, select an existing large model architecture and size, specify the down-stream tasks that they are interested in and fine-tuning data, and provide information about inference speed and accuracy trade-offs. The service will then take over.
In the M5 project, we see that pre-training medium-to-large-size LM models (.5B, 1.5B, 5B) on our own data leads to significant improvements; thus, there is a benefit over using an existing pre-trained model. The training infrastructure allows us to pre-train a 1.5B parameter model at $3.3K and a 5B model at $18.5K (16 p4d, CDO pricing). The fine-tuning and distillation costs will be no more than the pre-training. With such a service/cost, we will significantly expand the number of customers benefitting from large language models.
FAQ 4. What were your positive surprises over the last year?  How are you doubling down on them, and what are the specific actions and programs that evolved from these positive surprises?
On the education side, we were surprised by the impact of small bite-sized ML videos in China (over 30k subscribers in a matter of 2 months). We realized that the short video format works better is something that the audience enjoys and are planning to create more similar content. D2L community also surprised us, as they took it onto itself to create a Turkish edition of D2L.
We were also pleasantly surprised by AutoGluon’s internal and external popularity. In prediction competitions vs human data scientists, in both internal and external ML competitions, AG won 7 competitions. We are increasing our investment in AutoGluon to improve training and ease of deployment. We also plan to integrate it with SageMaker.
GNNs had a surprising adoption across science in many fields, e.g. neural science, chip layout, civil engineering, speech etc. DGL’s industrial adoption also exceeds our expectation. We are doubling-down by keeping our investment into the graph research and exploring PoCs with external customers.
FAQ 5. What did you commit to accomplishing in 2021 and what is the status of those initiatives?
MXNet: We will begin the graduation process in Apache.org (currently in incubator state) with MXNet 2.0 to let the MXNet open source community take over stewardship of MXNet.
Update: We released 1.9.0 and 2.0.0 alpha versions. We also resolved the legal and branding issues that were blocking graduation, which is expected in December 2021.
Meta: In 2021, we will launch Meta to accelerate 20 model families, including CNNs, RCNNs, and Transformers, on AWS CPU/GPU/ASICs which covers 80% of our top 20 customers’ workloads.
Update: We are on track to compile 20 popular models from TorchVision and HuggingFace, including image classification, object detection, and language models, to run efficiently on AWS hardware, with a prioritized focus on Trainium, followed by GPU. By the time of Trainium GA (03/2022), Trainium will use Meta to process 50% of its supported models. On the GPU side, by the end of 2021, Meta-processed models will cover 80% of our top 20 customers’ workloads, and the Meta-compiled performance is expected to be 20% faster than the XLA counterpart.
AutoGluon: In 2021 we will develop AutoGluon into an AutoML library capable of recognizing and processing mixed data (images, text, tabular & time series), advanced label types (e.g. segmentation, annotation) as well as a strong tuning backend (HPO and Network Architecture Search). In addition, we will provide support to SageMaker and improve model coverage for GroundTruth, Forecast and Custom Labels for images and text.
Update: In 2020/2021 we added new model types supported by AutoGluon (XGBoost, fastai). We implemented multi-modal text+tabular and started work on adding vision modality. We implemented time series POC. In collaboration with SAs, we made AutoGluon available in AWS Marketplace and developed training materials to show how to train and deploy AG in SageMaker. The Berlin team made improvements to existing HPO algorithms including multi-fidelity hyper-band; though HPO functionality is in experimental mode.
GluonCV: In 2021, we will add new models based on customer requests (video, pose, detection, GANs, efficiency gains), and add support for PyTorch and TensorFlow. In addition, we will develop autoML and semi-supervised learning capabilities to make modeling and deployment easier for our customers.
Update: We have added the PyTorch models, including 40+ video action recognition models (in April, 6+ by the end of July), 2 detection + pose estimation models (in May), 5 efficient monodepth models (in Feb) and 1 fast MOT model (by the end of July). The goal of AutoML support functionality will be added by the end of June and semi-supervised (self-supervised) learning capabilities will be achieved by September.
GluonNLP: In 2021 we will add new models based on customer requests (BERT and GPT variants for very long context, efficient deployment, sequence models for code). We will add support for PyTorch and TensorFlow. For ease of modeling and deployment by our customers, we will invest in AutoML.
Update: Due to Huggingface's ease of use in the most popular frameworks and significant investment in fostering a community, the Huggingface ecosystem grew a large active user base that also contributes to it, and it's often the first place where researchers make their models available. Furthermore, SageMaker and Huggingface recently joined for a strategic collaboration to simplify and accelerate the adoption of the models Huggingface ecosystem offers. Based on these facts, a competing solution of Huggingface Transformers will take prohibitively high investment from this organization to outpace the Huggingface community. Instead of making a competing OSS offering for PyTorch, we intend to explore a differentiating position with long-term sustainable advantage over other offerings through making research contributions first. While we believe that GluonNLP has advantage in flexible API design and implementation performance, making an equivalent offering available in PyTorch puts us in a competing position with other libraries with large communities, which requires order of magnitude more investment to sustain and overtake. As an alternative, we make use of these advantages by using them in our own research and boost our productivity in research efforts. Currently, we have PyTorch implementation of selected GluonNLP APIs in M5 Transformers and CoDEL.
GluonTS: In 2021, we will significantly increase feature coverage to address the needs of Thundera, Thor, Hubble and to provide missing functionality for Poirot and Capstone. This includes models for intermittent and periodical data, multivariate data, and marked point processes, as frequently requested by the services it supports. We will also support PyTorch and add AutoML capabilities.
Update: We have made a decision to de-invest in GluonTS in January 2021.
Kokoyi: In 2021, we will develop Kokoyi on PyTorch, develop a low-level IR backend using Meta, and deliver robust debugging.
Update: Kokoyi will have internal alpha release in mid-July, and a public release two months after. It streamlines DL model development with a LaTeX-like interface that maps to math formulation, thereby closes the gap between math and code.
Causality-based ML: We committed to 1) Deliver a SW library (codenamed Causal Nexus) which enables customer teams to perform causality-based ML. 2) Deliver root cause analysis (RCA) for Lookout for Metrics. 3) Deliver causal reasoning for SCOT. 4) Deliver a PoC for RCA in manufacturing with VW and, if successful, propose a PRFAQ for the solution.
Update: 1) On track: Causality Library is available on Brazil. It is mainly used by the SCOT team and us, but also by ElasticSearch. Using Causality Library requires users to have a causal graph of their business problem which limits adoption by non-experts. 2) On track: We delivered a solution for intra-metric RCA into production with Lookout for Metrics since 12/2020. It currently has a precision of ~89% and a recall of ~44%. We are working on getting more ground truth data and new algorithms to improve performance but also scope (to cross-metric analysis). 3) On track: We delivered cause-effect analysis to the SCOT team (in production since 5/2021) which lead to a 100x efficiency increase for root-cause analyses. 4) On track: PoC with Volkswagen is ongoing with promising intermediate results and customer feedback. Activity resulted in PRFAQ Particular.
DGL: Our planned releases in 2021 include distributed GNN training (CPU, GPU), temporal graph support, incremental model updating (key for social data), AutoML tuning for GNNs, support for higher order GNNs, and non-message passing approaches.
Update: For 2021, we commit to 1) improve ease of use, performance/scalability, documentation, and other activities for community adoption; 2) integration with AWS/Amazon services, and 3) robust models and model zoo coverage with community engagement.
Update: We have made 3 releases YTD. With respect to 1), we improved documentations over all releases. DGL scales to multi-machine multi-GPUs and performs significantly better than PyG on complex models. We started a DGL user group, growing from 300 to 800 in three months. For 2), we have onboarded 15 Amazon/AWS teams, launched Neptune ML in re:invent'20. For 3), we have +90 models in our model zoo, and developed novel robust and deep GNN models as well as new sampling algorithms to deal with large graphs. We plan to release 0.7 in 7/12 and are on target for 1.0 EoY. Features include: distributed GNN training (CPU, GPU), temporal graph support, incremental model updating, AutoML tuning for GNNs, support for higher order GNNs, and non-message passing approaches.
Deep Graph Applications: In 2021, we will develop Graphlytics for graph databases in collaboration with Neptune.
Update: NeptuneML was launched at re:Invent 2020. We are working towards the Q2 release of Neptune ML’s next version. We have finished the development of the new features including: (i) edge classification and regression; (ii) incremental model training and offline inductive inference; (iii) early-stop for model training and hyper-parameter searching; and (iv) auto feature engineering and the improvements including: (a) optimizing the RGCN model training speed on multi-GPU machines (the profiling results using the OGB-MAG benchmark shows a speedup of 2.4X on a g4dn.12xlarge instance and 2.7X on a p3.16xlarge instance over the previous version); (b) optimizing the user experience by providing easier-to-understand data processing configuration and better inference input/output contract; and (c) setting up the full CI/CD and regression pipeline. Neptune ML is going to graduate from Lab mode in the Q2 release.
Education: Integrate the components of D2L and MLU into an open-source academic curriculum that is made available to everyone.
Update: MLU's YouTube Channel enjoyed a splashy launch in 2020 earning 63.4k subscribers in nine months, and we released one new course (Decision Trees & Ensemble Methods) in 2021.
D2L: We will expand framework coverage from MXNet to a wider array of ML frameworks. In addition, we will publish an English-language version of the D2L book, update the Chinese translation, publish an update to the Chinese textbook and support further community translations. Last, we will transition the development model to a more community-driven approach in which new chapters are contributed by leading scientists in their respective fields.
Update: D2L is on track to publish v1.0 in English and a translated Chinese version, with added topics such as attention and modern NLP. We also engage with internal teams (MLU, PMM, Seychelles) to make AI more accessible for Amazonians and AWS customers. The PyTorch implementation for Vol.1 chapters is added. In April, we finalized 11 chapters of the English D2L version. The Chinese version released v2.0.0-alpha0 (8 chapters with 51 sections complete). The Chinese GitHub repo (d2l-zh) added +3.3k stars in the past 2 months. The Chinese version of the D2L website attracted 50k monthly users in April (vs 18k in February), the English version hit 64k monthly users (vs 54k in February). Our success in China is largely explained by the Chinese D2L MOOC that has attracted 33k followers within 1 month of its debut.
MLU: Train 35k people (or roughly half of the technical employee population of Amazon) by the end of 2021.
Update: 35k Amazonians complete one MLU course. We are green with ~4k completions YTD (6/1/20) and a planned surge for 2H.
FAQ 6. Are there any named programs in your area that don't have a single-threaded leader? What are they?
We began a reorganization of our team to better align researchers to single-threaded owners regardless of geographic location. For example, The Lablets and their research agenda are now headed by George Karypis. We have also realigned GluonNLP under George to unify research in support of Vector, M5 and M* projects. Potential candidate for no single-threaded ownership might be the CV Research that is split across 3 locations and has a complex reporting structure.
FAQ 7. What are your plans to grow outside of Seattle/Bellevue (i.e., HQ2, virtual, etc.)? Please describe if/how you are planning to staff resources outside of the Seattle/Bellevue HQ region over the next three years.
We already have teams in Tubingen, Germany and Shanghai, China. We plan for those teams to grow proportionally with the rest of our organization through local hiring. Additionally, we also have some presence on the East Coast through Scholar hires.
Appendix 3: Lablets Operational Framework
In this document, we lay out an operational framework for Lablets. We discuss how to foster impactful long-term research while at the same time transitioning scientific breakthroughs into AWS products and services.
How is the lablet’s research organized?
Lablets focus on long-term research projects that require a lot more exploration before they can be applied to solve customer problems. These research projects represent broad research initiatives with a strategic relevance to AWS and Amazon. For each of our Lablets, we ask for a research team that includes 1 science manager, 1 EA, 10 AS/RS, 2-3 SDEs, and several Amazon Scholars/Amazon Visiting Academics. The number of research projects addressed by this team is limited to 2-3 to ensure substantial and concentrated work (several Lablets may collaborate on a research topic). Proposals for new lablet research projects are reviewed for their scientific merit and future product/service impact by science and business leadership. Not all proposals need to be accepted, and accepted proposals can be revised in response to leadership feedback. The initial scope of a lablet research topic can be up to 4 years. Any extensions to the project will be granted only after a thorough review and weighed against other project proposals on the team’s plate. Each research project has an associated science lead. The science lead is expected to maintain a research roadmap with well-defined intermediate results (we should expect the first product ideas after 12-18 months) and to continuously adapt the roadmap based on recent learnings. The teams organize through regular planning & review sessions synchronized with the science MBRs. They also perform yearly reviews with senior leadership at a dedicated offsite to ensure progress of the research is aligned with Amazon needs. The yearly reviews are an opportunity to communicate successes, align on course-corrections, double-down on successful research, or terminate under-performing research.
How is the lablet’s research validated for solving customer problems?
Even though the duration of a lablet’s research topic can span several years, we want to continuously profit from the growing expertise and intermediary results the lablet scientists produce following their roadmaps. Technology transitions are the vehicle of creating short- to mid-term customer impact based on the scientific breakthroughs. To make the technology transition mechanism a success, we must look at lablet research artifacts from a customer perspective. For this purpose, each lablet has a Lablet Solutions Lab (LSL) team, consisting of 2-3 AS/SDE and a PM. The LSL team is close to the lablet’s research activities and deeply understands its potential. Its mission is to work closely with customer facing organizations like SA, MLSL, PMs from other organizations, and customer teams to propose and prototype solutions to customer problems leveraging lablet research. To deliver on its mission, the LSL team acts like a specialized MLSL team to develop small PoCs with representative customers which take 1-2 months to build. In addition, the LSL team is responsible for producing PRFAQs with the goal of starting technology transition projects for the most promising solutions.
How is a lablet’s research transitioned to new products and services?
Successful LSL engagements lead to technology transition projects. These working-backwards projects, which either result in new services/products or features for existing services/products, receive their HC through an explicit funding (e.g., a PRFAQ). This funding comes from partner orgs or AWS AI/ML and is project specific and may even include its own PM. After a technology transition project is funded its technology transition team takes over ownership of the project. This team can expect lightweight support from the original lablet and LSL teams, for example a weekly 1o1 with the science lead. Heavy support from the original lablet team is only provided in deeply problematic situations, for example in situations in which the core science behind a project is challenged. Typically, technology transition teams will reside in the product organizations which will own the product/service or feature. However, a product organization may opt to fund HC in the lablet’s organization to own the applied science for the new product/service or feature because such teams can be closer to the original lablet team which created the fundamental research results.
What is the ownership model of lablet research and technology transitions?
Even though there is a separation of concerns between the lablet teams (research and LSL teams) and technology transition teams, the proposed ownership models are overlapping so that there are no predetermined breaking points, see the illustration below. The lablet team owns the research agenda, proposing and executing research, ensuring that there are intermediate results which allow to build strong PRFAQs which are approved. After a technology transition is started, the lablet research team still supports in the case that unexpected challenges manifest. The ownership of the PMs also does not end with the PRFAQ getting approved but instead includes the technology transition until the customer problem is solved. The technology transition team (which includes the leadership and partner organizations which approve the PRFAQ) is responsible for a thorough review of the PRFAQ, makes a business decision which results in either the approval or the rejection of the PRFAQ and then owns the actual technology transition.
What is the role of open-/closed-source software toolkits?
Software toolkits are used to disseminate a lablet’s research. The toolkits may be periodically enhanced to incorporate new lablet research results, e.g., as papers with code. The toolkits are also important for LSL engagements as they can be the basis to create demos and PoCs. Because the Lablets’ focus is on long-term research, it cannot devote a large portion of its resources to support, maintain, and continuously enhance software toolkits. For this reason, when a lablet decides to release a toolkit as an AWS OSS, it will do so under the Amazon Web Services - Labs · GitHub or AWS Samples · GitHub organizations, which are the github organizations that AWS uses to signal progressively less support. If a toolkit requires ongoing support and enhancement because it caters to the needs of some products/services or customers, that effort has to be justified through a successful PRFAQ. This way, the effort can receive a dedicated funding, effectively creating a technology transition team for that toolkit. We propose to also use the technology transition mechanism to fund the continuous support, enhancement and community development of OSS toolkits and their potential graduation in the supported area (Amazon Web Services · GitHub) of AWS’ github presence. In the corresponding PRFAQ, the achievable impact on AWS/Amazon thought leadership simply replaces the usual business metrics.
How do lablet & LSL teams measure their impact (lablet team targets)?
The success of the lablet teams is measured by two metrics: 1) the number of top-tier publications, and 2) the number of products and features that have launched.
How do technology transition teams measure their impact (technology transition team targets)?
Performance of technology transition teams is measured just like the performance of the product teams they are cooperating with, using standard Amazon mechanisms.
Appendix 4: DGL-OSS Graduation
The Deep Graph Library (DGL) was started as a research & development project on graph neural networks (GNNs). In 3 years, it has become a leading platform for (GNN) researchers and application developers. Furthermore, the DGL team released SageMaker DGL in 2019, and the DGL-based Neptune ML Service in 2020. DGL saw Amazon-wide adoption including 15 AWS/Amazon teams taking a dependency in both R&D and in production. There are also 20+ external customers actively using DGL today, including key AWS customer accounts. From day one, DGL team has been staffed with mostly Lablet researchers. It is now becoming clear that the current team structure and goals must adapt to a new industry trend — an accelerated growth in GNN adoption. This document lays out our “graduation plan” with a deeper commercialization strategy for DGL.
DGL: history, milestones and adoption status
Deep learning on graphs. Learning from graphs and relational data is increasingly more important in applications including social network analysis, marketing, e-commerce, information retrieval, knowledge modeling, medical and biological sciences, engineering, and others. In the last few years, Graph Neural Networks (GNNs) have emerged as a promising new learning framework capable of bringing the power of deep representation learning to graphs and relational data. In 4 years, GNN related research publications grew 600% and spread quickly in all ML domains. GNNs replaced conventional graph algorithms (such as page rank, centrality, and label propagation) by algorithms that can be learned. This has enabled GNNs to achieve state-of-the-art performance for problems such as fraud detection, link prediction, target-ligand binding activity prediction, knowledge-graph completion, product recommendations, and even finding error in software. Moreover, GNNs are purpose-built to explore information stored in relational and graph databases. This has made GNNs an important class of deep neural network model architectures with better applications for an increasing number of domains.
Project history. Implementing high-performant GNN models is complex due to the irregular nature of the underlying graphs; it requires multi-disciplinary expertise in high-performance, scalable sparse matrix/graph algorithms, and deep learning (DL). To simplify GNN model development, we built DGL to enable fast, efficient, and scalable model training and inferencing on a diverse set of hardware architectures. DGL also provides abstraction for (MXNet, PyTorch, and TensorFlow). The project was under the DMLC organization in spring of 2018, with contributors from NYU/NYU Shanghai. AWS Deep Engine Science team took leadership in late 2018, and made the first release at Neurips’18 in December. Since then, the project has gone through 7 major releases with an average release cycle of ~4 months. Important feature milestones include sampling for large graph training, heterogenous graph, TensorFlow-based backend support and documentations for users of all levels. Performance and scalability optimization went in tandem.
Breadth in Academic adoption. With 7.4k github stars, DGL is one of the most popular GNN framework in research community, second only to Python-only PyG (Pytorch Geometric). Our Y2Y growth of Google Citation is ~7x (to 300). GNN’s adoption in different research fields has been surprisingly wide-spread and diverse, as shown in the Q4’20 stats below (overall/app/app-others):
Depth in customer adoption. DGL’s customer engagement predates SageMaker DGL (re:invent’19) and Neptune ML (re:invent’20), and was accelerated by the launch of MLSL China team in spring of 2019. Altogether, the team engaged +30 customers and conducted 20+ PoCs with GNN+DGL. Four of them are in production, including key accounts such as RED(Through DGL, RED became top3 GCR users of SageMaker with $10k monthly consumption), ByteDance; three have been handed off to the ProServ team, such as Nike(Fraud applications), and Bio-Techne. The pie chart below shows that recommendation and fraud detection are the most popular application domains. In April 2021, AWS Shanghai AI Lab, MLSL China team, and GCR Solutions team jointly launched an end2end DGL fraud detection pipeline as a reusable solution; this is an important step towards scaling out DGL use cases to global customers.
Adoption by Amazon and AWS teams  We have been working with a number of AWS/Amazon internal teams, including A9, AWS Fraud Detector(AFD), Kindle, CodeGuru, CTPS, Rekognition. Recent survey from internal teams, we tracked 4 large projects currently in production, there are 6 additional projects under active development, and 7 additional projects in research stage. There are dozens of smaller projects on our internal #dgl slack channel that we are not tracking due to resource limitations. During re:Invent 2020, we launched Neptune ML, a collaboration with AWS Neptune. A new service brings GNN capabilities to AWS graph database customers is in the planning stage.
We have observed that more and more internal Amazon teams (e.g., Amazon search, CTPS, Amazon Fraud Detection) work on GNN and have deployed GNN models in production or are in the process of deploying GNNs in production. Some of the teams approached the DGL team and were eager to establish the collaboration. Due to the resource limit, we could not help each of the teams.
Graduating DGL from Lablet
DGL was started as a fresh new project. The first two years of the project were research focused, as we undertook a model-system co-design approach to research an optimal design for a new GNN framework. At the same time, we have actively explored opportunities to drive GNN adoption at its early days, both within Amazon/AWS teams and  AWS’ customers. As more industry users are starting to adopt GNNs, we are seeing an accelerated growth in investments from both startups and enterprises. We saw 60% of Nvidia’s enterprise GNN customers list chose DGL for their GNN workloads. Moreover, the size of these GNN workloads are getting much larger, MeiTuan (A larger version of Uber Eats of China) recently revealed they are deploying graphs that are 10s of Billions in size using DGL at an invited talk. Our recent experience in creating a monthly GNN user meetup has confirmed the industry trend. The user group grew from 300 to over 800 in less than 4 months; much faster than anticipated. We received customer inquires on DGL and NeptuneML from user group members that otherwise has no easy way of reaching us through traditional AWS sales channel. We saw active participation from startup customers including, RED (Book), Outreach (using DGL for scaling customer chat App, tried NeptuneML), Katanagraph (strong commitment by CTO to use DGL as their default GNN engine; partnership with AWS on SageMaker), TigerGraph(SageMaker integration Partnership), Graphistry(CTO attened every meeting), Atomwise (DGL-life-science user), Flammatrix (Exploring Genomics application), and more. There are a dozen or so large enterprise customers that attend the user group meeting regularly including ByteDance (Default Graph Engine for recommender), Uber (Using DGL extensively for Fraud detection), Grab, Nike, Tesla, BoA, Paypal, JPMC, and others. With strong interest in GNNs, this is an opportunity for DGL to refocus on industry customer opportunities now.
After the 1.0 release, DGL will reach a status that we should consider it as ready for the Lablet graduation program. There are also multiple Amazon/AWS services and products (e.g., Neptune ML, SageMaker DLC, AFD, A9, CTPS, Rek) that are already taking a dependency on DGL. These teams believe that DGL provides advantage and differentiation for AWS AI and Amazon internal services and products. We propose to graduate DGL out of Lablet and staff it with a minimal HC of 5 (4 SDE, 1 AS) to refocus DGL towards serving internal and global industry customers that productionize their workloads on AWS. Instead of research priorities, the new team will add enterprise customer requested feature, deliver critical patches and bug fixes, improve usability, actively manage developer community for enterprise customer opportunities, and bootstrap additional resources from internal teams by building stronger relationships first. We believe DGL will emerge as a customer focused GNN library to maintain its industry lead. The project will share a PMT out of Lablet Solution Team.
Another area of focus is on building strong partners to scale our market reach. GNN is still a specialized Deep Learning, where customer acquisition can be expensive. We would like to also focus on partners that bring their own customers to AWS through deep integrations with DGL and SageMaker. Two Such partnerships are already in the works for KatanaGraph and TigerGraph. These are two fast of the fastest growing startups in the areas of Graph analytics, whom are also committed to invest in DGL and focus on their customer’s GNN adoption to differentiate themselves from being just a Graph Analytics applications provider.
The new team will also work on integrating DGL into SageMaker, JumpStart, DeepEarth and other AWS AI Services to provide sustained revenue boost to the services teams. With the future integration of JumpStart, DGL will also have the opportunity to serve vertical markets such as geospatial, bioinformatics, Knowledge Graphs, and other science communities.
While DGL will shift its weights to support Amazon/AWS teams, the open-source research community will continue to be its important customer. This is because GNN remains a vibrant research thrust with diverse new innovations, many of which will provide differentiation to existing and future AWS business. The responsibility of continuing optimizing the current state-of-art and identifying important future opportunities is core to GNN Research portfolio of the Lablet. DGL-LGP team will incorporate POCs from GNN Research and channel into DGL, thereby creating a flywheel of customer obssessed science.
Appendix 5: Causal Representation Learning Scientific Agenda
Project Description
Motivation: The standard workflow for successful applying machine learning starts by collecting the largest possible dataset, which should have exactly the same distribution as the intended use case: data must be independently and identically distributed (i.i.d.). After collecting this data, the next step is to manually annotate it and train or fine-tune a model minimizing some loss function over the training data. Unfortunately, the fact that a model achieves a satisfactory performance on the training and on a the validation set does not imply that it will work well in practice: even minor changes in the application, such as modifications to the camera placement or lighting conditions, can significantly reduce model performance. This is due to a fundamental theoretical tension between i.i.d. behavior and downstream performance, for example, when the data collection process of the downstream customer exhibits a selection bias. In these cases, the i.i.d. predictor will necessarily [d’Amour et al., 2020, Caruana et al., 2015, Arjovsky 2019] rely on spurious association and result in unexpected behavior. This is particularly problematic in the few-shot learning setting. For illustration, we collected some example problems of i.i.d.-based approaches:
Example problem 1: The work of [Yue et al., 2020, Fig 2] found that the better a visual backbone is performing, the more it is capable of exploiting spurious features such as the color of the grass to classify Lion vs African dog in the fine-tuning dataset.
Example problem 2: Objectnet is an object-recognition test dataset, with objects collected from unusual view-points. On this dataset, the performance of state-of-the-art neural networks trained on ImageNet, such as ResNet-152, Inception-V4, PNASNet-5L, drop by up to 45%. [Borji 2020] found that this performance drop can largely be attributed to more crowded backgrounds and objects appearing smaller than in the training set. Cropping augmentations at test time recovers performance by 30%.
Example problem 3: Another more subtle example is the impact of socio-economic factors on the accuracy of a detection system for diabetic retinopathy. Google Health claimed an accuracy higher than 90% (comparable to a human specialist), but was not able to produce predictions for images with suboptimal cameras and lighting conditions, disproportionately impacting clinics in poorer regions. This was extremely frustrating for customers as it required a second appointment at a different clinic even though nurses could clearly see that the rejected scan showed no sign of disease. This resulted in significant negative press [MIT technology review, Techcrunch].
Example problem 4: Pietro Perona’s internal work on "Causal Analysis of Bias and Fairness" explicitly showed that some of the apparent racial bias exhibited by an AWS computer vision service could be attributed to distribution shift. Outside researchers created a benchmark for evaluating systems offered by multiple companies. They used pictures of African politicians as representative of the Black population, and the difference in hairstyles of African Americans compared to African politicians confused our systems.
Such failures are hard to avoid, erode customer trust and the credibility of machine learning as a whole.
At the same time, the underlying reasons for such failures are easy to understand a posteriori when we consider that AI approaches based on the i.i.d. assumption miss out on two key aspects:
(i) They do not take advantage of the additional information in the dependencies of non-i.i.d. data, such as semantics changing more slowly than the low-level features (a slight light change affects all pixels but does not affect the semantic, see example 3), motion information (which reveals objects as they move separately from others, see example 1 and 2), and the arrow of time (time only goes forward). While most deep learning approaches already incorporate invariances through artificial data augmentations, these augmentations have to be carefully designed for the task at hand.
(ii) They overall have less data available. Collecting i.i.d. data is more costly because it must be curated, for example, selecting few representative images from a set of very similar ones, making sure objects are well visible and that the images accurately cover the expected test time distribution with all its aspects like lighting conditions, backgrounds or possible vantage points.
With our research, we want to pick up what i.i.d.-based AI approaches miss out on by learning and leveraging invariances from available non-i.i.d. data. We develop models that are less brittle and can benefit from more data being available without the extra processing effort. To this end, we formulate the following moonshot goal:
Moonshot goal: By learning and exploiting invariances extracted from non-i.i.d. data like videos and data exhibiting distribution shifts and drifts, we target to improve reliability and credibility of representation learning on downstream tasks. Focusing on computer vision applications, our goal is matching performance of conventional models trained on i.i.d. data while improving over existing technology in 3 key areas. (1) Improved robustness to unseen variation, uncertainty estimates, and generalization to task-dependent domain shifts. (2) Better interpretability and fairness of results. (3) A 10x reduction in the number of training samples required to reach the same downstream metrics compared to supervised learning and improvement in downstream metrics compared to few-shot learning with the same number of samples. Our baselines are the state-of-the-art specialist models and model zoos trained with the i.i.d. paradigm on available i.i.d. data.
After 4 years, we have a model zoo that is more robust, explainable, and requires less data to transfer according to the criteria above. We have credible and reliable deep learning pipelines that do not fail unexpectedly.
To reach our moonshot goal, we organize our work into 3 thrusts. In this paragraph, we will describe these thrusts on a high level focusing on how they relate to each other. For details concerning the individual thrusts and their deliverables, please refer to the following paragraphs. Thrust 1: Develop broader research evaluation strategies that, for a given downstream task and model, provide actionable feedback on potential issues that may arise at test time facing a realistic change in distribution. This will enable our pursuit of a new generation of representation learning algorithms that are less sensitive to spurious correlations present in i.i.d. training sets, allowing us to evaluate these dependencies in a data-driven way. Thrust 2: Test the hypothesis that biases from non-i.i.d. data expose invariances that are useful when models are deployed in the real world. We design new architectures improving robustness, interpretability, and transferability. Thrust 3: Develop a framework that allows us to collect and process high dimensional interventional data from the real world with annotations describing causal relationships that we can use to train and validate models. Causal mechanisms are less likely to change than non-causal conditionals ("sparse mechanism shift hypothesis"). While Thrust 1 will allow us to measure our progress, Thrusts 2 and 3 are complementary approaches to reach our moonshot goal. Within one year, we will deliver our evaluation toolkit powered by our novel validation strategies. With the support of the LSL, we will investigate the business opportunities of our metrics for “model inspection as a service”, adapting our evaluation toolkit to enable customers to find models meeting their unique requirements. Further, we explore different learning strategies on non-i.i.d. data and formalize a notion of causality in image data, collecting a prototype dataset. Our findings will be summarized in 10+ research papers accepted or in submission at top AI conferences until Q4 2022. At the end of year 2, Thrust 2 and Thrust 3 should show state-of-the-art result on the metrics defined in Thrust 1. As we partner with other teams in Amazon (e.g. CL, MLSL) to collect the problems in Thrust 1, each time a model improves on the relevant metrics we will trigger a technology transition. We will do a retrospective on the most promising approach to reach our moonshot goal and double down on it in year 3. After three years, we expect significant improvements on the metrics of Thrust 1. At that point, we will double down on the breath of tasks we support, extending to more complex tasks. At this point, it is hard to predict what the architectural innovations will be as they will have to be based on past learnings from the team and the research community. However, we will continue to work towards our moonshot goal: learning models that are more robust, more interpretable and that transfer with fewer data, which we aim to achieve at the end of year 4.
Thrust 1: empirical analysis. Being able to answer "in which scenarios is this model expected to work?" and "is it reasonable to expect that model A exhibits behavior B, given its intended use case?" would have identified the problems illustrated in the examples we have given in the motivation paragraph before these problems could have caused damage. Accordingly, the scientific challenge of this thrust is to design a set of stress-tests for downstream benchmark datasets and propose evaluation metrics that allow us to quantitatively answer questions concerning model robustness on downstream problems in a data driven manner. The stress-tests will be designed to have these 4 properties: (1) They are as thorough as possible, e.g., using real customer data and anecdotes to cover all potential issues that are relevant; (2) The stress-tests are interactive, for example asking the customer for feedback to questions like: "On this image, which is not in the data you provided, would A be a better answer than B"?; (3) The stress-tests are instructive. Their results have to be actionable for both scientists and non-experts. (4) The stress-tests can be performed on all models in a zoo to identify which one is the best fit for the customer dataset depending on specific customer requirements. More details on the concrete approach can be found in the appendix Thrust 1: Empirical analysis Requirements and considered problems can be advised by partner science teams and are integrated through the Lablet Solutions Lab (LSL), making Thrust 1 a mechanism for technology transitions and prototype development. In particular, we are targeting collaborations with MLSL and Custom Labels through shared projects and co-supervision of interns. We will explore offering evaluation-as-a-service. In the first year, we will focus on building our evaluation toolkit, which we plan to complete in Q1 2022 (for ICML). We target to validate our results integrating problems and tasks from at least one science or service team (likely Custom Labels or ML Solutions). For example, we work towards replacing the notion of statistical confidence in a label with a notion that is robust to slight changes in the data distribution. Another example is identifying models that behave differently on out-of-distribution data, which can be ensembled to hedge our bets for a better uncertainty quantification. After the first year, we will review Thrust 1 in its ability to measure proxies of causal representation learning as described in the moonshot goal (being more robust, being more explainable, relying on fewer training examples). Given that this is successful, we will decide whether to extend the API built to enable more diverse tasks (beyond classification and detection) and add metrics as needed. Together with our LSL team, we also investigate offering the outcome of Thrust 1 as a “model inspection as a service”, for which we will assess the business need through our collaborations with partner teams established in the first year. After the first year, as the infrastructure work is finished, the headcount will move to thrust 2 and 3, but we continue to investigate the research questions on model validation via our internship program.
Thrust 2: models and architectures. This effort focuses on incorporating different inductive biases from non-i.i.d. data into state-of-the-art neural networks. We focus on novel modular architectures performing perceptual grouping and reasoning. Exploiting both i.i.d. and non-i.i.d. data, we facilitate informativeness and smoothness across distribution shifts of the learned representation. The representation should remain as informative as when being trained from i.i.d. data alone but also be able to handle natural changes that happen in real-world data. The outcome of this thrust is the creation of a small model zoo that achieves our moonshot goal. The model zoo must will be suited for both diverse downstream tasks and it will be robust against distribution shifts as studied in Thrust 1. While the best case scenario would be to find a single modular architecture with interpretable and robust features that is easily expandable, we fully expect a strong interplay between the downstream task, the different architectural biases, and the training data which is better reflected in a model zoo. In the first year, we will establish strong techniques to learn models that exploit non-iid data and are applied in non-iid settings. As the analysis tool from Thrust 1 becomes available after the first year, we will apply these methods to continuously build models that increase in the desired capabilities. This is an open-ended continuous hill-climbing effort, exploring a variety of approaches detailed in the appendix Thrust 2: Models and Architectures for Learning from Non-IID Data. Thrust 2 is considered successful if the model behavior as measured by Thrust 1 achieves state-of-the-art. This requires working on non-i.i.d. data such as video exploiting different biases, which is resource and engineering intensive. Being able to iterate quickly on ideas will require significant infrastructure and manpower which we aim to provide via our internship program and re-assigning headcount from Thrust 1. At the end of the second year, we will have a compelling prototype supporting the hypothesis that additional non-i.i.d. data is a viable solution to achieve our moonshot goal.
Thrust 3: causal ground-truth. The output of this thrust is a technique to automate the annotation of causal information across multiple sources, producing a large-scale dataset to improve representation learning state-of-the-art. Research in this thrust includes a mathematical formulation of interventional data in images, annotation techniques, and tools to merge causal information across smaller causal graphs to create larger knowledge-bases. We use the outcome of this thrust as an explicit supervision signal for the methods developed in Thrust 2, exploiting the property that causal relations are invariant to distribution shifts. It is high-risk and we may need several tries but the potential upshot is huge and would lead to models that are wildly surpassing “Thrust 2 models” as measured by Thrust 1. This may lead to a leap forward that influences all products where deep learning models are currently in operation. Thrust 3 requires significant model innovations through exploration, as no dataset of this kind exists and existing training principles may not apply directly, details on our approach are in the appendix Thrust 3: Generating Causal Ground-Truth by Merging Causal Knowledge in Real World Data. After the first year, we have formalized a notion of causality that is suitable to annotate visual data and an accompanying synthetic data set. We also develop a theory for merging causal datasets which we will use to collected real datasets. If needed to scale the data set collection and model training, we re-assign headcount from Thrust 1. After the second year, we have a compelling prototype supporting the hypothesis that models trained on data with causal ground-truth are a viable solution to achieve our moonshot goal. Thrust 3 is considered successful if the model behavior as measured by Thrust 1 achieves state-of-the-art. If this is not the case, we will work towards driving adoption from the community, for example organizing competitions.
Appendix 6: AutoGluon Roadmap
Background
AutoML is becoming a default tool for ML modeling, both in the open source community (scikit-learn, TPOT, FLAML, Torchmeta, AutoKeras, etc.) and commercially (Sagemaker Autopilot, Azure AutoML, GCP AutoML Tables/Language/Vision, H2O, MLJar, DataRobot, etc.). It allows novice data scientists and engineers who are often domain experts in their field to solve sophisticated modeling problems with little or even no ML knowledge. It also helps advanced users explore the design space of solutions more efficiently. AutoGluon is an open-source toolkit that enables users to train state-of-the-art models on raw images, text, and tabular data in as little as three lines of code. It hides the tedious technical details of hyper-parameter optimization (HPO) to the optimization for entirely new network architectures (NAS), data preprocessing, feature selection, and model compression (see this list for an overview) to users.
Current State
AutoGluon delivers out-of-the-box SOTA performance competitive with the best custom engineered statistical models. In prediction competitions vs human data scientists, AG ranks: 1st place in MachineHack - Product Sentiment Classification, 1st place in MachineHack - Predict the Data Scientists Salary, 1st place in Kaggle - California house sales competition, 2nd place in Kaggle - Mercari Price Suggestion Challenge, 2nd place in MachineHack - Predict the Price of Books, as well as in the top 1% in popular Kaggle competitions Otto Group Product Classification Challenge and BNP Paribas Cardif Claims Management.  Likewise, AG achieves a 3rd place against 970 MLU graduates on their graduation task, and it has been used in winning solutions of internal Amazon prediction competitions (AMLC 2020 Grand Challenge in MultiModal Tabular Data, Sagemaker Community Trust Hackathon). Compared to other open source AutoML tools, we have a 69 - 33 win rate against the closest top competitor (mljar), even though it’s inspired by AG and uses roughly the same models and adopted our stacking techniques. The rapid innovation speed of AG makes it win. Over the past year, AG pushed 9 new releases covering accuracy, speed, API and functionality improvements. In particular, the 0.1 release (2/21) contained a major API refactoring to make AG more modular and supported multi-modal models, the 0.2 release (4/21) reduces average inference latency by 5x and disk usage by 10x.
The AG team (3 ASs, 1 SDE and 0.5 PM) also focused on developing the user community on the past year. Internally, AG has been adopted CDO teams, the ML Solutions Lab, Project Gecko, Catalog, and MLU. A full list of customers can be found in Appendix C. The internal slack channel #autogluon-interest has 558 members. The recent MLU talks brought in 500 (12/20) and 1000 (3/21) live participants with the highest CSAT scores for 2020 and 2021 respectively. Externally, AG has 3.2k stars on GitHub (+50% YoY), 50k image pulls on DockerHub and 619k downloads on PyPi (68k in 5/21, +75% YoY) as of 5/21. We published a 3min video to show how to win the house sales Kaggle competition with AG, it received 610K views in 3 weeks. We are also targeting consulting firms with a large population of data scientists as they work with many Fortune 500 clients. Our talk at Boston Consulting Group’s (BCG) Gamma data science team had over 100 live participants, and was deemed the most engaging in the history of external invited talks at BCG with over 14 pages of Zoom chat log in just 45 minutes. McKinsey’s Quantum Black is also using AutoGluon. NVIDIA started integrating AutoGluon with RAPIDS (GPU version of Scikit-Learn) to make common algorithms faster (40x for XGBoost, 1000x for kNN). The results were presented at GTC 2021. NVIDIA promoted and added AutoGluon to their RAPIDS container as the main AutoML tool, allowing AG access to RAPIDS’ 200k user community. Intel is contributing DAAL support for CPU optimization.
Learnings
Appetite for automated convenient tools for other kinds of ML tasks (beyond standard classification/regression) is very high: time series, graphs, other modalities, etc. Once users become accustomed to the pleasant experience of using AutoGluon to replace their existing classification/regression models, they ask if we can add support for other kinds of ML needed for their applications.
Our and 3rd party benchmarks have demonstrated that AG beats all other incumbent tools, AutoML or not, in terms of accuracy. AG helped solving CPP’s statistical modeling problem: predicting on 88k classifiers, with 3k are on multi-modal Catalog data. The quantile estimator in AG halved the error rate compared to the one used by the Central Economics team when estimating a replacement index for logistics. The Gecko team achieved 1.8% error with AG Multi-modal (text-tabular) out of the box compared with RobertaFin2 Large models at 4.4% error which took months to build and tune. “AutoGluon became a valuable tool in the Brand Protection ML team due to its ease of use. Scientists of the team use AutoGluon to estimate what performance is possible to achieve with ensembles of models. Using AutoGluon allowed the team to rapidly test various ideas regarding how feature space should look like, and make informed decision about next steps” said Sergey Sokolov, Applied Scientist on the Brand Protection ML team.
In the Lorien research project, using ML to accelerate compiler performance, we found AG models are 6% better than models hand-crafted by senior AS in our team.
While AutoGluon has become one of the most widely used AutoML solutions, significant community engagement is required for AutoGluon to be seen as the go-to tool when solving practical ML problems. To increase adoption we will invest into training and evangelism, which is needed to get CDO and AWS teams to replace their traditional ML models with AG for better accuracy. This also applies to community uptake. We are also pursuing and exploring additional avenues to make AG easily accessible to AWS’ customers, e.g. via DLC containers, cloud.fit(), JumpStart, and via Seychelles. This will make it easier for customers to benefit from AG.
Strategy
We will focus on functional improvements and deployability. In particular, a) as the demand for multi-modal applications grows, we are expanding the spectrum of multi-modal features automatically handled by AG from text to tabular data, images, graphs, and time series. Most of this functionality is driven by pre-trained encoders for image embeddings (e.g. ResNet), language (e.g. BERT) and graphs. We will add GluonCV and GluonNLP representations and release the models to SageMaker JumpStart [F:6, E:2]; b) Ease of deployment remains a challenge. To address this we will add integrations with model servers. Moreover, we will add detection of covariate label and concept shift and drift, in some cases designing new statistical estimators, to catch data issues. We will add an incremental training mode for continuous deployment, improved model distillation, and cloud.fit() training for hybrid cloud modeling locally and on SageMaker [F:1, I:2, I:++2]; c) we will improve community support by creating more notebook examples, write a chapter for D2L and teach a class at Stanford [F:1, I:1];  d) we will work on easy task definition for customers, via many shot learning. This is the riskiest part of the plan since there isn’t much research in the area between one-to-few shot learning (less than 10 observations) where heuristics abound and regular fine-tuning / learning (1k and more observations). This research will allow customers to define new estimation problems on their own without the need for sophisticated objective function engineering [I:2]. e) lastly, we will support the Amazon CPP team for matching tasks [E:3] and Central Econ for logistics applications [E:2]. We plan on funding the multi-modal aspects of this work by merging the corresponding parts of the GluonCV [I:-2] and GluonNLP [I:-2] teams into AutoGluon.
Features and Roadmap
Multi-modal Support
The ability for AG to ingest data automatically without the need for manual data type detection is a key enabler for robust performance. AG auto-detects whether variables are categorical, numerical, text, currency, index keys, and to some extent, even the type of labels used for estimation (binary, multi-class, regression) and uses this information to find meaningful feature representations. While any defaults can be overridden by the user, it protects novices from poor modeling choices and makes data wrangling less of a chore (the default is zero effort). This level of automation leads to higher engineering velocity and more robust deployment when problems change.
We have learned from customers that their data is often highly structured and that in many cases the real data contains many tabular (metadata) features in addition to textual snippets (e.g. titles, abstracts, product reviews), images, relational data, and even time series. This is the case, for instance with catalog and inventory data. Consequently AutoML systems need to be able to a) detect, b) represent and c) fuse records. AG is currently the only fully-automated solution available in the market capable of multi-modal ML out-of-the-box but we still have a long way to go. To address a) detection, we need to improve the data detectors to ensure that important types (e.g. language & domain detection for text) are sufficiently accurate and match available pretrained models [F:1]. With regard to representations, b) AG currently focuses on classification and regression problems. Other offerings from GluonCV/NLP are not exposed in any way. We will invest to integrate and expand available models/tasks via AG APIs. We will merge the representation learning work in both projects with AutoGluon [F:4]. We will allow customers to bring their own embeddings or use proprietary models derived from M5 (provided that they have the appropriate privacy provisions) when training on AWS. c) Once data types are detected and represented, they need fusing so that models can exploit predictive interactions across modalities. At present our fusion algorithms are still quite simple (multi-tower fusion, fine-tuning and stacking). We will work on improved fusion techniques, e.g. via multi-modal transformers and efficient neural adapters to cover a wider range of inputs at a level of quality matching (or surpassing) that of custom models for multiple domains [F:1].
Once wrapped, this functionality will be available for zero-code training via SageMaker JumpStart. Specific areas of interest are the gaps in SM JumpStart: 1) CV: video tools and semantic/instance segmentation, pose estimation and action recognition applications; 2) NLP: named entity recognition, intent classification and slot labeling tasks [E:+2]. Please see Appendix B for 36 proposed Models for JumpStart Model Zoo.
Goals and Metrics: We will deliver on CV and NLP embedding integration into AG. This includes improved data detectors, access to the CV and NLP representations and coverage of major CV and NLP tasks. We will publish work on fusion and we will deliver on the SageMaker JumpStart integrations.
Deployment
Ease of deployment is a key concern for data scientists and engineers, especially our customers who intend to integrate AG into commercial software. This refers both to their ability to train the models easily on their own devices or in the cloud transparently, the ability to create reliable inference endpoints with ease, and just as importantly, to statistical tools for ensuring that drifts and shift in the data between training and test set are detected and mitigated. Moreover, the combination of these attributes will allow us to deliver what we call an anytime endpoint, that is, an endpoint that acts both as training and as inference service where customers receive predictions and where the models keep on improving over time as more data is added. The benefit of this is that it entirely removes the data collection, training and deployment cycle and replaces it with a simple deployment step.
Our goals are: 1) Implement SageMaker training integration via a Cloud Fit/Deploy API, to allow seamless model training and deployment in the cloud from laptops, Seychelles, or even from Google Colab. 2) AutoGluon integration with Spark/Glue/Airflow/MLflow to enable local processing in BigData application (EMR/Glue use cases);  3) integrate with AWS cloud services, such as SageMaker, Glue, EMR, DynamoDB, Aurora to improve their offerings and simplify the use of ML via seamless cloud training.
In recent customer conversations (Roivant, etc.), we noted an increasing number of data scientists struggling with their routine model training workflow. The models they are training are no longer fitting onto their laptops in both training time and data sizes. It take hours to test their models in a fully-deployed engineering ML pipeline, which they don’t have control over. We plan to support a hybrid cloud strategy by borrowing from an established metaphor in deep learning: the notion of a compute device that allows engineers to move computation from CPUs to GPUs. In the same manner we will add devices to our training and deployment functions. As such, training in the cloud becomes as simple as defining parameters for the cloud device and then invoking cloud.fit() on (SageMaker Training) rather than fit() on the local machine. In addition to convenience, this also opens opportunities where secure models training is needed: the model development is done with a public data subset, while the full model training is performed in isolated secure environment with a restricted access to full data. The same holds for inference, albeit also with mechanisms for building Triton serving endpoints (with the latter being supported by SageMaker we also address the issue of SageMaker Serving) [F:1, I:1]. Additional details on Cloud.fit() design can be found in Appendix A.
Secondly, efficiency in deployment is becoming increasingly relevant, in particular for high-throughput settings and for deployment on the edge. We will invest into distillation algorithms for complex multi-stack models as they are generated by AG, in particular for multi-modal data [I:1]. By converting models to purely deep network or forest models we take advantage of the deployment infrastructure offered by SageMaker Neo.
There are many cases where training and test data differ, albeit in many nontrivial ways. The simplest case is where there are well-defined training and test sets for which such a shift can be observed, be it in the covariates or in the labels (both situations require different mitigation strategies, none of which are currently available as part of AutoML systems). A more insidious situation arises when things change slowly (aka boiling the frog). It is common, e.g. where the prevalence of a disease changes slowly (label drift) or where the actuators of a car experience wear and tear (covariate shift). This requires novel statistical estimators to test and correct for such a situation proactively. We will develop these and integrate them into AG [I:1, I++:1]. This will make AG the first AutoML system that can be trusted not to fail catastrophically on outliers encountered during deployment, and will also allow us to obtain more meaningful explanations of models.
Lastly, the ability to stand up a set-and-forget prediction endpoint has promise to open ML to a much wider range of customers: consider the scenario where one sets up a prediction service that generates labels when queried, but one that also keeps on improving automatically over time as more (covariate, label) pairs are being provided. This can be used for many adaptive services, from automatic modeling in MTurk to service optimization for logistics and planning. In order for it to work robustly, it draws on the notion of compute devices (cloud.fit), the ability to detect input types automatically, the ability to deploy automatically, and the ability to detect shifts in data when they exist. [I++:1]. Additional functionality, e.g. aleatoric vs. epistemic uncertainty (i.e. whether uncertainty in prediction is due to insufficient data or noise) will depend on support from partner teams.
Goals and Metrics: We will deliver  cloud.fit() functionality, Triton server integration, and deliver on the science for drift detection. Our ability to deliver on automatic drift correction and on the anytime endpoint will depend on incremental funding (I:2 and I++1). Furthermore, to capture usage metrics and reduce steps needed to package AG for production use, we plan to make AG available via DLC framework containers available in all regions. The official images will be available in CPU and GPU-optimized variants. The build process will ensure security scanning requirements and OSS license compliance.
Outreach
While we are happy with the increase in community engagement that we have seen over the past year, we still have a long way to go to make AG the industry recognized #1 AutoML toolkit. This is particularly relevant since AG holds the performance crown in its class, yet it isn’t the most popular toolkit due to lack of dissemination and entrenched incumbents. The risk is that the latter may copy our most salient features and thus level up in terms of performance (e.g. MLJar adopted many of AG’s more advanced features). The mitigation strategy is to ensure AG reaches a sufficiently large installed industry user base that will start contributing many features on their own. While we’ve made some progress (54 contributors on GitHub), their contribution is heavily power-law distributed. Note that keeping AG open source is a key requirement for growing the user community and for participating in the academic dialogue.
In 2022 we will focus on lowering the bar for access as follows: a) we will increase the coverage of examples through notebooks and tutorials that can be made available, e.g. via Seychelles to allow for easy access by SageMaker customers; b) we will add a section on AutoML to the D2L project covering multiple aspects of deployment. This allows us to tap into the over 60k monthly users visiting the D2L site; c) we will produce/refresh teachable material that can be used in MLU (see MLOps Syllabus - V2 details) to provide easy access for Amazonians. The MLU MLOps class targets researchers and engineers, helping them to architect and operationalize ML systems from end to end. The syllabus covers data ingestion, model training and serving, and dives into designing scalable, adaptable and secure ML systems. d) We will teach a full quarter course in Stanford (15 lectures of 80 minutes each plus exams & projects). This course will be broadcast on YouTube and other learning channels. The motivation is to create a halo effect of content that will be picked up by other universities; e) we will work with the Amazon engineering community to make access to AG even easier, e.g. via custom scripts; f) we will organize user group meetings, either virtual or in-person (SF Bay Area, Seattle, New York) to increase the developer base; g) we will continue working with our partners in Intel, NVIDIA, Nvidia RAPIDS team, BCG, on platform optimization [F:1, I:1] and to increase the number of dedicated external contributors to AG.
Goals and Metrics: In 2022 we will double the number of GitHub stars to 6k, we will double the number of members for the Amazon AG slack channel, we will deploy AG in at least 10 additional use cases in Amazon internally and with at least 25 external customers (M or larger). Our in person presentations, and videos on AG should receive at least 50k views in aggregate and we will acquire at least two meaningful external contributors to AG.
Easy Task Definition
AutoML has simplified ML modeling significantly, as far as the ingest of data is concerned. That is, it allows users to perform a relatively narrowly defined range of important tasks (classification, regression, annotation, quantiles, etc.) on a wide range of possible datatypes or combinations thereof. We believe that the next step for AutoML is to extend its range to tasks that are significantly less well defined in terms of specific loss functions but rather through more intuitive means. At present this is already evident in the form of research on zero, one and few-shot learning in computer vision, or the use of plain-text task definitions, e.g. as illustrated in prompt generation for GPT-3 and T5. We are convinced that much practical progress can be made by extending this to many-shot applications and conditional generation.
Multimodal generation. Currently, AG is mainly used to predict numerical or categorical columns. We will extend AG to conditional generation of other modalities, including text, image, etc. For example, the customer can prepare the dataset that contains image, text, tabular data to the multi-modal tabular format proposed by AG. After that, the customer can ask AG to train a model that predicts the image column or the text column given the other modalities. This is also known as “controlled text generation”, “summarization” if we are predicting text, or “text to image synthesis” if we are predicting image. The multi-modal conditional generation interface in AG unifies these generation problems. Internally, AG will automatically utilize the state-of-the-art pre-trained generative models trained by M*. [I++:1]
Few-to-many shot learning. Generative models via prompt generation and heuristics perform well on tasks such as image retrieval and zero-shot classification. That said, there are many situations where the customer has more than just a handful of datapoints but not enough to train a model fully from scratch or even fine-tune it using only the encoder part of the model. We will investigate algorithms to extend them to realistic situations where customers want to improve models further beyond what few-shot learning can offer, e.g. by updating not only the state but also the parameters of the generative process. [I++1]
Goals and Metrics: This is by far the most speculative aspect of the project. If we succeed, it will significantly accelerate the custom modeling work across AWS AI’s custom label projects, both in engineering velocity and in terms of the range of problems we can tackle. We define success by performing the required research and by demonstrating in a sufficiently wide range of applications (vision tasks, NLP tasks, NLG tasks) that this is capable of producing state of the art results.
Risks and Blockers
The biggest risk with AG is community uptake. Since we are working with different communities (developers, scientists, users, internal and external teams) there is no single silver bullet. Instead, we work on lowering the bar for adoption (Brazil, PIP, tutorials, videos, hackathons), ensuring that we are visible in the community (publications, lectures, benchmarks), ensuring that we address customer requirements efficiently. A significant blocker with regard to adoption is the fact that AG’s design is very different from Autopilot (in fact, the difference in design is where it derives much of its performance benefits and engineering velocity). We will work with the Autopilot team to see how to bridge this gap efficiently to allow the Autopilot team to take advantage of the features added to AG.
AutoGluon is open source project. If customers needs are not addressed in an efficient manner, this creates an opportunity for non-AWS commercial products to be built on top of it, monetizing commercial support avenues outside of AWS. This risk can be mitigated by having AWS provide the best available commercial cloud offering around AutoGluon (similar to the commercial open-source model successfully used by DataBricks, MongoDB, H2O, RedHat, etc.), such that it is not worth a competitor’s effort to purse this (unlike Amazon teams, competitors will not have direct access/collaboration with AG developers/roadmap).
Another risk is the state-of-the-art in ML fundamentally changing such that AutoGluon is no longer competitive with top data scientists. This risk is actually mitigated by AutoGluon’s current design: it is open-source without the engineering overheads/complexity of a fully managed service, meaning AG’s codebase can be rapidly updated and the external community may even contribute new advanced models shortly after their introduction. Furthermore, it is easy to add a  new model and improve AutoGluon’s overall performance, as AG is highly modular in its AutoML design (heterogeneous models are robustly trained with failure tolerance in individual models, and subsequently ensembled in a manner that outperforms any individual model).
Appendix 7: Meta Roadmap
Training and inferencing Deep Learning Models is commonly carried out in Python. While convenient for data scientists, Python suffers from many drawbacks such as lack of multithreading (the dreaded Global Interpeter Lock). It also makes scheduling of competing resources, fusion of operations and similar optimizations difficult. Runtime systems and compilers can be used to alleviate this problem. By now all major Deep Learning frameworks use some form of imperative modeling (PyTorch default, MXNet Gluon, TensorFlow Eager), supported by an engine that optimizes translation to various compute devices.
In order to execute code efficiently on Amazon’s own hardware (Inferentia and the upcoming Trainium chip) it is necessary to take the instructions and datastructures generated by DL frameworks and map them to the operations that these chips are capable of performing. This works by a sequence of lowering steps, e.g., from TorchScript in PyTorch. Meta aims at the holistic manipulation of the deep learning model. It jointly optimizes the graph-level and tensor-level of a model. For example, Meta considers operator differentiation together with the operator fusion and code generation. It also supports dispatching kernels to different implementations accordingly, e.g., vendor libraries, TVM generated code, Penguin (Neuron compiler for Trainium and Inferentia) generated code, etc. To the best of our knowledge, this is the first training compiler that thoroughly takes into consideration of every possible factor of optimization.
Background & Current State
The Meta team works with Annapurna Labs to build the Neuron compiler stack for the Trainium chip, particularly for PyTorch/MXNet workloads (there is a separate XLA to Penguin path that addresses much of TensorFlow). By now Meta compiles popular CV and language models defined in PyTorch’s TorchVision and HuggingFace model zoos all the way to the Trainium simulator. To be more specific, Meta is now able to convert MLP, LeNet, ResNet, DenseNet, MobileNet, VGG, SqueezeNet, BERT, GPT-2 and M5 1.5B model and run end-to-end training on CPU & GPU. We have made progress on Mask-RCNN in terms of 1) operator support, 2) dynamic workload support on Meta VM (runtime) and 3) automatic-differentiation support for control-flows. We are now able to run Mask-RCNN inference on CPU & GPU and the work on training is in-progress. As the ability to convert Pytorch models be the prerequisite of supporting Trainium, we now have MLP and LeNet training work end-to-end from Torchvision models to Trainium simulator. ResNet and BERT inference on Trainium has been working and the support for end-to-end training is in progress. The major difference between GPU and Trainium support is the Penguin compilation flow and we are working closely with Trainium team improve the end-to-end workflow. M5 and Mask-RCNN is on the list for Trainium in Q3 2021.
The benefit of using TorchScript as starting point, as opposed to lowering it to XLA first is that there’s a much larger space for optimization and less need for low-level operator implementation (this is tempered by the engineering risk of having to support a separate lowering path for PyTorch). As such Meta provides the best model coverage and Trainium-friendly IR. Trainium’s compiler team relies on Meta for performance improvement and as POC for distributed training. So far it delivers >30% PE utilization for Attention module in BERT. The Meta team is also working on improving the overall compiler infrastructure (e.g., moving Penguin to C++ and MLIR) and machine-learning based performance tuning algorithms.
Besides code generation for Penguin, Meta can also be used for GPUs. The team works with the SageMaker Neo team on the Hopper project to accelerate training workloads on GPUs. Hopper plans to adopt Meta as the major technique of its training compiler. To date, Meta takes popular CV and language models in PyTorch. When compiled (and executed) on P3 and P4 instances this leads to an up to 40% speedup compared to PyTorch Baseline. Lastly, the team works closely with the M5 team to support large-scale models. Meta helps accelerate the model training as well as reduce the memory consumption. It also plans to deploy M5 onto Trainium.
Proposal for 2022
In 2022, the team will pursue the following three directions: 1) optimization for distributed training is becoming a key requirement, both for teams inside Amazon and also for AWS’ customers; 2) DL compiler efficiency can be increased by by using just-in-time compilation, in particular when dealing with eager-mode imperative models; 3) on the edge federated learning is an enabler for privacy and data locality. Let us delve into these aspects in detail:
First, Meta will increase its scope from optimizing the training task on a single device (Trainium or GPU) to distributed training. Beyond the current state of the art (which is fairly well described in DeepSpeed) we will explore the following two ideas: 1) While many customers want to deploy large-scale parallel and distributed models, they struggle to use the right configurations and tools. In particular, customers need to have a profound understanding of how to partition and parallelize the models manually. This is rarely successful and if so, rather laborious. Instead, we propose to search the design space of distribution strategies automatically in a manner similar to NAS for DL network design. In particular, we plan to explore aspects of data parallelism, model parallelism, pipeline parallelism and their combinations. Using Meta as our compiler we can generate the executables for each node accordingly. 2) Even though EC2 offers fairly homogeneous bandwidth, it is nonetheless nonuniform in practice. Instances in different placement groups and racks have different bandwidth. Failing to taking this into account may result in unexpectedly slow communication in a distributed training job. Instead we can design an optimal spanning tree (or more precisely a convex combination) approach for communication to guide the communication of between accelerators during the distributed training. Once an improved distributed training solution is found, Meta can directly benefit the model training of M5, M* and similar models on both Trainium and GPU.
Second, Meta will investigate the just-in-time (JIT) compilation due to increasing customer demand: Trainium will support eager mode execution for users to quickly interact with the model, during which, the operators need be executed line by line. This cannot be done by batch compilation. Instead, JIT is needed in such cases. One way to do this is to pre-compile micro kernels and use them to construct the kernel for required operators on the fly. The micro kernels can be stored in Lorien, an operator database.
Third, we will extend to other use cases such as federated learning on other hardware like edge devices. As a training compiler solution, Meta doesn’t limit itself to the server level on the cloud. We observe that there are emerging training requirements on the edge devices for locality purpose and security concern. Federated learning (FL) is a representative area of this. Federated Learning takes model training to where private data resides, the edge devices. Current customer use scenarios are often complex (Australian Defence Force Academy) or large scale (Alexa). In 2022, we will jointly build an end-to-end demoable FL capability on edge devices or gateways, providing compiler coverage on edge accelerator enabled devices. The FL platform will provide customers the tools and infra to experiment and simulate their FL scenarios while helping AWS to iterate the new service. Meta FL optimizer provides tooling differentiation for iterating faster model training on edge devices.
Appendix 8: Education - Microlearning
Executive Summary
External customers report having to navigate a fragmented and often bewildering learning path to go from awareness to the application of AI/ML, relying on a combination of internet search, blog posts, online forums, YouTube videos tutorials, MOOC-based training & certification and developer documentation. AWS has a wide variety of teams that create AI/ML content including AWS Training and Certification, MOOCs through digital training partners (Coursera, Udacity), AWS Academy, AWS Educate, Machine Learning University, TechU, Technical Field Community (SAs), AWS Documentation, the Dive into Deep Learning (D2L) online textbook, DeepRacer, and an array of other initiatives. Most of the training produced by AWS are single monolithic courses that are designed to reach a large customer segment. Customers have to take significant time outside of their workflow to learn ML and most have to sit/sift through irrelevant content, inconsistent notation, and possibly outdated code, which is time consuming and frustrating. The 2021 AI/ML customer survey ranks targeted learning mechanisms embedded within our AI/ML services as the most desired customer support for our products. A smooth on-boarding process and easy access to learning during service usage would increase service adoption, customer satisfaction and the need for remedial support by solutions architects. In order to offer true “point of need” training, we need to increase the amount, diversity, flexibility and the distribution channels of our learning assets, and build mechanisms for rapid updates to code examples and content topics to keep our materials fresh, free from code rot, and aligned to AI/ML development. To meet this goal, the AI Research and Education (AIRE) group proposes to 1) create modular micro-learning assets on beginning through advanced ML topics from existing MLU and D2L content that can be distributed through a variety of channels (in-app training, YouTube, Twitch, modular tutorials), 2) assist service teams in developing micro-assets for embedding in AI services for on-boarding beginners and 3) set up a centralized AI/ML content repository to support a culture of content sharing across all AI/ML education teams. In this way we can reduce redundancy, increase the speed of content production and revision, and produce assets that are more targeted and reusable.
Status Quo
AWS has a wide variety of teams that create AI/ML content for both internal and external customers. Content produced by the major training programs (AWS T&C, AWS Academy, AWS Educate, MLU) is largely traditional course format either instructor-led or self-paced digital. The most granular level of content right now is the course, a single learning asset for a general target audience and meant to be consumed in a linear fashion over a period of days from start to finish. Course-creating teams generally work entirely independently with little to no organized content sharing across teams and as a result the final products are often highly similar with only minor differences. This approach to content development promotes redundancy and prevents us from creating the amount and diversity of learning assets we need to help customers overcome obstacles during the use of our AI/ML services. While the SA organization produces smaller informal content (slide decks, code demos, blog posts) to solve problems with customers on-site, very little is used by course-creating teams as source content and there are few standards for content coverage or instructional quality.  Lastly, we also have extensive “text” versions of ML education in developer documentation and in the Dive in Deep Learning interactive online textbook. Much like courses, these artifacts are designed as a monolithic learning asset and are meant to be consumed in a traditional academic way of “studying” or “reading the manual”.  Currently we do not have short targeted modular learning assets that can be delivered to the customer at their specific point of need.
Desired State
Ideally, AI/ML education teams would be aligned on different customer segments, the content required to create a seamless customer training experience and an easy mechanism to plan a joint roadmap and share content resources. Our goal should be to produce content that is specifically adapted to customer’s needs that is delivered exactly when they need it to further their work.  Micro-learning is a learning approach using relatively small learning assets and short learning activities to build specific skills. Content assets are designed to be completed by the learner in under 5 minutes, be highly engaging and address a specific learning objective. The goal is to have atomic elements of content that could fill the moments during a learner’s workday when they reach for a device to learn something or be easily embedded in existing workflows.  By engaging the learner repeatedly multiple times a day when they are seeking the information, retention of content is increased and the effective transfer of knowledge from the learning environment to the work environment is increased. We envision this content to include 1) short  < 5 minute videos, 2) interactive graphs 3) mini-tutorials, 4) code demos, 5) in-app coachmarks 6) interactive walkthroughs. Content would be designed for multiple use cases and deployable through a variety of channels to put training in front of the customers wherever they are when they need or want to learn something. Ultimately, this collection of small micro-learning assets could be the base for a future new interactive ML knowledge base that allows users to explore and learn ML in a totally self-directed way (see Prime Radiant PR-FAQ).
Objectives (Success criteria)
Increased adoption of AI/ML services by customers who initially investigate.
Increased user retention in services and conversion of users to power users/product advocates.
Decreased cost for content production.
Increased number and diversity of content assets.
Increased content usage / user engagement in learning assets by customers.
Decreased code rot in learning artifacts.
Benefits
Eases the obstacles to on-boarding and adoption of services.
Training within the services workflow keeps users in the product for longer increasing investment in AWS.
Increased retention of information by putting learning at the point of need.
Costs
Content development team of 3 AS/DS and 1 instructional designer to convert existing content to chunks for ML services training and to build net new modular micro-learning assets for AI service training.  NOTE: Ongoing creation of traditional MOOCs from MLU content would require an addition of 1 AS/DS and 1 ID to this content development team to support requests for larger courses adapted to run on Coursera/Udacity/EdX platform(s).
A development team of 2 SDE to take a software engineering approach to educational content product and support hosting/maintaining a shared content repository and maintain any systems for continuous deployment to auto-publish and update code in content artifacts.
An analytics team of 1 BIE and 1 data analyst to support collection and analysis of student engagement metrics on learning assets to inform content development.
Appendix 9: AWS Announces [DeepEarth]
New set of ready-to-use, planetary-scale AI models that makes geospatial and sustainability applications easy to build and deploy on Amazon SageMaker.
The Alan Turing Institute, NCAR, NSF sponsored STC Center, OS-Climate among the many contributing partners; Uber, Enview, KPMG, Allianz, BNP Paribas, Goldman Sachs among the many customers using [DeepEarth] to develop the next-generation AI-based applications for managing environmental-related risks and resource allocation
SEATTLE — (BUSINESS WIRE)— April 22, 2022 — Amazon Web Services, Inc. (AWS), an Amazon.com company (NASDAQ:AMZN) announced [DeepEarth], a set of more than 18 ready-to-use AI models and solutions that allow commercial researchers and developers to tackle sustainability related R&D challenges with deep learning-based techniques. [DeepEarth] provides a one-stop shop for the latest, most important AI models operating on commercial and open datasets from Amazon Sustainability Data Initiative (amazonsdi.com). [DeepEarth] has a full catalog of well documented, test-verified, and pre-trained models that can be accessed through SageMaker JumpStart. [DeepEarth] contains state-of-the-art models that have been jointly developed in collaboration with our strategic partners in the geospatial and earth science communities. [DeepEarth] brings communities, Amazon AI researchers, and a comprehensive selection of datasets together to help applied scientists and engineers from energy & utility, financial, agriculture, insurance, and other industries to solve practical environmental and sustainability-related R&D problems at scale. To get started, visit  https://aws.amazon.com/[DeepEarth]/.
Today, the proliferation of devices are making large-scale geospatial data collection affordable. Governments, mapping companies, and landowners are scanning the lands with Unmanned Aerial Vehicles (UAVs), LiDAR systems, and satellites. Advanced sensor data for solving climate and sustainability problems are being generated and collected with IoT sensors, weather simulators (such as European Centre for Medium-Range Weather Forecasts, or ECMWF), and RADAR (such as Synthetic Aperture Radar, or SAR). A massive amount of high-resolution climate, agricultural, weather, environmental, and urban geospatial data is being captured continuously at a planetary-scale. The huge amount of data from these different sources is simply being stored but challenging to clean and analyze. Commercial users who collect and own these datasets often have little expertise or scalable compute infrastructure to process them fully. More urgently, as climate change causes extreme weather events, financial losses are becoming more unpredictable. Sustainability related problems, given their interdisciplinary nature, can greatly benefit from data analytics and tools like AI to automate extracting insights and knowledge that support fast decision making. Companies like Goldman Sachs, KPMG, and Amazon are building up environmental risk assessment taskforces that are counting on the availability of AI automation and scalable infrastructures to process large amounts of complex environmental data to support business operations.
[DeepEarth] as part of SageMaker JumpStart, provides a comprehensive collection of AI algorithms, pre-trained models, and solutions that automate a broad range of common environmental use cases needed by commercial researchers and developers. This evolving collection of Models include 1) detecting deforestation, droughts, regional wealth index, etc. 2) identifying crop specific fields to predict yield and support precision agriculture, 3) predicting future rainfall patterns, and extreme weather events like Tropical Cyclones to manage the risk of weather variability, 4) detecting emissions of green-house gas through imaging, and 5) automating geospatial data cleaning. In addition, participating research communities can contribute their own innovative models or datasets to [DeepEarth]‘s public github repository, while commercial vendors also have the opportunities to sell their value-added products and services on AWS (e.g., AWS Data Exchange). [DeepEarth] brings compute, datasets, and communities together to innovate on the AWS AI platform.
“We will see exciting new Deep Learning (DL) innovations in the geospatial frontier to support innovation and problem solving in the sustainability space. Geospatial data is large in scale, real-time, highly multimodal, and more difficult to analyze than simple image, text datasets we use in DL today. For most commercial R&D, applying the state-of-the-art deep learning-based techniques in their businesses would require significant investments in hiring and building large scale compute infrastructures and teams.” says Alex Smola, VP and Distinguished Scientist at AWS. “With [DeepEarth], customers can build applications with these ready-to-use AI models and solutions. Since these models are developed together with our strategic partners in the geoscience research community, [DeepEarth] will keep up with SOTA as the research field advances.”
Customers that want to build a new business application (e.g., identifying crop fields from high-resolution aerial images) using their own data, can get started by a one-click deployment through SageMaker JumpStart Model Zoo. They can train a [DeepEarth] model on a subset of their collected data, or directly run inferences with pretrained models. After the initial prototyping and verification, the customer may consider scaling out the training and inference of their DL models automatically. They can also upload their large-scale dataset to AWS S3, select the [DeepEarth] model that is available in SageMaker JumpStart, and then launch the training job as part of a data processing pipeline. Once they have trained the model, customers can then deploy them through SageMaker inference endpoints. 

[DeepEarth] models will help us address urgent sustainability challenges like climate change which is leading to stronger and more frequent weather events, creating financial losses and business disruptions. Developing tools to better assess and mitigate these climate impacts supports a broader effort of assessing and managing the financial risks from climate change. Amazon is a sponsor of the non-profit, pre-competitive Linux Foundation based Open Source Climate organization (os-climate.org). This includes many financial customers (Goldman, BNP Paribas, Allianz, LSE, S&P Global etc.) who are starting to build asset risk modeling and forecasting systems based on open source data sets and ready-to-contribute new models and use scenarios to [DeepEarth].
“Weather impacts every part of Amazon’s global supply chain. Currently, weather disruptions are managed using varying sources of forecast data and reacted to at a site level. Our future solution is a central weather risk assessment platform that streamlines how operations prepares for, and executes around, severe weather events.” says John Felton, Amazon Vice President for Global Delivery Services, “With SageMaker and [DeepEarth] Models, we are able to accelerate the development of our risk models across a highly variable global supply chain.”
[DeepEarth] is leveraged by Amazon teams to improve our own operations, and assess risks to our own business and improve the experience for our customers. “The Amazon Sustainability Data Initiative has gathered free-to-use geospatial and climate data on AWS since 2018, but it’s often provided in formats that are hard to consume, and need work to transform and clean up,” says Adrian Cockcroft, Amazon Vice President for Sustainability Architecture. “We’ve funded a program for data source owners to do this work once, and provide data in cloud friendly formats like parquet. We’ve worked with [DeepEarth] models to create validation test suites to be sure that these datasets are high quality and provide examples of how to use them.”
“Catastrophe modeling is at a stage in its evolution where companies want to dig into sensitivity testing and gain a firm grasp of AI models’ assumptions, limitations, and capabilities,” said Paul VanderMarck, chief products officer at RMS. “As advances in deep learning science and data increase the granularity and precision of models, AWS AI and [DeepEarth] Models accommodate the mounting volume of information and enables companies to gain insight into, and importantly, to quantify key drivers of uncertainty and make more informed decisions faster.”
Appendix 10: AWS announces Amazon [IoT SiteWise Particular]
New IoT SiteWise feature for manufacturing makes it easy to quickly identify root causes of production issues and predicts the impact of process parameter changes on quality and yield to reduce quality-related cost.
Volkswagen, Honda, Lockheed Martin, TSMC and Tata Steel among the many customers using [IoT SiteWise Particular] to quickly resolve production-related issues and continuously improve their manufacturing process.
SEATTLE —(BUSINESS WIRE)-- December 1, 2022 — Amazon Web Services, Inc. (AWS), an Amazon.com company (NASDAQ:AMZN) announced [IoT SiteWise Particular], a new IoT Sitewise feature which makes it easy to quickly identify root causes for acute product-related issues in high-volume, highly automated manufacturing settings. The new service also allows to perform virtual trials that quantify the impact of manufacturing process parameters on product quality and yield. By employing causal ML techniques, [IoT SiteWise Particular] goes beyond mere correlational analysis and provides actionable insights based on the cause-effect relationships underlying each specific manufacturing setting. Both process and discrete manufacturing companies can leverage the insights generated by [IoT SiteWise Particular] to drive continuous improvements and reduce quality-related cost. To get started, visit https://aws.amazon.com/[Particular]/.
Many manufacturing companies have quality-related cost as high as 10-20% of sales revenue. The top drivers of quality-related cost are rework, repairs, scrap and returns/claims. Manufacturing issues cause most of the rework, scrap and a significant share of all returns or claims: in automotive for example, the share of returns or claims caused by manufacturing is 14%. While there are solutions that can help identify visual defects like Lookout for Vision, identifying root causes of product issues across a manufacturing line requires experts from different fields who have to analyze massive amounts of data from a multitude of sensor reads, test results and production parameters. For complex issues, this laborious task can take several days to resolve. While the analysis is ongoing, the production issue causing the defects persists, along with its negative business impact. If the impact of the issue is significant, manufacturers will even have to stop the line to investigate. Stopping a line for as short as an hour can lead to costs over 1M USD, for example in automotive. Once the experts have a solution candidate to address a production issue, manufacturing companies rely on expensive trials for validation just like they rely on expensive trials to validate all other process improvement ideas. Trials in manufacturing settings are expensive because they usually have to be implemented in a running manufacturing line. It does not only take time and effort to set up those trials, they also don’t always solve the issue on the first try, which adds to the overall time required to solve the issue and in the worst case, impairs production even further. Addressing these problems by automating root cause analysis and virtualizing trials in manufacturing is hard and requires causal ML capabilities that only few manufacturing companies have. In addition, companies have to tackle other challenging problems like collecting and organizing manufacturing data, estimating highly relevant metrics like equipment health and synthesizing the findings into actionable insights for the shop-floor operators.
[IoT SiteWise Particular] makes it easy to quickly identify root causes of production issues and to predict the impact of process parameter changes on product quality and yield to reduce quality-related cost. Using causal machine learning, the service provides insights into the cause-effect relationships underlying each specific manufacturing setting. Insights generated by [IoT SiteWise Particular]‘s root cause analysis tool allow operators to resolve production issues quickly, minimizing rework and scrap. For example, if some machine parameters are accidentally changed during a maintenance which causes yield to drop week-over-week, [IoT SiteWise Particular] will identify the machine causing the issue with just a few clicks. [IoT SiteWise Particular]‘s virtual trials provide insights that allow to fine-tune the parameters of the manufacturing process, replacing expensive and time consuming in-production trials. In a foundry process for example, [IoT SiteWise Particular] has the capability to quantify the positive downstream impact of controlling mold temperature more accurately which allows to fine tune the corresponding process control. [IoT SiteWise Particular] comes as part of an end-end system for manufacturing operators who can interact with the new service using AWS IoT SiteWise dashboards. In addition, [IoT SiteWise Particular] naturally integrates other AWS services: To quantify the impact of equipment health on product quality, the equipment monitoring services Amazon Lookout for Equipment or Monitron are integrated. If important product characteristics manifest visually, the accuracy of [IoT SiteWise Particular]’s estimates and predictions will be improved by integrating the visual inspection service Lookout for Vision. Finally, the anomaly detection service Lookout for Metrics is integrated to automatically trigger [IoT SiteWise Particular]’s root cause analysis for every anomaly which is detected. Combined, the AWS services provide actionable, data-based insights for the operators on the shop-floor and enable manufacturing companies to drive continuous improvement across entire manufacturing lines.
“Root cause analysis is one of the must-have capabilities of our manufacturing customers like Volkswagen, Honda or Flex because it is the first step towards automating the continuous improvement of their manufacturing processes” says Swami Sivasubramanian, VP of AWS Machine Learning “[IoT SiteWise Particular] solves this need by adding to our existing industrial service portfolio. [IoT SiteWise Particular]‘s root cause analysis allows our customers to easily and quickly identify the causes for any product-related issues they are facing. In combination with the virtual trial capability, this allows manufacturing operators to consciously improve their process and gain significant competitive advantages by saving millions in quality-related cost. By launching the new capability as a feature of IoT Sitewise, we provide an end-end solution which makes it easy for our customers to adopt these new capabilities.”
Process engineers can access [IoT SiteWise Particular]‘s capabilities through the IoT SiteWise dashboard. To get started, they simply select a manufacturing line and are automatically presented with all measurements available from the line. The process engineer then provides the number of processing steps and a name for each step. In a foundry, process steps would for example include shooting the metal into the mold, temperature control of the mold, squeezer procedure and spraying the mold with release agent. Using a drag and drop interface, the process engineer then groups the available measurements (e.g., temperatures, pressures in the foundry example) by processing step, specifying whether a measure is a product or a machine property by using a drop-down menu. The subsequent model creation, training, and deployment steps are automatic. Upon completion of deployment, the operator has access to two key tools, a root cause analysis tool and a tool to perform virtual trials. The user interface is graphical and represents the process steps defined by the process engineer who set up the service. By clicking on any of the process steps, all process parameters for any given part ID, any batch of product or batch of parts can be opened and visualized. A root cause analysis for any observation of interest, for example a part failing the final quality test, can be triggered manually with the push of a button or it can be triggered automatically based on user-defined rules or using the anomaly detection from Lookout for Metrics. The resulting analysis provides attribution scores in percent that quantify how much the upstream process steps contributed to the observation. The virtual trial tool is based on the same user interface and allows process engineers to calculate virtual trials by changing process parameters/measurements with [IoT SiteWise Particular] predicting the downstream impact of those changes.
“For us, this is a breakthrough solution.” says Yamada Hanako, Honda plant manager and responsible for Honda’s digital manufacturing initiative. “With [IoT SiteWise Particular]‘s root cause analysis tool, we have been able to reduce the time required for root cause analyses from days to minutes. In addition, the service allows us to quantify the impact of our upstream process steps, something we could never achieve with our expert-driven deep-dives. These new insights allowed further process improvements and resulted in a 10% reduction of scrap.”
“We reduced cost for rework, repairs and scrap by 15% using [IoT SiteWise Particular]‘s process experimentation tool.” says Gerd Walker, head of manufacturing VW Group. “In the last quarter, directly after starting our work with [IoT SiteWise Particular], the virtual trials allowed us to identify 5 high potential process improvements. We verified them in live trials and ended up implementing 3 of them - this is a 100% higher experiment success rate compared to what we used to achieve before.”
To get started with [IoT SiteWise Particular], visit https://aws.amazon.com/[Particular]/
Appendix 11: AWS announces [Bifrost]
New service discovers and links related information across distributed data storage locations such that analysts, data scientists and machine learning practitioners can make use of the information effortlessly and quickly.
Janssen Pharmaceuticals, ADP, Rivian Automotive, Autodesk, Uber among the many customers using [Bifrost].
LAS VEGAS—(BUSINESS WIRE)—December 7, 2021—Amazon Web Services, Inc. (AWS), an Amazon.com company (NASDAQ:AMZN) announces [Bifrost], a service that makes it fast and easy to discover and link billions of related pieces of information, making them accessible no matter where or how the data is stored. Using [Bifrost], analysts and data scientists can navigate an organization’s data, and machine learning practitioners can train their models without needing to know where the data is stored, how to process it and without having to move it to some central location. [Bifrost] unifies cloud and on-premise data storage and processes various data types, for example databases, tabular data, slide presentations, text documents and images. Integrating machine learning techniques like Amazon Textract for optical character recognition, Amazon Comprehend for natural language processing or Deep Graph Library for discovering links, [Bifrost] reduces the effort for discovering and linking existing pieces of information by over 10x. [Bifrost] stores the revealed links as well-defined relationships between the individual pieces of information using a fully-managed, searchable data model spanning across the organization’s storage locations: an enterprise knowledge graph. Users can leverage [Bifrost] to work with their data via a visual interface to browse the enterprise knowledge graph, a web application and API for querying and manipulating data and integrations with search services Amazon ElasticSearch and Amazon Kendra. To learn more about [Bifrost], visit https://aws.amazon.com/machine-learning/[Bifrost].
Today, finding and retrieving data concerning different aspects of an organization’s business is unnecessarily complex and time consuming, mainly due to 3 reasons. First, data is typically stored in different data storage locations across multiple locations (cloud-based and on-premise) and owned by different parts of an organization. Second, the data stores use different storage formats, for example databases, tabular data, presentations, text documents and images. Third, inherent links between stored pieces of information, e.g., entries in a table and text in a pdf file, are not explicit. For example, for an analyst to understand that variations in raw material quality cause returns and claims, that analyst needs to link information from supply-chain, manufacturing and after-sales databases. To do this, the analyst either needs to know where and how the information is stored or needs to investigate that as well. Imagine that in addition, the supply chain database only contains non machine-readable PDF reports on raw material quality per production batch. In such a case, the analyst’s work also includes reading those PDF reports and associating production batch IDs with product IDs. Search solutions are only of limited help due to their restrictions to text-based data types and because they can only assist in finding records matching the search query. In the example above, no existing search solution could have spared the analyst from reading the PDFs and manually associating product batch IDs with product IDs. More advanced approaches like building a machine learning application to automate the analyst task would require moving the relevant data to some central location for training, leading to additional effort and data migration cost. Accordingly, organizations are often blocked from deploying data-driven approaches on the information that exists across their data storage locations and thus they miss out on the opportunity to boost the efficiency of their business processes. To address these problems, some organizations started unifying their information through enterprise knowledge graphs. However, current processes of creating enterprise knowledge graphs are prone to missing relevant links and requires organizations to commit scarce resources for months, effectively blocking them from other value-adding endeavors. Worse still, most organizations lack the capabilities to create, deploy, secure, scale and maintain an enterprise knowledge graph.
[Bifrost] discovers and links billions of related information pieces across distributed data storage locations such that analysts, data scientists and machine learning practitioners can use the information effortlessly and quickly. Compared to current approaches, [Bifrost] is simple to set-up for an organization’s IT developers and achieves effort reductions of over 10x by utilizing specialized machine learning approaches to populate and curate an enterprise knowledge graph that unifies an organization’s data no matter where and how it is stored. These machine learning approaches include natural language processing for text files, optical character recognition, general computer vision for image data and graph learning for schema matching and link prediction. Users supervise the process, allowing [Bifrost] to learn from feedback and improve link detection accuracy. Analysts and data scientists profit from [Bifrost] by gaining powerful data querying and manipulation capabilities through graph query languages like Gremlin and SparQL, an API for interfacing via HTTPS, search services like Amazon ElasticSearch or Amazon Kendra and a web-based tool to visually navigate the data. Machine learning practitioners build applications like recommendation, forecasting and anomaly detection systems on top of the enterprise knowledge graph without necessarily having to move their training data to some central location. For example, a manufacturing company’s product data, processed by [Bifrost], has links to production data like the used raw material composition and after-sales data like product specific customer feedback and returns & claims. Using [Bifrost] to easily retrieve the relevant data with a single query, a developer builds a classification application that predicts the likelihood of increased return rates based on raw material composition which can then be used by the production team to reduce quality-related cost. Taken together, the capabilities provided by [Bifrost] provide organizations with an opportunity to boost their productivity by lowering the barrier of adopting data-driven business processes powered by machine learning.
“Our customers today strive to implement data-driven business processes and need fast and easy ways to find and access relevant data quickly, no matter where and how the data is stored. However, they often have to realize that discovering and linking relevant pieces of information in their distributed data storage locations is a task for which they lack crucial competencies or that it is simply not economically feasible due to the months of effort it requires,” says Swami Sivasubramanian, VP AWS Machine Learning. “With [Bifrost], we turn this daunting task into a quick and easy exercise that takes days instead of months. Our customers especially like [Bifrost]s capability to discover or predict links between pieces of information. It means that they don’t have to commit their own machine learning experts to data integration tasks any more. Instead, they can now leverage their experts to solve important business problems.”
To get started with [Bifrost], customers go to the AWS console and register the different data sources they want to unify by entering corresponding URLs, ports and access credentials. Alternatively, if the data is already on AWS, they enter links to individual S3 buckets, databases or AWS Lake Formations. Domain-specific data types, that [Bifrost] does not already support out-of-the-box, are connected via custom scripts specifying how [Bifrost] can crawl and process these specific data types. The subsequent population and curation of the enterprise knowledge graph is highly automated. The service identifies and predicts probable types of links and asks the users to confirm and name them. If a proposed link type is not confirmed, the user uses a visual interface to correct or delete the link type. Should Bifrost miss a certain type of link, users can provide example records that should be linked in combination with a name for the link type. All user inputs are taken into account to update the internal models used by [Bifrost] to propose new types of links. The result of this learning procedure is that over time, the required user involvement decreases steadily. For many application domains like healthcare, finance and life sciences, [Bifrost] comes pre-trained such that the required user interaction is low right out-of-the-box. Once a graph is finalized, it is stored in Amazon Neptune and the user can start accessing the information. This is possible via natural language and semantic search based on Amazon Kendra and Amazon ElasticSearch, a graph query search engine, a visual graph navigation tool and an API based on GraphQL. This API represents a key interface to the information captured in the graph and allows for easily building applications on top of [Bifrost], for example via SageMaker notebooks or via HTTPS requests. At the same time, the API is the best way to modify the graph by updating, adding or deleting data and link between pieces of information. In addition, Bifrost natively supports Amazon Neptune ML to build machine learning applications. Using the AWS console or the API, users can also add or remove whole data sources over the lifecycle of their enterprise knowledge graph and schedule regular updates to keep the enterprise knowledge graph in sync with each data source.
Janssen Pharmaceuticals, a subsidiary of Johnson & Johnson, conducts research and development to bring a wide range of drugs to market. “At Janssen we are focused on bringing safe, effective treatments to market as soon as possible to save lives. To do this, each functional team must collaborate and share their data to ensure our industry leading standards are met at every phase. We produce thousands of data points daily across teams pertaining to related experimental entities. Previously, scientists had no way of tracing measurements upstream and downstream thus limiting our pace of innovation.”, says Hal Stern CIO of Janssen Pharmaceuticals, a J&J company, “We tried several times to create a solution that could connect all of our data together, but we lacked the graph and ML expertise to pull it off. [Bifrost] was instrumental for us to unify data assets across the company, helping us harness decades of institutional knowledge by showing us the connections in our data while being flexible enough to quickly integrate new technologies. Training and deploying a model that can be used by our entire organization now takes days to weeks, instead of months. The resulting enriched information we receive about proteins has allowed us to increase our rate of new molecular entity development by 20%.”
To get started with [Bifrost], visit https://aws.amazon.com/machine-learning/[Bifrost].
Appendix 12: AWS Announces [M*]
New internal offering reduces the cost of language data collection, improves performance of language services, and breaks the dependency on third-parties by training large foundational models and providing a library of optimized models and tools
Teams across Amazon working on machine-learning-based language services & applications use M* as a cost- and time-efficient base to build their models
SEATTLE — (BUSINESS WIRE) — December 1, 2022 — Amazon Web Services, Inc. (AWS), an Amazon.com company (NASDAQ: AMZN), announced [M*], a new internal offering that breaks Amazon’s dependence on publicly available, large language models from third-parties like Facebook and Google, reduces language-related data acquisition cost across Amazon, and enables performance improvements for our language services and applications. With [M*], Amazon takes ownership of the end-to-end process for the creation of state-of-the-art language models, including the important step of training large models, for which Amazon today relies on third parties. To achieve yearly data-related savings of $10-30M USD, [M*] takes advantage of the data collection across different languages, language tasks and applications to train large models which are then reduced in size, refined for various tasks, and made accessible via a model library. By leveraging the [M*] model library and the tools [M*] provides to further customize and deploy these models, instead of starting from the publicly available models, language service teams can also improve performance.
Today, our machine-learning (ML)-based language services and applications rely on publicly available models like Facebook’s XLM-R or Google’s BERT. Those publicly available models come pre-trained, which means they have been trained by researchers using proprietary tools and data-sets with billions of words, mostly in the English language, making them broadly useful for researchers, but not great for specific tasks. These models are like the “raw material” which needs to be refined by our scientists to achieve state-of-the-art performance in a specific language task, like understanding, classification, or text generation. This is typically done through size reduction and fine-tuning, the latter consisting of feeding the model with additional task-specific annotated data. For example, to determine sentiment of customer reviews, the model must be fed review text that has been labeled with sentiment details like “positive,” “neutral,” or “negative”. These accuracy improvements are directly proportional to the amount of task-specific data that can be provided and oftentimes, building a large enough corpus of this data to meet performance targets is time-intensive and expensive. So, as the number of Amazon language services and applications grow, so do our investments in data annotation; AWS AI/ML language services plan to spend $62.5M in FY2022 (+60% YoY), with spending across all of Amazon growing to >$200M for acquisition, human evaluation, annotation, and labeling of task-specific data for language models alone. In addition to the escalating data acquisition spend, basing our language services and applications on pre-trained models from third-parties like Facebook and Google also leads to problematic dependencies: changes in the commercial terms of use for the pre-trained models can adversely impact our usage, there is a lack of control concerning data quality, bias, and fairness, and finally domain and language coverage are not in our control. Especially language coverage beyond English is critical for Amazon, due to the up to 55 languages currently supported by our language service teams. Any changes or deficiencies in these pre-trained models or their terms & conditions have a downstream impact on our language services. Finally, not all model innovations by these third-parties are made publicly available. Some of the biggest models, for example models with >=100B parameters - about 100x the size of the models we currently use in our services, are not accessible to our teams. However, recent research has shown that larger models are key to achieve state of the art performance with benchmark showing performance gains of up to 10% such that a lack of access to larger models limits the performance our services can achieve vs our competitors. Overall, Amazon is facing problems of soaring data collection costs for model fine-tuning, external dependencies and performance limitations for our language services and applications.
[M*] provides Amazon researchers and service teams a continuously updated library of reduced-size models derived from our own large pre-trained language models, along with the tools needed for everything from fine-tuning to deployment. By doing so, [M*] reduces the cost of task-specific language data collection across Amazon by $10-30M USD per year and breaks our dependency on third-party providers of pre-trained models like Facebook and Google. With the addition of [M*], Amazon now owns the creation of state-of-the-art language services end-to-end, including the important step of pre-training large models. [M*] creates training data-sets consisting of unannotated and annotated data like raw text, images, documents, forms, queries & code. Based on these training data-sets, [M*] pre-trains large models, including model architectures of up to 100B+ parameters and incorporating continuous innovation from the ML community. The resulting large models are also regularly updated when new training data becomes available, which improves performance of all down-stream applications. [M*] then provides a library of reduced-size models (based on our own large pre-trained models) for different tasks such as language understanding, classification and text generation, along with tools for service teams to further customize them to meet service-specific requirements, such as cost, accuracy, or latency goals. Since the models in the [M*] library are already much closer to the requirements of service teams compared to the large pre-trained models used today, less task-specific data is required for fine-tuning and time-to-market is reduced. The reduction in task-specific annotated data is expected to reduce data-related spending of language service teams by 5-15%, resulting in Amazon saving 10-30M USD per year. [M*] thus allows to share investments across teams, reducing the overall cost for the company while at the same time improving time-to-market & performance and winning independence from other companies.
“Instead of focusing exclusively on building bigger models, we also aim to build smarter.” says Swami Sivasubramanian, VP of AWS Machine Learning “[M*] is a completely new internal offering which powers our language services, unlocking savings and performance improvements. It enables us to launch new language services and features faster, at lower cost and risk.”
Scientists start by browsing available models in the [M*] library. Each model in the library is labeled by its task (e.g., understanding, classification or text generation), size, required inputs, domain from which the training-data was selected and the reported performance across different tasks. Scientists can choose any of the models in the [M*] library and instantiate them for common tasks using a Python-based software development kit (SDK). To start further customization, scientists can download the whole model, including source code and model parameters. Subsequently, they can use the tools in the Python SDK to customize the model to meet their specific requirements with a few lines of code and a handful of labeled examples.
“We were able to speed up the time required to launch a new service by 30% and save millions in data acquisition cost.” says Stefano Soatto, VP of Applied Science for AWS AI. “Last quarter, after incorporating [M*] models into one of our services, we quickly realized that this solution needs to be rolled out further and have adjusted our roadmap plans for other language services.”
To get started with [M*], visit https://aws.amazon.com/m-star.
Appendix 13: Open Source Contributions
Lines of code contributed by Amazonians in 2020
| Project | LoCs |
|---|---|
| Gluoncv | 51405 |
| Gluonnlp | 66258 |
| Autogluon | 109371 |
| d2l-en | 129675 |
| d2l-zh | 207109 |
| dgl | 200668 |
| mxnet | 52771 |
| Total | 817257 |
