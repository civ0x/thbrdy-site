This document presents the key risks and status for the NU Alexa Teacher Model (AlexaTM) program ([A-Team: 269538](file:///Users/georgla/Documents/Alexa%20Science/Horizontal%20Initiatives/2021%20AlexaAI%20Goals/Alexa%20Teacher%20Model/Risk%20Reviews/269538)). The program encompasses seven core workstreams related to Data Preparation; Pretraining Architecture[^1]; Pretraining Tooling; Model Evaluation and Benchmarking[^2]; Distillation[^3]; Batch Inference Service; and Model Storage and Vending. Through these workstreams, we are: 1\) pre-training multilingual transformer based language models of 1B+ parameters, 2\) distilling those teacher models into student models with different architectures (BERT, MT5) and sizes ranging from 18MM to 170MM parameters for use in production, 3\) evaluating these models on three core tasks of 1P NLU, semantic parsing, and dialog response generation, and 4\) running batch inference for offline use cases such as paraphrasing, generative and extractive Q\&A, and NLU classification in a secure, reliable, and optimal way. While the program has a longer list of p0/p1 customer use cases, to drive focus in delivery, we are tracking 2 production customer goals: (a) launching a multilingual NLU runtime model for India (supporting English and Hindi) and for Italy (supporting French, Italian, Spanish, and German) that leverages AlexaTM[^4], (b) reducing the tail error rate in en-US/GEM NLU production models by 10% in at least two domains[^5]. We are also tracking an innovation production goal to demonstrate that using AlexaTM can reduce the error rate by 10% for the Top-50 3P skills in en-US, en-GB, en-IN, and de-DE.[^6]

1. # **Executive Summary**

We are GREEN for overall goal (launching two use cases to production leveraging AlexaTM). Since the last risk review on 04/20, we met internal milestone in delivering and evaluating by 04/30: a 1B parameter pretrained model (encoder) and 2 distilled student variants (170M, 17M[^7]). These deliveries are a key stepping stone towards our goal of productionizing a large general purpose multi-lingual language model. Compared to smaller publicly available models (XLM-RoBERTa-Large from Facebook) and Alexa production models, the non-quantized versions of the aforementioned Alexa teacher models (1B, 170M) perform better than or at parity on benchmarks such as Masked Language Modeling fill-mask whole-word accuracy and on Alexa-specific metrics such as Intent Recognition Error Rate (IRER). We expect further performance improvements as we increase model size and incorporate Alexa data. Looking ahead, we will deliver a 2B parameter encoder model incorporating Alexa data by 06/30.

Meanwhile, as noted below in our highlights, the AlexaTM tiger team made major strides in reducing costs and training duration associated with 10B+ parameter models, thanks to close collaboration with M5/M\* teams in Amazon Retail and AWS. We are benchmarking larger models to see how techniques such as data parallelism scale for 20B \- 100B parameter sized models. This will help us re-prioritize our top work streams for the tiger team moving forward.[^8]

The program’s top risks are ensuring that the AlexaTM model can match or outperform production benchmarks to meet customer goal success criteria without regressing runtime user-perceived latency (UPL). To address that risk, we are (a) conducting second-stage pretraining with Alexa critical data to improve model performance, and (b) exploring different quantization approaches (partial quantization, dynamic quantization, quantizing during fine-tuning), data balancing across languages, and workflow optimizations (pre-finetuning and incremental learning). 

2. # **Highlights/Accomplishments**

1) **Delivery of Distilled Models**: On 04/20, we delivered two distilled students variants of the larger 1B AlexaTM pretrained model (at 327M and 135M parameter size, the latter being comparable to the pretrained encoders used in production. For 1P NLU, fine-tuned version of these distilled models achieved on-par performance (-4% to \+1% across language) on existing test sets and clear gains on test sets comprising code-mixed utterances.   
2) **Improved throughput for AlexaTM Model training**: By optimizing training configurations (thanks to close collaboration with M5/M\* teams in Amazon Retail and AWS) and using algorithmic approaches like data and pipeline parallelism, we have reduced the projected AWS costs for model training by 57% (from $272k to $116k per run for a 10B+ parameter model) and reduced training duration by 4x (from 120 days to \<30 days).  
3) **Training with \>1T tokens:** As noted in last month’s AlexaTM risk review, by combining data associated with multi-lingual Common Crawl (mC4) dataset (921B tokens) with previous data from Open Subtitles, Wikipedia, and CC-100, we are currently pretraining on \>1T tokens.   
4) **Delivery of Alexa critical data for pretraining**: On 05/09, the AlexaTM DataPrep team prepared and delivered unannotated Alexa critical data for second-stage pretraining (831M unique utterances and 970M upsampled training utterances across 15 locales).   
5) **Tooling available for model training with critical data**: On 05/11, AlexaTM tooling team enabled partner teams in pretraining, distillation, and fine-tuning to train with critical data. This delivery, combined with the earlier highlight on Alexa critical data, enables second-stage pretraining. We plan to conduct second-stage pretraining with the initial 1B parameter model by 05/30 and incorporate it for the 2B parameter model by 06/07. 

3. # **Key Risks:**

1) **Meeting accuracy success criteria without regressing runtime UPL**: While results show that non-quantized multilingual models are close to their non-quantized monolingual counterparts in terms of accuracy, we observe up to 24% regression on SEMER across locales when we quantize the multilingual models. Quantizing models are key to maintaining latency. Mitigation: we are exploring different quantization approaches (partial quantization, dynamic quantization and quantizing during finetuning) to reduce observed SEMER regression – DFD 05/21. Early results with partial quantization and balancing calibration data across locales reduces the SEMER regression to 8%. For more details on efforts, see Appendix B.   
2) **Production tooling readiness:** To achieve NLU production goals, we need to enable support for PyTorch and AlexaTM model architectures within the current developer-facing tooling (DeepNLU, UMBP). We had missed interim milestone of running 1% smoke test for PyTorch by 04/27, but we are targeting 06/30 to achieve GA readiness for production workflow incorporating PyTorch. 

4. # **Updated Overall Timeline**

\*New/changed rows from last risk review are shaded. 

| Category | Deliverables: | Date | Status |
| :---- | :---- | :---- | :---- |
| Initial 1B | Data processing of Wikipedia and CC-100 Dataset | 03/01 | COMPLETE |
| Initial 1B | Data processing of Open Subtitles Dataset | 03/26 | COMPLETE |
| Initial 1B | Pretrained 1B parameter encoder-only model | 04/06 | COMPLETE |
| Initial 1B | Distillation of Initial 1B parameter model | 04/20 | COMPLETE |
| Initial 1B | Evaluation of Initial 1B parameter model | 04/30 | COMPLETE |
| Initial 1B | Second-stage pretraining on 1B parameter model | 05/30 | GREEN |
|  |  |  |  |
| 2B | Data processing of mC4 dataset for pretraining 2B and 10B models | 04/19 | COMPLETE |
| 2B | Data processing for unannotated Alexa data for pretraining | 05/11 | COMPLETE |
| 2B | Pretrained 2B parameter model | 05/30 | GREEN |
| 2B | Second-stage pretraining on 2B parameter model | 06/07 | GREEN |
| 2B | Distillation of 2B parameter model | 06/30 | GREEN |
| 2B | Evaluation of 2B parameter model | 06/30 | GREEN |
| 2B | Launch multi-lingual NLU runtime models using AlexaTM 1B+ model | 10/29 | GREEN |
| 2B | Reduce error rate of 1P en-US/GEM models through using AlexaTM 1B+ model | 11/01 | GREEN |
|  |  |  |  |
| 10B | Pretrained 10B parameter model (innovation) | 08/30 | GREEN |
| 10B  | Distillation of 10B parameter model (innovation) | 09/30 | NOT STARTED |
| 10B | Evaluation of 10B parameter model (innovation) | 09/30 | NOT STARTED |
|  |  |  |  |
| BIS | Provide alpha environment of batch inference service (BIS) | 03/08 | COMPLETE |
| BIS | Provide beta version of BIS and onboard main use cases based on public HuggingFace models | 04/09 | COMPLETE |
| BIS | Enable BIS with initial 1B AlexaTM model and enable initial experimentation with RED models | 05/30 (previously 05/14) | GREEN |
| BIS | Support fine-tuning experimentation with RED models | 07/02 | GREEN |

5. # **Detailed Status for Core Workstreams**

| \# | Workstream | Status |
| :---- | :---- | :---- |
| **1** | **Name: Data Prep for Alexa Teacher Model** **Deliverable:** Aggregate, deduplicate, filter, and clean at least one 1 TB+ text dataset, including 17+ predefined languages, by building flexible tools and services that can later scale to many different pretraining datasets, mixtures, and formats. **Key POCs:** Amit Chauhan, Sumith Mathew, Vinod Mamtani (STO) **Detailed Status:** On 05/09, we completed delivery of Alexa critical dataset for 15 locales (831M unique utterances, 971M total utterances in train+validation sets) pulled from previous 98-day window. Next steps: investigating requirements for annotated critical data (DFD 05/30) and AlexaTM OP1 2022\.  | **Completed delivery of preparing Alexa critical unannotated data; all p0 requirements delivered** |
| **2** | **Name: Pre-training Architecture** **Deliverable:** Deliver multilingual Transformer-based language models with at least 100B parameters  **Key POCs:** Wael Hamza (STO), Shankar Ananthakrishnan **Detailed Status:** While we remain RED to achieve goal of delivering a multilingual transformer-based language model with at least 100B parameters by 08/30 due to key innovation risks to overcome scalability limitations, we are GREEN for delivering a pretrained 2B parameter model by 05/30 (which will then be distilled/evaluated and delivered to downstream production customers by 06/30) and a 10B parameter model by 09/30. The dedicated innovation tiger team has reduced the training time for a 10B+ parameter pre-trained language model by 4x from 120 days to \<30 days (based on scaling estimates) through optimizing model configurations (partnering with the M5/M\* teams in Amazon Retail and AWS) and parallelizing model training tasks. We are now pivoting to scalability efforts in the 50B+ parameter regime. | **RED for overall goal of 100B parameters; GREEN for meeting next interim milestone (2B parameter model by 05/30) [VP: 274059](https://kingpin.amazon.com/#/items/274059)** |
| **3** | **Name: Tooling for Alexa Teacher Model Training** **Deliverable:** Execute a small number of training steps of a 10B+ parameter model by Q2 2021 (30B+ parameter model by Q3, and eventually 100B+ parameter model by Q4) by building the necessary tools and systems **Key POCs:** Swami Balasubramanian, Jong Lim, Kjel Larsen (STO) **Detailed Status**: We completed our milestone to enable pretraining tooling with critical data on 05/11, including building a Python-based S3 client for critical data download and decryption. We are working closely with other AlexaTM teams to onboard onto the tooling.  | **COMPLETE for enabling pretraining of AlexaTM models using critical data by 05/11** |
| **4** | **Name: Alexa Teacher Model Evaluation System** **Deliverable:** Develop Alexa Teacher Model evaluation system to create benchmarks for three core NU tasks within PROVE (ETA Q2 2021), and the target customer(s) for self-serve certification of two additional NU tasks (ETA Q3 2021\) **Key POCs:** Jianhua Lu, Konstantine Arkoudas (STL) **Detailed Status**: We are GREEN for overall goal, and we met our internal milestone of delivering benchmarks on 1P NLU and semantic parsing (SP) benchmarks by 04/30 for fine-tuned versions of the overall 1B parameter AlexaTM model and the distilled 327M variants. For these benchmarks, the 1B model and the 327M variant performs at parity or better compared to analogous public encoders (XLM-RoBERTa-Large from Facebook). For next milestone, we will be evaluating larger (2B parameter) pretrained encoder models by 06/30 as well as evaluating the first AlexaTM encoder-decoder on neural response generation (NRG) (DFD 05/30). | **GREEN for overall goal; COMPLETE for fine-tuning and evaluating first distilled AlexaTMs in April; [VP: 274054](https://kingpin.amazon.com/#/items/274054)** |
| **5** | **Name: Distillation of Alexa Teacher Model variants** **Deliverable:** Deliver distilled Alexa Teacher Model variants in the 1B, 100M, and 10M parameter range that are evaluated for three TBD downstream use cases **Key POCs:** Fabian Triefenbach (STO); Wael Hamza (STL); Mohamed Gouda **Detailed Status**: We are GREEN in working towards overall distillation goal date 9/30. We completed internal milestone of delivering a two distilled student variants in different sizes from the 1B AlexaTM model, BERT (327M) and leanBERT(135M), and shared them with AlexaTM-Evaluation on 4/20 and Multilingual teams on 4/30. We then populated the first results for the first AlexaTM based multilingual model by distilling the BERT model (327M) a second time into leanBERT size (similar to current production setup), finetuned it on FR/IT/ES/DE Alexa data, and compared SemER against XLeanBERT baseline (derived from multilingual BERT XLarge). Overall, we see on-par performance (-4% to \+1% across languages), with clearer gains on code-mixed utterances (up to 20%). Nevertheless, better performance is expected with following AlexaTM model (6/30), as it will get larger, include more public data (1B more tokens), and will incorporate Alexa data (critical data). | **GREEN for overall goal and COMPLETED intermediate milestone of delivering distilled version of 1B encoder model [VP: 274061](https://kingpin.amazon.com/#/items/274061)** |
| **6** | **Name: Batch Inference Service** **Deliverable:** Build a batch inference service for the full-sized big model, including a CLI accessible from both cloud desktops and Hoverboard. **Key POCs:** Mariusz Momotko, Abishek Ravi, Kelly Vanee (STO) **Detailed Status**: While we have completed thread model review, Keymaster onboarding, and HIPPA certification, completion of our security review is delayed 1.5 weeks due to unavailability (personal illness) of security officer. Separately, we are working with other AlexaTM workstreams incorporate fine-tuned version of initial 1B AlexaTM model for Batch Inference Service use cases and enabling fine-tuning workflows for downstream customers (a key ask). New target date for M3 milestone (supporting initial experimentation with first RED models and providing 3rd version of inference service that incorporates 1B+ pre-trained encoder model) is 05/30.  | **GREEN for OVERALL, YELLOW for next milestone with date shift from 05/14 to 05/30** |
| **7** | **Name: Model Storage and Vending** **Deliverable:** Provide capability to vend all centrally-owned models from a model store, including both the full-sized Alexa teacher models and the distilled versions **Key PoCs:** Yvon Guy, Sunil Kotagiri (STO) **Detailed Status**: We are RED for the first milestone of providing an end-to-end manifest metadata ingest with model definition file and UI list view (no CLI) by 06/07 due to the high KTLO operational load for the team, but we are GREEN for completing by 06/21 (delay by 1 sprint). This delay will cause 1-2 week cascading delays in delivery of subsequent milestones; until the 3rd milestone to deliver RED-certified publishing pipeline by 08/13, critical models will be handled via S3 buckets managed by Hovermart. | **RED for initial milestone by revised date 06/07; GREEN for 06/21 [Team: 292118](https://kingpin.amazon.com/#/items/292118)** |

6. # **Detailed Status for Customer Goals Workstreams**

| \# | Workstream | Status |
| :---- | :---- | :---- |
| **1** | **Name: Multilingual Models based on ATM** **Deliverable:** Launch one multilingual NLU runtime model for India available in en-IN and hi-IN and another in Italy (supporting French, Italian, Spanish and German) leveraging Alexa Teacher Model by 10/29 **Key POCs:** Fabian Triefenbach; Mohamed Gouda; Kelly Vanee **Detailed Status:** We are GREEN for the overall goal, launching two multilingual models, the European model in it-IT and Indic model in en-IN and hi-IN, by 10/29. But we are RED for our next interim milestone to identify the best performing encoder and decoder architecture setup for multilingual models by 4/19, new ETA- DFD 5/12. We have partially completed this milestone for the decoder architecture by 4/19 and settled on the joint-language decoder setup as it works best for the target multilingual models. Yet, further experimentation and testing on feature expansion is recommended to ensure that adding a feature for one language doesn’t negatively impact the other languages. For the encoder architecture, results show that non-quantized multilingual models are close to their non-quantized monolingual counterparts, with improvements on 3 European languages (de-DE 4.4% rel., fr-FR 3.1% rel., es-ES 8.3% rel.) and degradation in one language (it-IT 4.1% rel.). However, we observe up to 24% regression on SemER across locales when multilingual XLeanBERT models are quantized while monolingual models only degrade by 1-3%. AlexaTM encoders have not yet been quantized. As quantization is a hard requirement to meet latency exit criteria, we will prioritize resolution of this by exploring different quantization approaches (ex: Partial/Dynamic Quantization) putting focus on the AlexaTM encoders. | **GREEN for 10/29**  |
| **2** | **Name: Tail Error Reduction with Alexa Teacher Model** **Deliverable:** Reduce tail error rate for production by 10% in en-US/GEM NLU models in at least two domains by leveraging Alexa Teacher Model. **Key POCs:** Jianhua Lu **Detailed Status:** We are GREEN on the production goal; we are targeting 05/21 for tail data creation and curation, which is prerequisite for  finalizing the two domains on which we plan to incorporate AlexaTM for production (ETA: 06/30). We are closely tracking 2 risks: 1\) ability to efficiently run large-scale evaluations and select and evaluate tail data sets (05/20 for solution from Hadron/NLU engine); 2\) DeepNLU-Pytorch inference engine support for AlexaTM models (06/30). Mitigation: prior to availability of aforementioned tooling, we would use an IRER proxy metric to select and evaluate tail utterances and to convert AlexaTM to MXNet models. We have added one more scientist to workstream on tail data creation and curation (ETA: 05/21). Our ETA for finalizing the two domains is end of June. | **GREEN for 11/01**  |
| **3** | **Name: Alexa Teacher Model for 3P** **Deliverable:** Leverage Alexa Teacher Model to reduce error rate by 10% for the Top-50 3P skills in en-US, en-GB, en-IN, and de-DE **Key POCs:** Mohamed AbdelHady; Konstantine Arkoudas **Detailed Status:** We are GREEN for innovation goal. Over past month, we have experimented with pre-finetuning off-the-shelf models (mBERT), and our current best model outperforms the our production baseline LSTM DNNs by 2% SEMER relative and 5% IRER relative on average across de-DE and en-US locales. We’ve learned that optimizing learning rate schedule and batch size can improve performance by 2-3% and 25%, respectively, and we are investing in a more sophisticated hyperparameter search pipeline. The key risk we face is dependence on migration of production model fine-tuning tools (DeepNLU) to support PyTorch. Our mitigation is extending the 1P NLU modeling pipeline (which was developed for AlexaTM evaluation goal) and building the 3P pipeline on top of it. | **GREEN for 12/31**  |

### **Appendix A: Deeper Dive on Progress from AlexaTM Scalability Tiger Team Efforts**

This section provides a deeper-dive into the progress of the AlexaTM scalability tiger team. The team was originally created to overcome key scalability challenges (limited network bandwidth) to training a 10B/20B encoder within a timeframe for our production goals (delivery of pretrained/distilled/evaluated models by 06/30)[^9]. The team initially prioritized investigations into (1) pipeline/model parallelism, (2) gradient compression, (3) custom gradient and parameter partitioning, (4) Al-Salbert model, and (5) sparsity/mixture of experts. 

Thanks to close collaboration with M5/M\* teams, the team deep-dived into the discrepancy between AlexaTM and M5 10B model performance, identifying several configuration changes (toggling contiguous\_gradient flag, upgrading to NCCL 2.83 and increasing page memory) that enabled us to increase TFLOPS/GPU (from 51.76 to 82\) and reduce costs (from $272K to $177K). We further increased micro-batch size to improve performance, resulting in further increase of TFLOPS/GPU to 123.72 and decrease in costs ($116K)[^10] 

Separately, the team investigated model parallelism approaches that enable us to train a 9.7B parameter PreLN encoder-only model with 900B tokens in 31 days (using 64 p3dn machines). This model training is currently underway. 

With the latest successes, the team is now turning its attention to strategies that further address scalability challenges in going from 10B to 50B models. The updated priority list is: (1) pipeline/model parallelism, (2) DeepSpeed Zero Stage 3 \+ ZERO Infinity, (3) UniLM for Encoder/Decoder/Seq2Seq, and (4) Mixture of Experts. 

### **Appendix B: Deeper Dive on current quantization efforts and progress**

We have pushed forward with full quantization and partial quantization experiments on XLeanBert models with different calibration data sizes and balancing strategies. As initial baseline, we observed up to 24% regression on SEMER across locales when we quantize the multilingual models. Fully quantized models with 100k balanced calibration data show a 19% average regression on SemER across locales. Partially quantized models with 100k balanced calibration data and with quantization applied on all layers except for the first encoder fully connected layers, show an 8% average regression in SemER. However, we need to get the latency metrics on these models before we draw any strong conclusions. We will further explore partial quantization of encoder and decoder layers and dynamic quantization to identify ways to mitigate the SemER regression while meeting latency criteria.

[^1]:  Associated VP Goal: [https://kingpin.amazon.com/\#/items/274059](https://kingpin.amazon.com/#/items/274059)

[^2]:  Associated VP Goal: [https://kingpin.amazon.com/\#/items/274054](https://kingpin.amazon.com/#/items/274054) 

[^3]:  Associated VP Goal: [https://kingpin.amazon.com/\#/items/274061](https://kingpin.amazon.com/#/items/274061) 

[^4]:  Associated VP Goal: [https://kingpin.amazon.com/\#/items/268148](https://kingpin.amazon.com/#/items/268148) 

[^5]:  Associated VP Goal: [https://kingpin.amazon.com/\#/items/269021](https://kingpin.amazon.com/#/items/269021) 

[^6]:  Associated VP Goal: [https://kingpin.amazon.com/\#/items/269123](https://kingpin.amazon.com/#/items/269123) 

[^7]:  [327M distilled BERT model](https://wiki.labcollab.net/confluence/display/Doppler/Model+Card%3A+DistilledBERT-327M-170M-20210426) comprises 170M encoder \+ 157M embeddings; [135M parameter distilled BERT model](https://wiki.labcollab.net/confluence/display/Doppler/Model+Card%3A+DistilledBERT-135M-17M-20210503) comprises 17M encoder \+ 135M embeddings and was further distilled from the 327M BERT model. 

[^8]:  For more details on the tiger team efforts, see [Appendix A.](#heading=h.fxq0ospm33f9)

[^9]:  For more details, see detailed writeup [here](https://amazon.awsapps.com/workdocs/index.html#/document/98440764b4e1542de00daae647e3a55f60cee6d74201ac937d5310e8a2e6dcbe). 

[^10]:  See more detailed writeup at [https://quip-amazon.com/74t5A9G0JYUi\#LGQ9AAKkomZ](https://quip-amazon.com/74t5A9G0JYUi#LGQ9AAKkomZ) 