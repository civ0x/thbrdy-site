**AWS Machine Learning 2022 OP1**

Machine learning (ML) allows computers to learn human-like capabilities and improve from experience, and is useful to virtually every industry. To illustrate how ML adoption is becoming pervasive let us look at a few examples. Georgia Pacific uses ML to improve paper quality, Bayer to detect pests in farms, Discovery to personalize customer experience, American Express for automatic document processing of loans, Amazon.com to monitor conveyor belts, NFL for sports analytics and player safety, Lyft for autonomous vehicles, and Boston Dynamics for robotics. Our mission is to enable everyone to use ML in their business, and we have come a long way in pursuit of that mission. Five years ago, AWS did not have a meaningful presence in ML and now we have 22+ services and a leadership position when measured in terms of adoption, features, rate of innovation, and quality. While more than a hundred thousand customers use our ML services today, we believe ML adoption is still in its infancy, and over time, ML will become an integral part of how customers do business and become one of the pillars of AWS’ business. 

To meet customers where they are, we invest in four tiers of capabilities. First, we invest in capabilities like ML frameworks, compilers, and inference engines to make AWS the best place (in terms of performance and cost) to train ML models and run ML predictions on CPUs, GPUs, and our own ML chips (Trainium and Inferentia). This enables expert ML practitioners to build powerful self-managed ML systems. Second, we provide SageMaker \- a fully managed ML infrastructure for the entire ML workflow. SageMaker takes care of the undifferentiated heavy-lifting of building and managing the ML infrastructure thereby allowing ML practitioners to focus on building ML models and get business value from ML quicker and cheaper. Third, we provide artificial intelligence (AI) services where developers can use (and tailor) pre-trained models to embed common ML-based functionality in their applications: Rekognition (image and video analysis), Transcribe (speech to text), Polly (text to speech), Comprehend (sentiment analysis and entity extraction from documents), Lex (building chatbots), Forecast (forecasting), Personalize (personalization), Textract (extracting information from scanned documents), Kendra (unstructured document search). Finally, we provide AI solutions that customers can use to reinvent business processes without writing a single line of code or thinking about ML. These include Contact Lens (contact center analytics), Monitron (industrial predictive maintenance), DevOpsGuru (DevOps), CodeGuru (software development), and HealthLake (healthcare data storage and analysis). 

As of 6/1/21, aggregated across all the tiers, we achieved a 4-week annualized run rate of $304.1M (+71.1% YoY, and \+$9.2M vs OP2). This breaks down as follows. Machine learning frameworks and infrastructure earned $50.7M ARR and helped drive $680M ARR of ML EC2 revenue (primarily on GPU instances). SageMaker and related services have an ARR of $138.5M (+86.3% YoY and \+$1.0M above OP2). Our AI services combined have an ARR of $114.8M (+134.4% YoY, and \+$4.3M above OP2). Additionally, we provide consultation and implementation help to customers through the ML Solutions Lab, with an ARR of $127.3M (+94.5% YoY, \+$26.0M above OP2). If we include ML Solutions Lab, we are at $431.3M ARR and we expect to exit 2021 with $658.3M ARR. We also observe that these services drive a revenue-weighted downstream impact multiplier of 3.1, which drives an additional $2.0B ARR in 2021\. We expect to exit 2022 at $1.04B ARR (+86.5% YoY) of direct revenue (excluding revenue from ML Solutions Lab) six years since we started the organization from scratch. Additionally, we are encouraged by internal (CDO and AWS) consumption of AWS ML services. For instance, if CDO was a paying customer they would be generating $289.3M ARR today (even after the CDO rate cards) through usage from Alexa, A9 search, Ads, Prime Photos, FCs, and Robotics. We expect our internal adoption to accelerate in 2022 since internal teams are able to innovate faster and avoid investing hundreds of engineers and scientists by using our services.

Let us next discuss a few key hits and misses. First for the hits, we continue to be excited about the adoption of SageMaker and the number of marquee customers it is attracting (e.g., Bloomberg, Lyft, Fruitstand, and Salesforce). When we launched SageMaker, we took a different approach from Azure and GCP by creating an integrated end-end platform with a unified UI, and we are glad to see them (and others like Databricks) pivoting to copy our approach; even analysts say SageMaker has more functionality compared to others. Second, we pioneered large scale distributed training and are pleased that our early bet is paying off as customers like Bloomberg, Lyft, Sparrow, and Anthropic adopt AWS due to our distributed training performance. We are also excited to see that other early technology investments such as the Neo ML compiler is now helping accelerate Trainium compilers (and a large part of our team is helping with that) in addition to accelerating the Inferentia compiler where a quarter of the code base comes from Neo. Third, we are glad to see our AI services like Transcribe, Textract, and Rekognition (all operating in roughly $20-$30M ARR and growing greater than 100% YoY) starting to get traction with marquee customers (e.g., T-Mobile, Slack, Netflix, Intuit, and Barclays) and we can see clear signs of these becoming meaningful businesses. On the misses, we believe some of our AI services like Forecast and Comprehend Medical have not performed as well as we hoped. We believe we can grow these businesses faster by pivoting Forecast to power Foxtrot (our supply chain app) and embedding in other ISV applications, and expanding Comprehend Medical’s coverage to specialties like cardiology and ontologies like SNOMED.  Our second biggest miss is the poor adoption of CodeGuru (our ML powered code review service). While we still believe a service that helps developers write code is important (Snyk, a hot startup, got $470M funding to address this use case), we made a few mistakes in our pricing and core feature set (e.g. lack of coverage on languages). We believe with updated pricing and coverage, adding features to detect security defects, and iterating on customer feedback we can restart the adoption flywheel for CodeGuru. We will discuss our plans to address these misses later in the doc.

Our Tenets are in Appendix A. A list of projects we will deliver per service is in Appendix D. All of the projects are funded with flat headcount (prefixed with F); or with top-down allocation (prefixed with I). Let’s review our plan for 2022, tier by tier.

**ML Frameworks and Infrastructure:** This tier of services is targeted to customers (e.g. Facebook, Uber, and Samsung) who build and manage their own ML infrastructure because they have highly custom infrastructure needs, or want portability across multiple clouds, or have built legacy infrastructure before our managed services were created. We provide optimized ML frameworks so that customers can build the most performant and cost effective ML systems on AWS. Unlike other cloud vendors that offer a single framework (e.g. TensorFlow) optimized for a single system hardware (e.g. TPU), we deliver all popular ML frameworks with AWS-specific optimizations (e.g., optimized S3 access, EFA based communication) across all available hardware architectures (CPU, GPU, Inferentia, Trainium). As a result, customers get up to 20% better performance while using these frameworks on AWS as compared to other cloud providers (F:14). As of 6/1/21, we drove \~$680M ARR in indirect revenue on EC2.

We pioneered distributed training, which has now become the preferred mode for training state-of-the-art ML models (F:26, I:2). Last year we made a bet that large models with billions of parameters will become popular and introduced model parallel training that allows large models to be split among multiple GPUs for up to 40% faster training. We now see several customers (e.g., Bloomberg, AI21, Sparrow, and Anthropic) training large models to develop new text processing capabilities such as automatic generation of news articles. To maintain our leadership and deliver a 20+% performance improvement over other cloud vendors, we will enhance our distributed training libraries with more efficient point-to-point communication primitives \[Q1’22\] and enhance the compiler for distributed training \[Q3’22\]. As the implementation of these training techniques depends on the underlying hardware, we will add support for Trainium \[Q2’22\]. After training, customers want to perform inference on their trained models with optimal performance and lower cost. We build and maintain inference engines such as Multi-Model Server and PyTorch TorchServe that can scale to thousands of concurrent models on a single CPU- or GPU-based instance (F:16). We will scale out these inference engines to multiple instances \[Q4’22\] and optimize their performance on Inferentia and Graviton \[Q2’22\]. Looking around the corner, we see customers needing to efficiently deploy the large models being trained by them, and therefore we will launch *Rubikon* \[Q4’22\] to enable large models to be split across multiple GPUs during inference. We will continue to invest in ML compiler technology by helping build the Neuron compiler for Trainium, optimize ML models for Graviton \[Q1’22\], implement model compression \[Q2’22\], and accelerate data pre-processing \[Q3’22\](F:23,I:3). 

**SageMaker**: The majority of customers who build ML models tell us they prefer not to build the ML infrastructure themselves, but rather focus on building ML models. This is because ML infrastructure requires significant investment to create a secure, highly available distributed infrastructure with real time SLAs. Building this infrastructure is undifferentiated heavy lifting and takes resources away from the differentiated work of building ML models. To address this we launched SageMaker (SM) which provides fully managed ML infrastructure for customers to build and deploy ML models. Using SM, Lyft reduced their model training time from days to hours, Vanguard deploys ML workloads up to 20 times faster, and ADP reduced time to deploy ML models from 2 weeks to just one day. Since launch, SM has become one of the fastest growing services in AWS history. As of 6/1/21, SM’s combined internal \+ external revenue run rate is $326.3M ($137.6M external, $188.7M internal) on track for a 2021 exit ARR of $543M ($229.7M external, $313.6M internal) and a 2022 ARR of $863M ($401.5M external, $461.9M internal). Focusing on external revenue, we expect to end 2021 with $162.3M (93% YoY) and hit $304.7M (88% YoY) in 2022\. 

Let us next discuss the key product strategies for SageMaker. First, we want to provide the best developer experience through an integrated set of ML tools so that ML practitioners will standardize on SM, thereby creating stickiness to AWS.  Second, we want to provide ML infrastructure with the best performance and lowest cost of ownership because we expect complexity of ML development to keep growing. Three years back customers deployed a handful of models, trained models with few million parameters, and SM supported less than a billion predictions/day. Today customers want to deploy up to a million models, train models with 100 billion+ parameters, and SM supports 20 billion+ predictions/day. Third, we want to provide purpose-built data preparation capabilities across multiple modalities (e.g. tabular, text, images, video) since ML practitioners spend up to 50% of their time on data preparation. Fourth, we want to expand beyond the enterprise ML practitioner and encompass data analysts, students, and other potential ML users so that SageMaker becomes synonymous with ML development. 

Next, we will discuss investments to provide the best developer experience for ML. As customers (e.g., Vanguard, JPMC, Fidelity, FannieMae, Fruitstand, and Nike) are standardizing on SageMaker Studio they have asked us to deliver a user-centric collaborative workspace on Studio. To address this, we will deliver *Juno* \[Q1’22\] (F:4,I:2) where multiple users and teams can collaborate on projects and share code, datasets, and models. To make Studio pervasive among ML practitioners we will extend Studio to IDEs beyond the cloud by enabling developers to connect to Studio from IDEs running on their laptop. We will start with PyCharm IDE \[Q2’22\] (F:6,I:4) and will expand to IntelliJ and Eclipse. As ML gets used in production workflows, customers (e.g., JPMC, Alexa) want support for data governance. For this, we will launch *Hawkeye* \[Q3’22\] (F:0,I:10) to support capabilities such as access control for models and data, data validation, and auditing. We will also launch *Zurich* \[Q4’22\] that will enable privacy preserving ML through techniques such as differential privacy or partial homomorphic encryption techniques.

To provide the most compelling infrastructure, in 2021 we are implementing innovations such as reducing training startup time to \< 20s, hosting millions of models on a single inference endpoint, automatic inference instance recommendation, and serverless inference. We will make training cheaper by enabling Spot instances to be added/removed to a training job on the fly based on the instantaneous Spot availability \[Q2’22\] (F:2,I:2) and remote debugging of training jobs on instances to emulate debuggability of self-managed instances \[Q2’22\] (F:3,I:6). We will scale SM Inference to 100M TPS (from 1M TPS) to support large customers like Twitter, enable workloads with complex inference graphs (e.g., Nike and Liberty Mutual) (F:4,I:2), and deliver shadow and one-box deployments (F:7,I:0).

Customers (e.g., Airbnb and Snap) use SageMaker Ground Truth (SMGT) for data preparation (i.e. labeling) of unstructured data (e.g., text, images, and video). SMGT was built for SW/ML developers, whereas most customers today have non-technical personas (e.g., program managers) running their data labeling operations. To address this, we will launch *Beacon* at re:Invent 2021 which will provide customers like Nike, Magna, and TRI-AD with a turnkey solution wherein they provide SMGT with the raw data and SMGT manages the entire data labeling process including setting up the workflow, selecting the annotators, farming out the annotation work, and auditing the results. In 2022 we will continue to iterate on customer asks and expect Alexa to move 90% of their data labeling operations to SMGT. For structured data, customers (e.g., Fidelity and Fannie Mae) use SM Data Wrangler and we will keep improving it by reducing startup time \[Q1’22\], supporting more data sources such as Databricks \[Q2’22\], data transformations such as pivot tables \[Q3’22\], and automating data transformation steps such as fixing anomalies \[Q4’22\] (F:21, I:12). To help customers store ML data, SM provides a purpose-built ML datastore called Feature Store. We will add additional storage modalities (e.g., in-memory storage, video and image storage) and transforming features during feature retrieval and ingest \[Q4’22\] (F:0,I:6). At re:Invent ’21 we will launch a preview of *Axis* to enable ML on geospatial data(F:0,I:19). While 80% of all data captured today has geospatial tagging, only 10% of it is used for ML because current data science tools are not equipped for geospatial data. Customers like BMW, Strava, and HERE have told us Axis can accelerate ML on geospatial data. We will also launch *Tempest* \[Q2’22\](F:0,I:5), a set of primitives that make it easier to do ML on time series data sets.

Customers like Merck, Koch, Invista and others have told us they want to enable data analysts to build ML models, while still having data scientists easily review the analyst’s work. To that end, we will launch *SageMaker Canvas* at re:Invent ‘21, which will provide a no-code interface for doing ML, and automate all the steps of building and deploying a ML model. Canvas’ key novelty is that analysts can closely collaborate with data scientists by exporting their work to Studio, let data scientists make required modifications, and then import those modifications back into the analyst’s Canvas workspace (F:10, I:3). ML students and researchers usually don’t need SM’s enterprise capabilities but instead want to just get started quickly with ML tools. For this we will launch *SageMaker Seychelles* (F:8) at re:Invent ‘21, which will provide a subset of SM Studio features at no charge and with zero setup required, and aim to reach more than 100K users in 2022\. Finally, as SM gets more feature rich and ML infrastructure gets more complex we are seeing whale customers like Fruitstand, CapitalOne, Nike, and JPMC migrate to SM. We expect this trend to accelerate and want to double down on this, establishing SM as the ML standard for enterprises. Hence, we will launch *Teleport* (F:0,I:10) to offer a set of tools for customers to migrate from DIY solutions and other ML vendors.

**AI Services:** Let us next discuss the AI services starting with the Vision services. Today, enterprise processes rely heavily on manual analysis of documents. For example, Cigna Corp processes 3.5M paper claims annually consuming 87.5K human work hours costing $3MM per year. Customers have told us they wish to use AI/ML based automated document analysis since traditional document automation (e.g., OCR) relies on hardcoded rules and template-based extraction that fail in real-life scenarios. To address this need we offer **Textract** which uses AI/ML to not just extract raw content, but also understand the meaning of documents and eliminate many failure modes associated with traditional document processing. As of 6/1/21, Textract is showing signs of becoming a meaningful business for us with an ARR of $14.4M (up 152% from $5.7M as of 6/1/20, 66% above OP2) driven by usage from customers like Cigna, Intuit, and PennyMac, and we are also pleased that Textract is ahead of Google and Microsoft in accuracy. We have multiple customers committing to long term contracts, for example Penny Mac committed to $18M EDP for 5 years. To double down on the momentum and address the needs of customers like Salesforce, Ellie Mae, Black Knight, and Anthem we need to further improve the accuracy of our tables and forms models (currently 71% for table detection, 50% for table structure recognition) (F:10, I:3). We will leverage visual, spatial, and language information in our tables and forms models to first achieve 90% accuracy \[Q4’21\], then 75% on table structure recognition \[Q2’22\] and 95% accuracy on table detection \[Q4’22\]. We will launch *Barracuda* (F:4, I:5) \[Q4’21\] which will improve accuracy by enabling customers to provide hints on the information they wish to extract. We will also launch use case-specific products (F:8) to analyze expense documents \[Q3’21\] and ID documents \[Q1’22\]. Customers like Ellie Mae have told us they want Textract to provide domain-specific document classification and extraction capabilities because understanding a document’s schema is still difficult (e.g., distinguishing a paystub from a bank statement in a loan application). We see potential of large workloads from mortgage industry customers (e.g. Penny Mac, Ellie Mae) by building customized models that automatically extract and validate critical information (e.g. income, assets) from a mortgage document. We expect to exit 2022 with $76M ARR.

Image and video analysis is driving automation across many sectors. For example, online test providers such as Honorlock and Monash University use **Rekognition** (Rek) for virtual proctoring (to confirm student identity and non-use of books, headphones during a test), security companies like Verkada use it to enhance their monitoring and access control, while SmugMug and Pinterest use Rek to moderate their online content. Rek makes it easy for our customers to add image and video analysis to their applications without requiring ML expertise. As of 6/1/21, Rek has an ARR of $18.5M (up 105% YoY, \-5% vs OP2).  To improve the customer experience for virtual proctoring, face authentication, and enterprise security, we will update our Faces (F:7) model to reduce the false non-match rate from 17% to 11% and improve the false non-identification rate from 37% to 26% \[Q3’21\]. To address the asks from customers like DeNA, Baidu and Amazon Advertising we will update our Content Moderation model (F:5)\[Q3’21\] to improve the detection of nudity from 78% to 90% and cut down false positives from 9% to 5%.  These improvements should increase adoption for face-based authentication (2022 ARR $13M), virtual proctoring (2022 ARR $12M), and content compliance and moderation (2022 ARR $12.5M). We will launch a new service *Tangelo* (I:10) that will enable Alexa Smart Home and Ring to leverage Rek for person, pet and package detection \[Dec’21\]. Tangelo is estimated to reduce headcount and capex costs for Amazon by $35M-$40M over a three-year period, enable Ring to build new features for an incremental revenue pipeline of $151M ARR, and provide a new capability for AWS customers like Simplisafe and Abode. We expect Rek ARR to grow from $45M in 2021 to $69M in 2022\.

Now let’s talk about speech and text processing using AI/ML. Enterprise customers have large volumes of audio data, especially in their contact centers. However, extracting insights embedded in customer service calls is manual, slow, and expensive, and customers (e.g., Intuit and Barclays) want to modernize their contact center workflows with ML. To enable this, we are taking a two-pronged approach using our speech to text service, **Transcribe** (F:11, I:21). For customers with existing contact center investments, we provide Transcribe APIs and a set of pre-built solutions through APN partners to enable contact center modernization. For Amazon Connect customers, we offer **Contact Lens** (call analytics) and Voice ID (voice biometrics-based authentication and fraud detection) as turn-key experiences that they can turn on with 1-click. This year, we’re seeing Transcribe nearing escape velocity where it can be a stand-alone business. As of 6/1/21, Transcribe has an ARR of $32.3M (+1.9% vs. plan, \+243% YoY) driven by adoption from customers such as T-Mobile, Enterprise Rent-A-Car, and Square. Since transcription accuracy continues to be the top criterion in customer evaluation we will invest in All Neural ASR (F:12), a next-generation ASR modeling framework that uses end-to-end models instead of the current approach of piecing multiple independent models (e.g., acoustic and language) together \[Q4‘22\]. To help call center agents efficiently summarize key conversation points, we will launch abstractive *call summarization* to summarize important aspects of a call, and *Trend Detection* to identify customer issue patterns over a period of time (F:4). For Voice ID, we will improve accuracy, reduce bias, and detect more fraud types \[Q4’22\]. The increase in virtual meetings triggered by Covid has created a strong need for customers (e.g., Slack, WebEx) to provide meeting captions (F:4) for improved accessibility. Thus, we will enhance meeting captions by adding far field audio, distinguishing speakers, and mixed accents \[Q1-Q3 ‘22\]. We will also launch toxicity filtering (I:3) that enables customers like Twitter, Clubhouse, and Riot Games to remove profanity from real-time conversations. We expect to exit 2022 with $126M ARR.

Unstructured text data comprises 80% of all data in enterprises, yet organizations are unable to glean useful information from this data. We launched **Kendra** (F:67, I:8) to address this need and since GA in May‘20, 20+ lighthouse customers such as Dow Jones, Telstra, CVS Health, Pinterest and Vanguard have adopted Kendra and shared that Kendra provides greater search accuracy in unstructured content versus traditional search engines. For example, Barclays said, “Kendra wipes the floor clean compared to our Solr search solution.” As of 6/1/21, Kendra’s ARR was $11.1M, and we will innovate further through features like providing contextual results (F:6) for each user, and adding connectors to 30 data sources (F:2)\[Q1-Q4‘22\]. We will also make it easy for customers to deploy Kendra with just a few clicks by building the “Kendra search application” (F:2). Finally, we will explore leveraging Amazon Elasticsearch (AES) ($1.1B ARR) to grow faster since 30% of AES business is focused on text search. To that end we will partner with AES team to provide customers a one-click option to get Kendra Question Answering capability on AES (I:6)\[Q3’22\]. **Comprehend** (F:48, I:7) enables customers like Thomson Reuters, McDonalds, and Starbucks to automatically identify key elements in text data (e.g., company names in SEC filings), accurately detect sentiment in text documents, and categorize information into specific categories (e.g., classifying support tickets). As of 6/1/2021, Comprehend has an ARR of $11M (+5% vs. plan, \+100% YoY) with 60% of revenue coming from Comprehend Custom models that allows developers to customize Comprehend for their domain by providing annotated data. In 2022, we will use data augmentation and few-shot learning to reduce the number of annotations required by 75% to further grow its adoption (F:5)\[Q1’22\]. Moreover, customers (Moody’s, Intuit, and Twilio) have suggested they would benefit from text summarization capabilities spanning technical documents, financial statements, and news articles. To that end, we will deliver *Cuttlefish* (I:7), an automatic document summarization capability at a fraction of their current cost \[Q4’22\] and expect to exit 2022 with an ARR of $36M.

**Lex** (conversational AI service for chatbots) (F:137, I:28) and **Polly** (text to speech service) (F:51) are focused on providing differentiated self-service interactive voice response (IVR) capabilities to contact center customers, and along with Connect can position AWS as a leader in contact centers. In the past, we have faced headwinds due to lack of languages, accuracy and developer experience gaps. Lex launched 10 languages across Americas, Europe, and APAC regions resulting in adoption from customers such as T Rowe Price, Change Healthcare, and RBS. As Connect expands globally to provide telephony services in more countries and deploys the service in additional AWS regions, we will increase language coverage from 19 to 32 languages to meet customer needs \[Q4’22\]. To improve accuracy, we will integrate Lex with Transcribe to support multiple accents and dialects (Spanish, French, German, and Portuguese) through multi-accented global speech models \[Q4’22\]. Lastly, we need to improve the bot building experience as developers move from restricted dialog used in legacy solutions to dynamic conversations enabled by Lex. We will launch *Genie*, a new capability that learns from historical conversation logs to reduce IVR bot build time by 10x \[Q4‘21\]. We will reduce ongoing operational overhead of IVR bots by enhancing Genie (F:8, I:4) to discover new intents from call logs on a continuous basis \[Q2’22\]. For **Polly**, in order to support global expansion of telephony services and AWS region availability, we will increase coverage from 13 to 33 languages, and reduce new voice delivery time in new languages from 18 to 9 months \[Q4’22\]. We estimate that these investments will contribute to increasing the IVR adoption amongst Connect customers, yielding $1.09B in ARR for IVR workloads in 2025\.

**Personalize** enables customers to quickly build personalized user experiences with individualized product and content recommendations without making significant science/engineering resource investments. As of ’ 6/1/21, Personalize has an ARR of $8.5M driven by customers (e.g., Discovery, NBC Universal, and Intuit) using it within their websites, apps, and marketing campaigns. In 2021, based on customer feedback, we are creating industry specific recommendation APIs for Retail and Media & Entertainment \[Q4’21\] to make it easier for customers to connect Personalize to their marketing use cases, and will continue that effort in 2022 to expand the selection and user experience (F:3). We will also integrate with top Customer Data Platforms (CDPs) such as Segment and mParticle (F:3) \[Q2’22\] so that customers can build personalized user experiences quickly from their data in CDPs, and we can forge partnerships with the CDPs to drive adoption. We will add support for importing clickstream data directly from Google Analytics (F:1)\[Q1’22\] which 80%+ of websites use today. Further, as a unique differentiator, we want to use the wealth of data at Amazon to provide pre-trained personalization models and plan to lead with a ‘similarities’ use case (e.g. similar style recommendations for apparel using Amazon.com catalog data) (I:2)\[Q3’22\]. Today, Personalize addresses lower funnel marketing use cases, but customers are interested in ML powered solutions for other use cases such as user segmentation. We propose building **Stark** (I:25) as a joint AWS and Amazon Ads effort, which will enable customers to apply ML to common marketing operations, and automatically activate marketing campaigns through the Amazon Demand Side Platform (DSP) or a marketing channel of their choice. **Forecast** enables customers to generate highly accurate forecasts for their business with no ML experience. While many customers are satisfied with Forecast’s accuracy, nevertheless most customers prefer to buy end-to-end demand planning solutions, rather than use forecasting APIs. Hence, we are pivoting our focus to enabling customers through ISVs such as Anaplan, RetentionX, Lightspeed, and Kibo Commerce, as well as **Foxtrot**, which is AWS’s first party application for demand planning. Forecast is resonating with ISVs such as Anaplan since it reduces their time to bring ML based demand forecasting to market and allows them to focus on adding value through domain-specific tools. We will use our flat HC to address biggest ISV needs related to providing a turn-key solution, adaptability to different data characteristics (e.g. sparse data) (F:3), what-if analysis (F:3), and integration with additional data sources such as Redshift (F:2).  

**AI Solutions:** Customers can reinvent existing enterprise processes without writing a line of code by using these services. Let us start with **DevOps Guru** that auto-instruments telemetry on AWS resources and uses pre-trained ML models to alert users of anomalous behavior in their applications and proposes mitigation actions. Launched on 5/4/2021, DevOpsGuru is off to a reasonable start (ARR of $1.7M) and we want to build on it by integrating DevOps Guru with customer’s existing tools and provide more targeted recommendations. Therefore, we will integrate with AWS EventBridge (F:2) and associate targeted recommendations with the top ten infrastructure issue patterns (e.g., a web app built with EC2, ELB and RDS, a serverless mobile app powered by DynamoDB) (F:9, I:11). We will also provide insight into operational health of container applications (F:3, I:4), and log analysis(F:3, I:0)**.** DevOps Guru is expected to close 2022 with $28.5M in revenue and $46.6M ARR. **CodeGuru** provides customers with an opportunity to improve software development by using ML. Today, developers spend more than 10% of their time on manual code reviews and yet many bugs go undetected. Applying ML and automated reasoning, CodeGuru Reviewer automatically detects code defects (e.g., deadlock detection, AWS APIs best practices) and recommendations to fix the defects. We are disappointed by CodeGuru’s low adoption (ARR of $310K) which happened because our pricing was too high and we didn’t have integrations into customer’s CI/CD flows. We updated our pricing on 5/5 and see positive early signs with our monthly churn rate dropping from 33% to 20%, and we will integrate with customer’s existing CI/CD flows (e.g. GitHub, GitLab, and Jenkins) in Q4’21. Moreover, we want to target InfoSec developers since enterprise CISO are highly motivated to automate compliance and security checks (e.g., validate no use of 128-bit encryption, avoid specific open-source libraries). Therefore, we will launch *SecurityGuru* (F:11, I:12) that leverages CodeGuru that leverages ML and automated reasoning to detect security defects and will be made accessible to customers through Inspector.  

We believe ML can transform industrial processes significantly and we are investing in two primary areas. First, predictive maintenance can help industrial companies reduce downtime costs by proactively detecting upcoming failures of their equipment. However, customers such as John Deere and Fender told us that building a predictive maintenance system that includes sensors, data infrastructure, and ML is complex and costly. At re:Invent 2020, we launched **Monitron** (F:20, I:13), an end-to-end system with sensors to capture data from equipment, a gateway to securely transfer data to AWS, ML models to analyze data, and a mobile app to receive alerts for potential failures. Amazon Fulfillment Centers (FCs) are deploying 152K Monitron sensors at 100+ FCs over next 12 months (a purchase value of $133M over twelve years) and are projecting to mitigate 153K hours of equipment downtime, save $492M costs, and reduce late package deliveries by 384M over 12 years. John Deere and Koch are looking to use Monitron to monitor conveyor systems and pumps and Greenfield industrial customers such as Washify and Depia have started their AWS journey with Monitron. As of Jun ‘21, Monitron has an ARR of $368K (+16% vs. plan) and an internal usage ARR of $4M. While current external revenue is small, we see lots of promising POCs that could lead to large scale deployments and could be a big business for AWS. In 2022, we will support large scale deployments (F:4) and build native connectors to work orders (WO) systems so that technicians can seamlessly receive WOs to investigate failure alerts \[Q2’22\] and launch in Asian regions (I:1)\[Q3’22\]. With these capabilities, we expect to exit 2022 with $9.5M ARR. Second, customers today rely on manual video monitoring to inspect physical processes. Customers like Best Buy, Honda, and Dafgards are eager to replace slow, error-prone manual inspection processes with automated ML solutions on edge devices. To solve for this, **Panorama** provides a fully managed experience for enabling ML solutions at the edge with secure edge devices for running ML apps, a cloud service for managing devices and apps, and an app SDK for building apps. Panorama will be generally available on 8/9/2021 followed by a Marketplace for transacting Panorama apps on 9/30. In 2022, in order to accelerate customer adoption, we will add fleet management (F:5)\[Q2’22\] and support hot backups in case of device failure (F:2)\[Q2’22\]. We believe that these features will help us close 2022 with $11.5M ARR ($9.0M HW, $2.5M service). While Panorama provides the core end to end infrastructure to deploy and operate CV applications at the edge, we launched **Lookout for Vision** (L4V) service to identify visual defects in industrial products for customers like Toyota, Rolls Royce, and VW, and we will enable customers to deploy L4V models to cameras on top of Panorama for edge inference \[Q4’21\]. HP, Honda, and Accton want to identify the location of  anomalies on their product so they can guide workers, hence we will enhance L4V to perform anomaly localization \[Q2’22\].

In the last decade, healthcare organizations have focused on digitization through initiatives such as electronic medical records. Now customers like Merck, CorticaCare, and Roche want to use ML to glean insights from medical data and improve patient care by predicting disease onset, tailor treatment to individual needs, and accelerate drug discovery. However, customers currently struggle to make progress because healthcare data is unstructured and stored in disparate data silos, and the analysis tools are difficult to setup and use.  To address this, we will pursue the following: First, we will close feature gaps in **Comprehend Medical** (F:24, I:5) by expanding supported ontologies (e.g. SNOMED\[Q3’21\], CPT\[Q2’22\]) and specialties (e.g. radiology) \[Q1’22\] so that customers like Merck can analyze unstructured medical records (e.g., doctor notes or lab reports). Second, we will launch **HealthLake** (F:20, I:13) on 07/15, which will provide a HIPAA eligible managed data repository, and use ML to extract and tag patient data from medical records so that customers (e.g., CorticaCare and MedHost) can easily search and analyze patient data. In 2022, we will integrate HealthLake with SageMaker to make it easier for customers to use ML such as for predicting the onset of disease based on an individual’s medical history. Third, genomics is a fast growing area in AWS driven by customers like Roche and Illumina. To make computational genomics easier on AWS, we will start by optimizing popular open source genomics frameworks to run well on AWS \[Q3’21\]. In Q2 ‘22, customers will be able to use **Rosalind** (F:16, I:22) which will provide a fully managed service to analyze genomes,  apply ML to genomic data through integration with SageMaker, and correlate genomic data with an individual’s medical data from HealthLake. Customers like Illumina and Astra Zeneca will be able to use Rosalind to understand drug efficacy faster, predict disease predisposition in individuals, and tailor medications to individuals. With these capabilities we aim to exit 2022 with $17.5M ARR from our health AI services.

Now, let us talk about ML-powered low-code/no-code (LCNC) ideas. AWS is seeing increased demand from developers and a new set of user personas (e.g., business analysts, program managers) who might understand programming constructs, but want LCNC tools to help them quickly build and deploy apps in the cloud. LCNC tools will allow us to tap into non-consumption of AWS, while avoiding the risk of AWS capabilities getting disintermediated by other LCNC tools. Therefore, we are leveraging recent breakthroughs in NLP techniques to build two new services. At re:Invent 2021, depending on private beta feedback from CAB customers and internal Amazon users, we will launch a preview of **Vector Code Builder** (F:39, I:4) or VCB, a new ML-powered service that will provide professional developers and low-code users with computer-generated code based on their commands such as “transcribe all the audio files in the S3 bucket recordings”. VCB analyzes these commands, determines which AWS services and libraries are best suited for the specified task, and then generates the desired code in target languages that include Java, JavaScript, and Python. For no-code use cases, we will launch **Vector Query Builder** (F:27, I:10), a new ML-powered capability for Amazon Honeycode that allows business users to specify their app’s expected behavior in plain English without having to write complex formulas or draw complex data model diagrams to build their apps \[Q1‘22\]

To conclude, we continue to be excited about the ML business at AWS. The ML business is super early (compared to the rest of AWS) and we are fully cognizant of the level of investment the company is making here; nevertheless we are working hard to provide the best ROI in terms of direct and indirect revenue. We are confident about building a durable long-term business that will not only be stand-alone profitable, but will also drive overall AWS momentum.