---
title: "What If You Could Feel Someone Breathing From Across the Room"
description: "On nervous system co-regulation, the science of physiological synchrony, and a research program for making interpersonal resonance tangible through consumer hardware."
date: 2026-02-23
tags: ["Co-Regulation", "Psychophysiology", "Spatial Audio", "Apple Watch", "Contemplative Technology"]
connected_project: "Notice"
draft: true
---

import { SectionDivider } from '../../components/islands/shared/SectionDivider.tsx'
import { PullQuote } from '../../components/islands/shared/PullQuote.tsx'

You've felt this before. Sitting next to someone in silence — a partner, a close friend, a fellow meditator on retreat — and noticing that something in you settles. Not because of anything they said. Not because of anything you decided. Your breathing slowed. Your shoulders dropped. Something in your autonomic nervous system responded to their presence and adjusted.

This is co-regulation. It's one of the oldest capacities mammals have — the ability to use another nervous system as a regulatory resource. Feldman's bio-behavioral synchrony research shows it in the data: mothers and infants synchronize heart rhythms during face-to-face interaction. Romantic partners' respiratory patterns converge during empathic touch. Therapist-client dyads show correlated skin conductance shifts during moments of therapeutic alliance. The behavioral phenomenon is robust across dozens of studies and diverse dyadic contexts. The mechanistic explanation is less settled — Porges's polyvagal theory offers one account involving the ventral vagal complex, but the specific neuroanatomical claims have faced substantive criticism. What matters for what I'm building is the behavioral observation, not the theory: regulated nervous systems measurably influence each other.

The phenomenon is real. What's less clear is whether it can be mediated by technology — whether you can take the physiological signal that passes between two co-located bodies and transmit it through hardware, preserving enough of whatever makes co-regulation work that the receiving nervous system responds as if the other person were present. That's the question I'm trying to answer.

<SectionDivider number="01" label="The Idea" client:visible />

## From Noticing Your Own State to Noticing Each Other's

Notice — the interoceptive awareness app I've been building — trains a single person to read their own physiological signals. You tap your Watch, capture your heart rate and HRV, name what you're feeling in your body, and receive a reflection that helps calibrate your felt sense against your biometrics. It's individual awareness training.

Co-regulation extends this to a dyad. Instead of just noticing your own state, you receive a continuous, ambient representation of your partner's autonomic rhythm — their breathing pace, their arousal trajectory, their nervous system's current signature — rendered as spatial audio through AirPods Pro and haptic patterns through the iPhone. Not as data. Not as a dashboard. As a felt presence. Something your body can entrain to without your conscious mind needing to interpret it.

The philosophical continuity with Notice is strong. Notice asks: *what is your body doing right now that you haven't noticed?* Co-regulation asks: *what is happening between your bodies that neither of you has noticed?* Both make invisible physiological structure perceptible. Both bet that awareness itself is the intervention.

The technical continuity is strong too. Notice already runs a HealthKit workout session on the Watch, captures heart rate at ~0.2 Hz, streams data to the iPhone via WatchConnectivity, and persists it in SwiftData. About 60% of the sensing pipeline I need already exists. What's new is the bilateral networking layer, the breathing-rate derivation from accelerometer data, and the generative audio and haptic rendering. Those are substantial — but they're engineering problems with known solution paths, not research questions.

There's an architectural positioning question I'm deliberately deferring: does co-regulation live inside Notice as a new mode, or does it become a separate product? The arguments for integration are real — shared sensing pipeline, existing beta community, philosophical continuity from individual to relational awareness. The arguments for separation are also real — fundamentally different UX paradigm (continuous session vs. momentary capture), a bilateral networking layer Notice doesn't have, and a potential clinical/IP identity that benefits from standing alone. For Phase 0, the question is moot — the prototype is standalone regardless. The decision resolves at the Phase 0 → Phase 1 gate, informed by what the technical build and the dyad testing reveal.

<SectionDivider number="02" label="What the Evidence Says" client:visible />

## The Literature — Genuine but Early-Stage

I wrote a feasibility and regulatory analysis before committing to build anything. The research base for interpersonal physiological synchrony is solid; the research base for technology-mediated co-regulation specifically is thin. That asymmetry matters, and I want to be precise about where the evidence is strong, where it's suggestive, and where it doesn't exist yet.

**What's well-established:** Palumbo et al.'s 2017 systematic review across 52 studies found that interpersonal physiological linkage occurs reliably across heart rate, skin conductance, respiration, and HRV in diverse dyadic contexts — romantic partners, therapist-client pairs, parent-child, even strangers in structured interaction. Goldstein et al. showed that physical touch increases respiratory and cardiac coupling between romantic partners during empathy for pain. Feldman's developmental work demonstrates that bio-behavioral synchrony between parent and infant is foundational to attachment and emotional regulation. The phenomenon is robust and replicable.

**What's promising but limited:** The Breeze prototype (Frey et al., CHI 2018) is the closest prior art to what I'm building — a wearable system that shared breathing biofeedback between two people and found that participants synchronized their breathing rates. Bögels et al. (2022) showed that real-time bidirectional physiological feedback produces greater synchrony than unidirectional feedback. Järvelä et al. found enhanced empathy and social presence in VR environments with shared physiological signals. These studies demonstrate that technology-mediated physiological sharing produces measurable synchrony effects.

**What's missing:** None of these studies measured clinical outcomes. The leap from "measurable physiological synchrony" to "clinically meaningful co-regulation" has not been validated in any rigorous trial. The sample sizes are small (21–39 dyads). No study has used a proper sham control — giving participants physiologically plausible but uncorrelated feedback to distinguish real entrainment effects from the expectation of being connected. And critically, Xu et al.'s finding that haptic heartbeat stimulation produces opposite autonomic effects depending on the individual's interoceptive accuracy suggests that a one-size-fits-all feedback design will help some users and harm others.

<PullQuote client:visible>The existential risk is not technical or regulatory — it's whether the effect is real enough, in a clinically meaningful sense, to survive a sham-controlled trial.</PullQuote>

This is the honest state of the science: a well-established natural phenomenon, promising early signals that technology can mediate it, and no definitive evidence that mediated co-regulation produces lasting therapeutic benefit. That's what makes it a research program rather than a product spec.

<SectionDivider number="03" label="The Platform" client:visible />

## Why Apple Hardware, and What It Can and Can't Do

The rendering side of Apple's ecosystem is remarkably well-suited to this. AirPods Pro with head tracking and HRTF spatialization through AVAudioEngine can position a sound source in world space — the partner's breathing rhythm can feel like it's coming from a fixed location in the room, regardless of how you turn your head. Core Haptics on iPhone allows continuous, dynamically modulated haptic patterns. Together, these create two sensory channels for delivering a partner's autonomic rhythm as felt experience rather than displayed data.

The sensing side is more constrained. Apple Watch provides heart rate at about one sample every five seconds during an active workout session — adequate for trend but far too slow for beat-to-beat synchronization. More problematically, Apple doesn't expose real-time respiratory rate during waking hours. The workaround is to derive breathing rate from the Watch's accelerometer: chest-wall and wrist micro-movements produce a detectable signal in the 0.1–0.5 Hz band when you bandpass-filter the raw accelerometer data. The Breeze prototype validated this approach using a simple IMU at 24 Hz. Apple's accelerometer runs at 50–100 Hz during a workout session, so the raw signal quality should be sufficient.

Whether this actually works reliably on-wrist, across different body types and sitting positions, at the accuracy needed for breathing-pace entrainment — that's the single most important open technical question. If breathing rate can't be derived from the Watch accelerometer at ±2 breaths per minute accuracy, the primary co-regulation channel is blocked.

The end-to-end latency budget is the other critical constraint. Watch to iPhone via WatchConnectivity: up to 500ms. Breathing rate estimation: ~100ms. MultipeerConnectivity between phones: 20–80ms. Audio rendering to AirPods: ~100ms. Total: roughly 800ms for the breathing rhythm channel, potentially 1.5–6 seconds for the full pipeline including heart rate. For breathing-pace entrainment — where cycles run 3–10 seconds — sub-second latency is perceptually transparent. But the system must be designed around autonomic trends, not momentary physiology. You're rendering a slowly evolving portrait of someone's nervous system state, not a real-time mirror.

<SectionDivider number="04" label="The Program" client:visible />

## A Staged Research Program with Kill Criteria

I've structured this as a sequence of bets, each answering one question before committing resources to the next. The structure is deliberately borrowed from how I think about any research program: explicit hypotheses, defined success and failure criteria, and phase gates that force honest assessment before proceeding. Enthusiasm doesn't get to override evidence.

**Phase 0 is a technical proof-of-concept.** Four to six weeks, my time only, no external dependencies. The deliverable is a working prototype on two sets of Apple Watch + iPhone + AirPods Pro that can bilaterally stream physiological data and render it as spatial audio and haptics. The question it answers: can Apple hardware deliver bilateral physiological signal transmission with sufficient fidelity and low enough latency to produce perceptible entrainment?

The build reuses Notice's existing HealthKit workout session, heart rate observation, WatchConnectivity pipeline, and SwiftData persistence. New components: accelerometer-based breathing rate estimation (bandpass filter + peak detection on the Z-axis), MultipeerConnectivity for same-room peer-to-peer data exchange, AVAudioSourceNode with AVAudioEnvironmentNode for generative spatial audio synthesis, and Core Haptics for breathing-synchronized haptic patterns on the iPhone. The networking layer is abstracted behind a protocol so that MultipeerConnectivity (local, same-room) can be swapped for a WebSocket relay (remote, cross-network) without touching the rendering pipeline — same-room first, remote co-regulation later if the mechanism validates.

Success means the breathing rate algorithm hits ±2 BPM accuracy, end-to-end latency stays under one second, the audio renders continuously for 20+ minutes without glitches, and at least some of the 3–5 dyad test sessions with Jhourney community members show a breathing convergence trend. Kill criteria: if the accelerometer can't produce usable breathing data, or WatchConnectivity drops more than 20% of sessions, or the audio experience is more anxiety-inducing than settling, the program stops.

**Phase 1 is the study that matters.** A randomized, single-blind, sham-controlled crossover with 30 dyads — romantic couples, meditation practice partners, and strangers — comparing real bilateral physiological feedback against sham feedback (physiologically plausible but temporally shuffled) and a no-feedback baseline. The primary outcome is respiratory synchrony measured by phase-locking value; secondary outcomes include HRV coherence, subjective connectedness, and affect change.

This phase requires an academic collaborator with IRB infrastructure and research-grade physiological recording equipment. I contribute the technology platform; they contribute the research apparatus. The critical design element is the sham condition: if participants can't distinguish real from sham feedback, and real feedback produces significantly greater physiological synchrony, the effect is genuinely mediated by the signal rather than driven by expectation. If they can easily tell the difference, the sham is inadequate and the study can't distinguish entrainment from placebo.

**Phases 2 and 3 are sketched, not planned.** Phase 2 would move from controlled lab sessions to at-home use over six weeks, measuring whether the effect accumulates with repeated exposure and produces durable changes on validated clinical instruments. Phase 3 is the regulatory fork: launch as a general wellness product under FDA's 2026 guidance, or pursue De Novo classification if the clinical data warrants it. These phases depend entirely on what Phases 0 and 1 reveal. Planning them in detail now would be premature optimization.

<SectionDivider number="05" label="Open Questions" client:visible />

## What I Don't Know and What Worries Me

**The individual variability problem is underappreciated.** Xu et al.'s finding about interoceptive accuracy suggests that the same haptic heartbeat stimulus calms some people and activates others, depending on how accurately they perceive their own cardiac signals. If co-regulation feedback has this kind of bidirectional effect — helping the somatically attuned and dysregulating the somatically disconnected — then the intervention isn't universally beneficial, and the app needs an onboarding calibration process I haven't designed yet. Research showing that small, subliminal deviations from baseline produce better physiological synchrony than large, obvious changes reinforces this: the rendering can't be heavy-handed.

**Whether placebo is the whole story.** The most deflationary reading of the existing evidence is that people synchronize their breathing because they believe they're connected, and the actual physiological signal is irrelevant. The sham control in Phase 1 is designed to test this — but if the sham is perceptually distinguishable from real feedback (different latency characteristics, subtly different audio texture), the blinding fails and the study can't answer the question. Designing a convincing sham for a continuous, embodied experience is harder than designing a sugar pill.

**WatchConnectivity reliability.** Multiple developers report the Watch-iPhone communication channel failing silently after extended use, requiring device restart. For a co-regulation session that needs to maintain continuous physiological streaming for 20–30 minutes, a silent connection drop means abrupt loss of the partner's signal with no graceful degradation. Phase 0 testing will surface how severe this is, but I'm not confident Apple's framework was designed for this kind of sustained, real-time use case.

**The adoption asymmetry.** Both partners need Apple Watch, AirPods Pro, and iPhone — roughly $1,800+ per person at current pricing. This constrains the addressable population to affluent, tech-forward couples or clinical settings that provide devices. A co-regulation tool that only works for people who can afford $3,600 in hardware has an equity problem that no amount of good design resolves.

**Whether "co-regulation" is the right theoretical frame.** As I mentioned at the outset, polyvagal theory — the most commonly cited foundation for this work — has faced substantive criticism on its specific neuroanatomical claims. I'm building on the behavioral observation (nervous systems measurably influence each other), not the mechanistic theory. But the framing matters for collaborator recruitment and publication. Feldman's bio-behavioral synchrony framework is more empirically grounded and less theoretically contentious — it documents the phenomenon without committing to a specific vagal mechanism. That's probably the better anchor for the research program, with polyvagal theory acknowledged as one proposed explanation rather than the foundational framework.

**The Pear Therapeutics cautionary tale.** Pear obtained the first standalone digital therapeutics FDA clearance in 2017 and filed bankruptcy in 2023 because payers wouldn't reimburse. The clinical path — even if the science validates — is long, expensive, and littered with companies that cleared every regulatory hurdle and still couldn't build a business. The new CMS digital mental health treatment codes (effective January 2025) provide a reimbursement pathway at modest rates, but "pathway exists" is very different from "pathway works." If this program proceeds to Phase 3, the reimbursement strategy needs to be validated before filing, not after.

<SectionDivider number="06" label="Why Now" client:visible />

## The Bet

The competitive space is empty. No one is building real-time interpersonal physiological co-regulation on consumer hardware. The patent landscape is open — no granted US patent covers the full pipeline of continuous autonomic sensing, real-time interpersonal transmission, and haptic or spatial audio co-regulation feedback. Apple's platform has matured to the point where the rendering capabilities (spatial audio, head tracking, Core Haptics) are excellent, even if the sensing pipeline requires workarounds. And I've already built 60% of the infrastructure through Notice.

The timing argument for starting Phase 0 now rather than later is pragmatic. Notice's beta just shipped to the Jhourney community, and I'm in the "waiting for feedback" window before the next development cycle. Phase 0 is four to six weeks of builder time that answers the hardest technical questions cheaply. Even if the research program dead-ends — if the accelerometer can't produce usable breathing data, or the sham-controlled study shows no effect — the components have standalone value. The breathing rate algorithm and spatial audio rendering pipeline fold back into Notice for individual breathing biofeedback. The MultipeerConnectivity layer opens the door to shared meditation sessions. Nothing built in Phase 0 is wasted.

But I want to be clear about the overall probability structure. Phase 0 is likely to succeed — it's an engineering challenge with known solution paths and an existing codebase to build on. Phase 1 is genuinely uncertain — the question of whether technology-mediated co-regulation produces effects beyond placebo hasn't been answered, and there's a real chance the answer is no. Phases 2 and 3 are contingent on results I don't have yet and business model assumptions that haven't been tested. The expected value calculation works because Phase 0 is cheap and informative, not because the full program is likely to reach clinical deployment.

The deeper reason I'm drawn to this is the same reason I built Notice. The contemplative traditions have always known that regulation is relational. You sit in a room with a teacher whose nervous system is settled, and something in you settles too. The Theravāda framework makes this explicit: *kalyāṇamitta* — spiritual friendship — isn't a nice-to-have supplement to practice. It's the practice. The Buddha reportedly told Ānanda that admirable friendship isn't half the holy life; it's the whole of it.

What I'm exploring is whether that relational settling — which every practitioner has felt and which the psychophysiology literature has documented — can be made available when the other person isn't in the room. Not as a replacement for presence. As an extension of it. The question is whether the body responds to a technologically mediated signal the way it responds to another body. I don't know the answer. But Phase 0 is the cheapest possible way to start finding out.

---

*This essay describes a research program in its earliest stages. Phase 0 begins in February 2026. The feasibility analysis and full research program document are available on request.*
