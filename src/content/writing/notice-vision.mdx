---
title: "The Body Knows First"
date: 2026-02-27
description: "A biofeedback-assisted state navigation app that trains interoceptive awareness — helping you notice what your body already knows, in the middle of a life."
tags: ["Interoception", "Contemplative Technology", "Apple Watch", "AI Architecture", "Vision"]
connected_project: "Notice"
draft: true
---

import { SectionDivider } from '../../components/islands/shared/SectionDivider.tsx'
import { PullQuote } from '../../components/islands/shared/PullQuote.tsx'
import { NoticeCompetitiveGap } from '../../components/islands/NoticeCompetitiveGap.tsx'
import { NoticeArchitectureDiagram } from '../../components/islands/NoticeArchitectureDiagram.tsx'
import { NoticeInteractionFlow } from '../../components/islands/NoticeInteractionFlow.tsx'
import NoticeVisionTextureGrid from '../../components/islands/NoticeVisionTextureGrid.tsx'
import NoticeVisionLeadTime from '../../components/islands/NoticeVisionLeadTime.tsx'
import NoticeVisionScaffoldingDecay from '../../components/islands/NoticeVisionScaffoldingDecay.tsx'
import NoticeVisionExpansionRings from '../../components/islands/NoticeVisionExpansionRings.tsx'
import NoticeVisionTimeline from '../../components/islands/NoticeVisionTimeline.tsx'
import { CollapsibleAppendix } from '../../components/islands/shared/CollapsibleAppendix.tsx'

<SectionDivider client:visible number="01" label="The Gap" />

## The Gap

You have felt this before. Your shoulders are tight for twenty minutes before you notice you are bracing. Your heart rate climbs through a conversation you tell yourself is fine. Your breathing shallows during a meeting and you only register it when someone asks if you are okay.

The body leads. The story follows. There is always a gap.

The gap is measurable. Heart rate variability drops before conscious anxiety arrives. Heart rate rises before excitement registers. Skin conductance shifts before you can name the feeling. [[ref:craig-2002|Bud Craig's]] work on [[term:insular-cortex|insular cortex mapping]] shows that [[term:interoception|interoceptive signals]] — the body's report on its own internal state — travel a dedicated neural pathway, but conscious access to those signals varies enormously between individuals. The core finding: interoceptive accuracy is not a fixed trait. It is a skill. And it is trainable.

Notice exists to train that skill.

&gt; The most valuable structures in our lives — emotional patterns, somatic intelligence, relational dynamics — are invisible until something makes them navigable.

This is part of a broader thesis I have been developing across two projects. [Scholion](https://thbrdy.dev/writing/the-circuitry-of-science/) makes the dependency structure of scientific claims visible and navigable — the hidden load-bearing walls behind published findings. Notice does the same thing for the dependency structure of your inner life: the connections between what your body is doing, what you are feeling, and how you are relating to it all. The full technical essay — [You're Already Feeling Something You Haven't Noticed Yet](/writing/notice/) — covers the architecture and interaction design in depth. Different domains, same problem. The structures that matter most are the ones you cannot see.

<SectionDivider client:visible number="02" label="What Notice Is" />

## What Notice Is

The core interaction is called a **[[term:frame-snap|Frame Snap]]**. When you notice something shifting — a tightening in your chest, a surge of energy, a settling you cannot quite name — you tap your Watch or phone. That tap captures a biometric snapshot: heart rate, [[term:hrv|heart rate variability]], and contextual signals assembled by on-device intelligence. Then you debrief.

<NoticeInteractionFlow client:visible />

The debrief is not a form. It is a felt-sense encounter. An emotion picker organized by somatic texture invites you to name what you notice. Six groups, three labels each, organized by where you feel them in your body:

<NoticeVisionTextureGrid client:visible />

The taxonomy is sized to a research sweet spot. [[ref:lieberman-2007|Lieberman et al. (2007)]] showed that [[term:affect-labeling|affect labeling]] — the act of putting feelings into words — downregulates amygdala reactivity via right ventrolateral prefrontal cortex activation. But the effect depends on granularity. Six labels are too coarse. Unrestricted free text is too effortful on a Watch screen. Eighteen labels, organized by somatic texture and grounded in Gendlin's [[term:focusing|Focusing tradition]], hits the range where the intervention works. Every Frame Snap is a micro-dose of a well-documented regulatory technique, embedded in a gesture that takes three seconds.

Then Claude — Anthropic's AI — generates a contemplative reflection. Not advice. Not a diagnosis. A mirror. The reflection orients toward *relation* — how you are meeting your experience — never toward *object* — what the experience supposedly is.

&gt; "You said calm, but your heart rate variability was lower than usual. That is not a contradiction — it is information. What happens when you hold both?"

This distinction, drawn from [[ref:barrett-2017|Barrett's]] [[term:constructed-emotion|constructed emotion theory]] and [[link:jhourney|Jhourney's]] contemplative pedagogy, is the deepest design constraint in the product. Emotion labels are frames, not facts. The app's job is to support noticing without grasping.

Claude reflects at three timescales: a brief sentence at snap time — a small act of witnessing; an exploratory paragraph during debrief — an invitation toward curiosity; and daily and weekly synthesis — longitudinal pattern detection that surfaces what you cannot see from inside a single moment. The weekly reflections consume daily syntheses hierarchically, so the AI reasons over compressed patterns rather than re-aggregating raw data. Over weeks and months, these syntheses surface the recurring shapes of your inner life — the patterns you did not know you had.

<SectionDivider client:visible number="03" label="What's Built" />

## What's Built

Notice is not a concept. It is a working product in closed beta via TestFlight with eight testers from the Jhourney contemplative community. Fifteen development sessions have produced a complete end-to-end experience — about 24 hours of build time, starting from zero iOS experience.

The full snap-debrief-reflection loop works on both Watch and iPhone. Voice-initiated snaps via Siri and AirPods let you capture a moment without looking at a screen. On-device intelligence handles context assembly — HealthKit trends, calendar, location, recent snaps — without any data leaving the device.

<SectionDivider client:visible number="04" label="Privacy by Architecture" />

## Privacy by Architecture

Notice's privacy model is not a policy. It is an architecture.

<NoticeArchitectureDiagram client:visible />

Tier 1 reads everything. Raw heart rate, calendar entries, GPS coordinates, contacts. It assembles a structured summary, stripping absolute values, timestamps, and identifying details. Tier 2 — Claude via the cloud — sees only those summaries. "HRV lower than your baseline." "Back-to-back meetings this morning." Never a heart rate number. Never a location. Never a name.

A stateless Cloudflare Worker proxy holds the API key server-side and validates device identity via Apple's [[term:app-attest|App Attest]] before forwarding any request.

This is not just a privacy choice. It is a regulatory strategy. The [[ref:fda-wellness|FDA's General Wellness Guidance]] classifies Notice as non-invasive wellness only if it avoids disease claims — one diagnostic-sounding phrase could trigger FDA jurisdiction. The [[ref:ftc-hbnr|FTC's Health Breach Notification Rule]] applies because Notice combines biometric data with emotional self-reports: any unauthorized third-party disclosure triggers breach notification at $53,000 per violation. By keeping raw health data on-device and sending only structured summaries through the proxy, the architecture *is* the compliance strategy.

<SectionDivider client:visible number="05" label="What the Science Says" />

## What the Science Says

Three research threads converge in the product.

### Affect Labeling

The act of putting feelings into words is not just expressive. It is neurologically active. Affect labeling activates the right ventrolateral prefrontal cortex and downregulates amygdala reactivity — a specific regulatory mechanism distinct from cognitive reappraisal or suppression. The emotion picker is an affect labeling intervention. Every Frame Snap is a micro-dose of this technique.

### Emotional Granularity

Barrett's research shows that people who make finer distinctions between emotional states — distinguishing *irritated* from *frustrated* from *exasperated* — demonstrate better emotion regulation. This is [[term:emotional-granularity|emotional granularity]], and it is trainable. Notice measures it: label diversity over time is a behavioral proxy for granularity development. As users' felt-sense vocabulary expands, their regulatory capacity should expand with it. The data can show whether it does.

### Interoceptive Lead Time

This is the metric I am most excited about, and as far as I can tell, no one else is measuring it.

Notice already collects both data streams: continuous biometric samples from HealthKit running in the background, and discrete conscious reports from Frame Snaps — the moment you notice a shift. The temporal gap between when your body shifts and when you consciously register it is your [[term:interoceptive-lead-time|interoceptive lead time]]. And it is trainable. Shrinking that gap *is* interoceptive development.

<PullQuote client:visible slug="notice-vision" quoteIndex={1}>A month ago, your body would shift 40 minutes before you snapped. Now it is 15 minutes. You are noticing sooner.</PullQuote>

<NoticeVisionLeadTime client:visible />

That is a training outcome you can feel — not a score on a dashboard, but a mirror that shows you something true about your own development. If interoceptive lead time reduction correlates with [[term:maia-2|MAIA-2]] score improvement across beta users, Notice has demonstrated measurable interoceptive training. That is a peer-reviewable finding and the strongest possible evidence for the product's core claim.

<SectionDivider client:visible number="06" label="An App That Gets Quieter" />

## An App That Gets Quieter

Most apps optimize for engagement. More time on screen. More sessions. More data. Notice is designed to do the opposite.

**[[term:scaffolding-decay|Scaffolding decay]]** is the deliberate, progressive reduction of app support as you develop independent interoceptive capacity. The app starts rich and present. As your capacity grows, it steps back.

<NoticeVisionScaffoldingDecay client:visible />

**Full support:** reflections after every snap, active felt-sense suggestions, full biometric context. This is the current app. **Reduced:** reflections shift to on-demand, suggestions fade, biometric display simplifies to trend arrows. **Minimal:** no automatic reflections, the app becomes a quiet archive you consult when you choose to. Your interoceptive capacity is the primary instrument. Notice is documentation.

Phase transitions are triggered by behavioral signals — snap count thresholds, vocabulary stabilization, biometric-label convergence — and confirmed by the user. The app never decides for you that you are ready. It notices, and invites.

This is a genuine commercial bet. An app that trains its users to need it less sounds counterintuitive in an industry built on retention metrics. But the value of Notice is not in the screen time it captures. It is in the capacity it builds. Users do not churn because they are bored. They graduate because they have developed the skill. And the on-device AI model that personalizes to their practice over months creates a switching cost no competitor can replicate.

<SectionDivider client:visible number="07" label="The On-Device Future" />

## The On-Device Future

The Claude API is powerful, but it introduces ongoing cost, latency, and a data pathway outside the phone. The north star is every reflection tier running locally. No API dependency. No data disclosure. No marginal cost per reflection.

On-device [[term:lora|LoRA adaptation]] means the model learns *your* phenomenological vocabulary, *your* somatic patterns, *your* characteristic ways of relating to experience. Not a generic wellness model shaped by population averages — a contemplative mirror that becomes more precise the longer you practice with it. The personalized weights never leave the phone. The privacy-maximizing architecture turns out to be the quality-maximizing architecture too — a rare alignment.

Timeline: on-device brief reflections are days away from shipping. Exploratory reflections on-device are plausible within weeks. Daily and weekly synthesis may require a generation of model capability improvement. The cloud API remains available as fallback throughout.

<SectionDivider client:visible number="08" label="The Market" />

## The Market

Three product categories surround the space Notice occupies. None connect all three layers.

<NoticeCompetitiveGap client:visible />

Calm and Headspace own guided meditation. WHOOP and Oura own biometric tracking. How We Feel and Daylio own mood logging. Rosebud owns AI journaling. None connect all three layers. Notice is not a meditation app that added HRV, or a tracker that added journaling. It is a new thing.

The window to establish this position is narrowing. Calm is adding HRV biofeedback. Headspace is integrating with Oura Ring. WHOOP is publishing peer-reviewed mental health research and adding journal prompts. Each competitor is extending toward the triad from their corner. None have arrived yet, but the trajectories are visible. Time-to-market for the core loop matters more than feature completeness.

Premium, anchored against WHOOP and Oura, not meditation apps.

<SectionDivider client:visible number="09" label="The Larger Vision" />

## The Larger Vision

Notice's trajectory follows a natural widening of the aperture of awareness — mirroring the developmental arc of contemplative practice itself.

<NoticeVisionExpansionRings client:visible />

### Individual Interoception — Now

The current product. You learn to read your own internal states: noticing shifts, labeling felt sense, seeing patterns in how you relate to experience. The mirror faces inward.

### Relational Attunement — Later

Co-regulation expands the mirror to face the space *between* two people. The full research program is documented in [What If You Could Feel Someone Breathing From Across the Room](/writing/coregulation/). The research basis is [[ref:feldman-2007|Feldman's]] [[term:bio-behavioral-synchrony|bio-behavioral synchrony]]: mothers and infants synchronize heart rhythms, partners' respiration converges during empathetic touch. The engineering question is whether you can transmit one person's breathing rate — derived from Apple Watch accelerometers — to a partner in real-time via spatial audio and haptics, and whether the body responds to that mediated signal the way it responds to another body.

The research program is staged with kill criteria at each phase. Phase 0: can the hardware extract breathing rate within ±2 BPM? Phase 1: a sham-controlled study with 30 dyads to distinguish real entrainment from placebo. Phase 2: at-home ecological validation. Phase 3: regulatory pathway assessment. [[term:polyvagal|Polyvagal theory]] faces empirical criticism, individual variability is high, the hardware cost per couple is $3,600+. These are load-bearing constraints, not footnotes.

### Collective Field Awareness — Speculative

The furthest horizon. If dyadic co-regulation works, the same architecture extends to small groups — a meditation sangha, a therapy group, a team. Group physiological coherence is measurable. Notice could surface how a collective field forms and dissolves, who anchors it, how individual states propagate. This is genuinely speculative. But it is the logical terminus of the thesis: making invisible structures visible and navigable, applied to the most invisible structure of all — the felt sense of being in a room together.

Each expansion is gated by the one before it. At the individual level, Notice competes with mood trackers and meditation apps. At the relational level, there are no competitors. At the collective level, the field does not exist yet. Each expansion widens the moat.

<SectionDivider client:visible number="10" label="What Comes Next" />

## What Comes Next

<NoticeVisionTimeline client:visible />

The on-device reflection model is the key technical milestone. Brief reflections moving on-device eliminates the largest cost center, makes the Core tier economically viable at zero marginal cost, and delivers the privacy promise in its fullest form — "nothing leaves your phone" becomes literally true for the most common interaction.

---

Notice is built on a conviction I keep returning to: the most important thing you can learn is how to read your own experience with honesty and precision. Not to fix it. Not to optimize it. To see it clearly enough that you can choose how to respond rather than being carried by reflex.

The technology is in service of that learning. A temporary scaffold that builds a permanent capacity. The biometric data is a mirror for the body you already have. The emotion label is a frame for the feeling you already feel. The AI reflection is an invitation to look at what you already know but have not yet noticed.

*The app gets quieter as you get better. That is the design.*

<div style={{ marginTop: '64px' }}>
  <div style={{
    fontFamily: "'JetBrains Mono', monospace",
    fontSize: '10px',
    fontWeight: 600,
    letterSpacing: '2.5px',
    textTransform: 'uppercase',
    color: 'var(--accent)',
    marginBottom: '24px',
  }}>
    APPENDICES
  </div>

  <CollapsibleAppendix client:visible id="pricing" title="Pricing & Unit Economics">
    <div style={{ marginBottom: '12px' }}>Tiered model anchored against biometric wearables, not meditation apps.</div>
    <div className="nv-appendix-table-wrap">
      <table>
        <thead>
          <tr>
            <th></th>
            <th>Core</th>
            <th>Full</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Price</td>
            <td>~$80/year</td>
            <td>$149–199/year</td>
          </tr>
          <tr>
            <td>Reflection tiers</td>
            <td>Brief (on-device)</td>
            <td>Brief + Exploratory + Daily + Weekly synthesis</td>
          </tr>
          <tr>
            <td>AI runtime</td>
            <td>On-device model (zero marginal cost)</td>
            <td>Claude API (cloud)</td>
          </tr>
          <tr>
            <td>Biometric context</td>
            <td>Basic</td>
            <td>Full enrichment via Foundation Models</td>
          </tr>
          <tr>
            <td>Anchor comparison</td>
            <td>Oura ($70/yr + $300 hardware)</td>
            <td>WHOOP ($239/yr)</td>
          </tr>
        </tbody>
      </table>
    </div>
    <div style={{ marginBottom: '12px' }}>The economic logic: once the on-device model ships brief reflections, the Core tier costs nothing to serve at scale. The premium tier gates longitudinal pattern analysis — daily and weekly synthesis reflections that require Claude's reasoning capacity — not the core snap-debrief loop. A 7-day free trial with full features is the conversion mechanism. Gate nothing during the aha window.</div>
    <div>Willingness-to-pay research suggests 2–4% conversion at the premium tier from a qualified audience. This is viable if the funnel delivers contemplative practitioners and serious quantified-self trackers, not casual meditation-curious downloaders.</div>
  </CollapsibleAppendix>

  <CollapsibleAppendix client:visible id="gtm" title="Go-to-Market Strategy">
    <div style={{ marginBottom: '12px' }}>Three-phase channel sequence, ordered by community-product fit and cost per qualified user.</div>
    <div style={{ marginBottom: '12px' }}><strong>Phase 1 — Community validation (now).</strong> Jhourney contemplative community: 1,000–3,000 practitioners, high philosophical alignment, produces testimonial quality needed to seed other channels. Simultaneously: r/quantifiedself and r/ouraring — technically literate, wearable-native, actively seeking what Notice does in different vocabulary ("HRV-correlated emotional pattern tracking"). Highest ROI per post of any channel.</div>
    <div style={{ marginBottom: '12px' }}><strong>Phase 2 — Niche amplification (post-launch).</strong> Podcasts: Buddhist Geeks, Technology for Mindfulness, Quantified Self podcast, Ten Percent Happier. Contemplative press: Tricycle Magazine. Communities: r/biohackers, Oura partner program, QS local meetups. Beta-listing sites (Product Hunt, BetaList) timed to App Store launch.</div>
    <div style={{ marginBottom: '12px' }}><strong>Phase 3 — Adjacent expansion (months 3–6).</strong> Insight Timer community, Waking Up / Sam Harris community, r/meditation. Each community requires segment-specific vocabulary: contemplative practitioners hear "noticing practice," biohackers hear "biometric-emotional correlation," the QS audience hears "interoceptive accuracy training." The product doesn't change; the framing does.</div>
    <div>Key insight from competitive analysis: the QS/biohacker audience may be the faster growth channel, not just the later one. They're already tracking HRV, already frustrated by the numbers-without-meaning problem, and already primed for exactly the layer Notice adds.</div>
  </CollapsibleAppendix>

  <CollapsibleAppendix client:visible id="on-device" title="On-Device Technical Path">
    <div style={{ marginBottom: '12px' }}>The north star is every reflection tier running locally — no API dependency, no data disclosure, no marginal cost per reflection.</div>
    <div style={{ marginBottom: '12px' }}><strong>Runtime reality.</strong> MLX is blocked for 3B models on iPhone. MLX's memory overhead (~15 GB for Llama 3.2 3B 4-bit vs. llama.cpp's ~3.67 GB) stems from three architectural decisions: no memory-mapped weight loading, an aggressive buffer cache that never returns memory to the OS, and per-operation Metal buffer allocation for intermediates. The theoretical minimum for Llama 3.2 3B at 4-bit with 1K context is ~1.95 GB; llama.cpp gets within 25–50% of this floor. Two viable paths: llama.cpp for 3B models (4–8 second generation, higher quality) or MLX for 1B–1.7B models (under 2 seconds, lower ceiling).</div>
    <div style={{ marginBottom: '12px' }}><strong>Model candidates.</strong> SmolLM3-3B is the leading candidate — purpose-built for on-device, strong instruction-following, Apache 2.0. Llama 3.2 3B is the safe default. Qwen3 1.7B for the MLX/small-model path. Recommendation: benchmark SmolLM3-3B via llama.cpp against Qwen3 1.7B via MLX on target hardware. Let quality evaluation decide.</div>
    <div style={{ marginBottom: '12px' }}><strong>Training pipeline.</strong> Teacher-student distillation from Claude API. Target: 1,200 reflection examples covering diverse snap sequences plus 150 correction examples that demonstrate constraint boundaries — what the model should not say. The correction examples are critical: LoRA fine-tuning on domain-specific output degrades general instruction-following if training data doesn't include instruction-following examples alongside contemplative reflections.</div>
    <div style={{ marginBottom: '12px' }}><strong>Hybrid routing.</strong> <code>.brief</code> reflections: on-device primary, cloud fallback. <code>.exploratory</code>: cloud primary for now, on-device plausible for 3B models as training data accumulates. <code>.daily</code> and <code>.weekly</code> synthesis: always cloud — requires analytical reasoning over variable-length sequences that exceeds small model capability. On-device <code>.brief</code> covers ~80% of API calls.</div>
    <div style={{ marginBottom: '12px' }}><strong>Three-tier evaluation before shipping.</strong> Tier 1: automated constraint gate (no diagnostic language, no raw biometric values, no prescriptive framing). Tier 2: LLM-as-judge scoring relational orientation, phenomenological precision, novelty, tone. Tier 3: blind A/B with experienced practitioners. Ship threshold: Tier 1 &gt;99% pass, Tier 2 within 15% of Claude baseline, Tier 3 preference &gt;40%.</div>
    <div><strong>Long-term differentiator.</strong> On-device LoRA adaptation: the model learns your phenomenological vocabulary, your somatic patterns, your ways of relating to experience. The personalized weights never leave the phone. This is architecturally impossible with a cloud API and represents a genuine moat — the value increases monotonically with use, tied to your specific device and practice history.</div>
  </CollapsibleAppendix>

  <CollapsibleAppendix client:visible id="timeline" title="Operational Timeline">
    <div className="nv-appendix-table-wrap">
      <table>
        <thead>
          <tr>
            <th>Week</th>
            <th>Milestone</th>
            <th>Key Deliverables</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1</td>
            <td>Foundation &amp; Feedback</td>
            <td>Validate Foundation Models on physical hardware (interpreter &lt;1s, context assembly &lt;3s). Incorporate beta taxonomy feedback. Deploy Cloudflare Worker proxy with App Attest. Push new TestFlight build (sessions 11–15). Begin synthetic training corpus generation.</td>
          </tr>
          <tr>
            <td>2</td>
            <td>On-Device &amp; Measurement</td>
            <td>Ship on-device brief reflections via llama.cpp or MLX (runtime fork resolved by benchmark). Implement scaffolding decay phase 1 triggers. Begin measuring interoceptive lead time across beta cohort. Fix keyboard drawer default on debrief screen.</td>
          </tr>
          <tr>
            <td>3+</td>
            <td>Launch Preparation</td>
            <td>Expand beta beyond Jhourney (QS/Reddit channels). Launch notice.tools landing page. Finalize App Store metadata and screenshots (FDA-compliant language). Privacy settings view. App Store submission.</td>
          </tr>
        </tbody>
      </table>
    </div>
    <div style={{ marginTop: '12px' }}>Each week's deliverables are gated by the previous week's outcomes. If Foundation Models latency exceeds budget, the fallback to direct framework calls is straightforward. If the on-device model doesn't meet the three-tier evaluation threshold, cloud reflections remain the primary path with no user-visible degradation.</div>
  </CollapsibleAppendix>

  <CollapsibleAppendix client:visible id="discovery" title="App Store Discovery">
    <div style={{ marginBottom: '12px' }}>Keyword strategy targets low-competition, high-fit terms that no major competitor owns.</div>
    <div style={{ marginBottom: '12px' }}><strong>Primary keywords:</strong> "interoception," "somatic awareness," "HRV journal," "biofeedback journal," "felt sense." These are high-intent, low-competition — the contemplative-tech and quantified-self audiences search for them, but Calm and Headspace don't target them.</div>
    <div style={{ marginBottom: '12px' }}><strong>Secondary keywords:</strong> "body awareness," "emotional awareness," "mindful HRV."</div>
    <div style={{ marginBottom: '12px' }}><strong>Avoid:</strong> "mindfulness" and "meditation" — saturated, dominated by competitors with 8-figure marketing budgets.</div>
    <div style={{ marginBottom: '12px' }}><strong>App Store subtitle (30 chars):</strong> "Body Awareness &amp; HRV Journal" or "Somatic Awareness Training."</div>
    <div style={{ marginBottom: '12px' }}><strong>Screenshot story:</strong> Lead with wisdom, not dashboard. First screenshot: a reflection that names something the user didn't consciously know. Second: the emotion picker with biometric context. Third: the privacy architecture diagram. The aha moment sells; the data display supports.</div>
    <div style={{ marginBottom: '12px' }}><strong>Category:</strong> Health &amp; Fitness (primary), Lifestyle (secondary).</div>
    <div><strong>Critical constraint:</strong> No disease-specific language anywhere in metadata, descriptions, or screenshots. The FDA General Wellness Guidance classification depends on what the app says, not just what it measures.</div>
  </CollapsibleAppendix>

<div style={{ marginTop: '4rem', paddingTop: '2rem', borderTop: '1px solid var(--border-mid)', textAlign: 'center' }}>
  <div style={{ fontFamily: 'JetBrains Mono, monospace', fontSize: '0.65rem', letterSpacing: '0.25em', textTransform: 'uppercase', color: 'var(--accent)', marginBottom: '1.5rem' }}>
    TRY IT
  </div>
  <div style={{ fontFamily: 'Cormorant Garamond, serif', fontSize: '1.15rem', color: 'var(--text-mid)', maxWidth: '480px', margin: '0 auto 1.5rem' }}>
    Notice is in active beta on Apple Watch + iPhone.
  </div>
  <div style={{ marginBottom: '2rem' }}>
    <a href="https://testflight.apple.com/join/2AR4qQYp" target="_blank" rel="noopener noreferrer" style={{ fontFamily: 'JetBrains Mono, monospace', fontSize: '0.8rem', letterSpacing: '0.08em', color: 'var(--accent)', textDecoration: 'none', padding: '0.6rem 1.5rem', border: '1px solid var(--accent)', borderRadius: '4px', transition: 'background 0.2s ease, color 0.2s ease' }}>
      Join the TestFlight →
    </a>
  </div>
  <div style={{ fontFamily: 'JetBrains Mono, monospace', fontSize: '0.7rem', color: 'var(--text-light)', letterSpacing: '0.04em' }}>
    If this resonates — as a user, investor, or collaborator — <a href="mailto:jthomasbrady@gmail.com" style={{ color: 'var(--accent)', textDecoration: 'none' }}>reach out</a>.
  </div>
</div>
</div>
